Data Engineering on Microsoft Azure: Storage Accounts
Microsoft Azure Blob storage is a container system for storing a variety of file types. In this course, you'll learn about the capabilities of blob storage and how to architect a deployment for optimal performance and scalability. Then, you'll explore the options for redundancy and how to recover from disasters. You'll discover where Azure Data Lake Storage Gen2, a feature set within blob storage, can be utilized for big data operations. You'll also learn how to plan for a data lake deployment, examine best practices, and explore how to deploy a Data Lake Gen2 account on Azure. This course is one in a collection that prepares learners for the Microsoft Data Engineering on Microsoft Azure (DP-203) exam.
Table of Contents
    1. Video: Course Overview (it_cldema_01_enus_01)

    2. Video: Azure Blob Storage (it_cldema_01_enus_02)

    3. Video: Blob Storage Performance and Scalability (it_cldema_01_enus_03)

    4. Video: Azure Storage Geo-redundancy (it_cldema_01_enus_04)

    5. Video: Storage Account Disaster Recovery (it_cldema_01_enus_05)

    6. Video: Azure Data Lake Storage Gen2 (it_cldema_01_enus_06)

    7. Video: Big Data Processing with Data Lake (it_cldema_01_enus_07)

    8. Video: Planning a Data Lake (it_cldema_01_enus_08)

    9. Video: Data Lake Storage Best Practices (it_cldema_01_enus_09)

    10. Video: Creating a Data Lake Gen2 Account (it_cldema_01_enus_10)

    11. Video: Course Summary (it_cldema_01_enus_11)

1. Video: Course Overview (it_cldema_01_enus_01)

This course reviews Microsoft Azure Blob Storage and how it can be used to store a variety of file types. You will examine the capabilities of blob storage, and see how to architect a deployment for optimal performance and scalability.

discover the key concepts covered in this course
[Video description begins] Topic title: Course Overview. [Video description ends]

[Video description begins] Your host for this session is Charles Robinson. He is an IT consultant and trainer. [Video description ends]
Hi, my name is Charles Robinson and I've been an information technology professional for over 20 years.

As an IT solution architect, I've used my combination of computer science degree, IT training and years of progressive industry experience in 24 by 7 mission critical environments to deliver technology solutions that meet specific business needs. Since I specialize in the software and application domains of development and information systems, I've gained valuable experience and exposure across all aspects of the IT solutions, design, deployment and operations lifecycle.

Microsoft Azure Blob Storage is a container system for storing a variety of file types. In this course, you're going to learn about the capabilities of blob storage and how to architect deployment for optimal performance and scalability. Then you'll explore the options for redundancy and how to recover from disasters. After that, you'll discover where Azure Data Lake Storage Gen2 feature set within blob storage can be utilized for big data operations. Your plan for a Data Lake deployment cover best practices and then learn how to deploy a Data Lake Gen2 account on Azure.

2. Video: Azure Blob Storage (it_cldema_01_enus_02)

Explore the fundamental features and benefits of Microsoft Azure Blob Storage, such as high availability, encryption, scalability, and managed services. Examine the anatomy of blob storage, access control, data encryption, and data migration methods.

describe the storage capabilities Azure Blob storage provides
[Video description begins] Topic title: Azure Blob Storage. Your host for this session is Charles Robinson. [Video description ends]
In this video we're going to talk about Azure Blob Storage, so Microsoft Azure Blob Storage is the service available through the Azure Cloud platform for object storage. Blob Storage is ideally suited to storing unstructured data and does not have specific requirements for data organization or type. Blob Storage can function in a multitude of ways. By design, it is accessible natively over HTTP and HTTPS, with the addition of NFS version three to those support. Being accessible over HTTP means Blob Storage can be used to serve files directly to the browser or to a media stream. Static websites can be hosted or simple file transfers can be performed. Now any file or blob can be hosted in a Blob Storage account, so they can be used for backup, archiving logging files, file-sharing or data analysis by data services like Azure Analysis Services.

So using Azure Blob Storage provides many benefits over a traditional file server. The service offers high availability options with multiple replicas of all data being stored within the same data center. Within the same availability zone or replicated across regions. Data stored within Blob Storage is encrypted at rest and can be configured to be encrypted in transit. The service is scalable to store large quantities of data with up to five PETA bytes per storage account available on demand and up to 190 petabytes available upon request.

The service is fully managed by Azure, requiring no additional intervention from users. Client access libraries are provided by Microsoft for most popular programming languages to integrate client access and controls to the storage accounts into your applications. Libraries are provided for .NET, Java, Node.js, Python, Go, Ruby and PHP. Now Blob Storage is broke up into three components and understand them is critical to a well-designed storage infrastructure. The storage account is the unique namespace that your data will be accessed through. The unique account name is combined with the Azure Blob Storage endpoint to create the addressing for the data.

The container is an organizational unit for blobs. It is a relational and functions in the same way as a folder in a file system. There is no limit to the number of containers present within a storage account. Azure Blob Storage supports three types of blob. So you have a blob. Block blob, which is a text and binary data, and you can have a way to append blobs which are stores of block data as well. However, they can accept append operations to the end of the block, marking them suitable for operations like continuously writing logs, and the third are page blobs, which are used to store random access files like virtual machine disks or databases.

Now Access to data stored in Blob Storage accounts requires authorization. So this can be provided through multiple supported methods. So role-based access controls can be enforced using Azure Active Directory as an authentication mechanism. Shared access signatures or SAS provides a token. That provides delegated access to the contents of the storage account. A shared access signature is user created and specifies the permissions granted to the user as well as a period of validity before the SAS expires. The token can be signed with either a shared key or using the admins Active Directory credentials.

Now shared keys are used to build a connection string used programmatically by an application to access the contents of the storage account using an authorization header in the request. Anonymous access can be provided for read access to resources within a Blob Storage account, bypassing the need for authorization. Now, encrypting the contents of an Azure Blob Storage account can happen in two operations. Data written to the account is automatically encrypted At rest. Using 256 bit AES, and is FIPS 140-2 compliant. This cannot be disabled and requires no manual intervention or configuration by the user.

Encryption at rest can be performed using a customer-provided key. If security requirements do not support using the default platform-provided keys. Now, granular storage encryption can occur within the client by including the encryption key using client-side encryption and decryption operations. This is limited to whole blobs only and cannot be used piecemeal. Microsoft provides multiple methods for performing seed loading of large quantities of data into Azure Blob Storage accounts. So you can send your own hard drives via Courier to the Azure receiving location. Who will then extract the data and load it into a blob account on your behalf?

Now there are CLI tools like AzCopy for performing data operations against an within storage accounts. They are available for download on both Windows and Linux platforms. Microsoft includes the AzCopy utility in their Azure Storage data movement .NET library for interacting with Blob Storage. Blobfuse is used within the Linux file system to access data in a Blob Storage account and is delivered as a virtual file system driver. Now for organizations who already have data in the cloud but are looking to consolidate it or move it into a Blob Storage account.

Azure Data Factory provides a platform for data transformation and copying operations, and can use shared access signatures, account keys, service principles, or managed identities for authentication and authorization. Now finally Azure Data Box is a service offering where Microsoft will ship you a secured container with data drives included. You can copy your data to the box and ship it back and they will upload the contents to a Blob Storage account before securely wiping the devices.

3. Video: Blob Storage Performance and Scalability (it_cldema_01_enus_03)

Study how to architect a blob storage deployment to meet performance and scalability requirements. Review performance tiers, access tiers, lifecycle management, partitioning, blob types, and latency.

recognize how to architect a blob storage deployment to meet performance and scalability requirements
[Video description begins] Topic title: Blob Storage Performance and Scalability. Your host for this session is Charles Robinson. [Video description ends]
In this video we're going to talk about Blob Storage Performance and Scalability. Now, Microsoft Azure Blob Storage is a service for storing any form of unstructured data on highly scalable infrastructure. The Blob Storage Service supports 2 performance tiers of underlying physical hardware. Standard Tier is the most cost-effective option, and it's optimized for high capacity. It uses spinning disks for the hardware, so offers a less throughput and slightly higher latency. This makes it suitable for storage options for backup datasets.

Let's use for file sharing, batch data processing, and media sharing. The premium tier uses solid-state drives for the hardware, offering higher throughput and less latency than the standard tier. So their premium tiers should be used for workloads that will basically, be read many times or have a high rate of small transactions that can benefit from the improved response times. So, examples of workloads that would gain from using the premium tier are things like data streams, interactive workloads, high-performance computing and data transformation processes. So access tiers are available on Blob Storage accounts, regardless of whether they are standard or premium tier and offer cost-effective methods for storing data based on its frequency of access.

So the hot tears used for storing data that is frequently accessed or written to, so this tier is set at the storage account level, has the highest storage cost. Now the cool storage tier is used for data does not frequently accessed, so it offers a reduced storage cost, but higher costs for data access and is set at the account level. So example workloads are short-term backup or archives. An accumulated datasets for batch processing later. The archive tier is set at the blob level and offers the lowest storage cost, but the highest access cost and data is not immediately available. It is used to store data that is infrequently or never accessed like backups.

Data in the Archive Tier requires rehydrating prior to access, so may be unavailable for hours and there's a cost penalty for removing it within 180 days of storage. Now policies can be applied to the storage account to manage data. The lifecycle management policy is set of rules-based policy that set governance over the data residing within a storage account. So the rules are applied daily and execute at the account level. The container level or against a subset of blobs defined through name, prefixes, or index tags. The policy manage is the full lifecycle of the data. So they can identify age blobs and delete them. Version blobs to mark them as archival when their access time is basically met its threshold or take a snapshot of them defined by their stage in the lifecycle.

So lifecycle management can also automate data moving through the access tiers data in the hot tear can be automatically moved to cold if it has not been accessed in a defined period, or it can be moved from cold to hot if it has been accessed. Data can also be marked as archival and moved into the archive tier for access requirements that have been met there. So data access performance can be enhanced through efficient data partitioning. Partitioning is performed automatically based on the name of the blobs. So the partitioning scheme in Blob Storage uses a range of based approach to load-balanced data and try to limit hotspots as well as to scale the data up without reducing performance.

The partition key is based on the full blob name, which is constructed from the account name, the container name, and the Blob name. So to improve performance of load-balancing operations. Which bounces the partition key range. You should follow the best practices for blob size blocks. So for data stored in the standard performance tier accounts, try to ensure blobs are larger than 4 megabytes to force the system to use high throughput block blobs. For data in the premium performance tier, tried to keep the block size above 256 kilobytes?

Now, appropriate naming conventions will reduce latency as the partitions are read and balanced as well. So Microsoft recommends prefixing the name of other the account, the container, or the blobs with a 3 digit hash and try to avoid append-only if you're implementing timestamping or numerical identifiers on your data. So Blob Storage accounts support basically three types of blob, and the type is specified on creation and cannot be changed afterwards. So Block blobs are best used for large amounts of text or binary data. The Blob is comprised of multiple blocks that are assigned a unique ID that can be managed individually. Append blobs are optimized for continually appending data. They function very well in situations like constant log writing, where a new block is only written to the end of the blob.

Append blobs could not update or delete the already written blocks, and a block can be set up to 4 megabytes in size. Now page blobs are used for random access storage. Blocks are 512 bytes in size, and when the Blob is written to pages, it can be modified from a single page up to 4 megabytes in individual page increments. So Latency is the time taken to deliver information to an from a blob in a single request, and storage operations the latency is related to the number of input-output operations per second. Also called the request rate. So to determine the throughput required for your application to perform within acceptable tolerances, multiply the request rate by the request size.

This helps to determine how much network bandwidth is required between the application and the storage account. Azure Portal provides two metrics for latency in the storage account. So we have end to end latency, which is the time from when storage receives request. From the client to when the client acknowledgment is received, after it has received the less packet from the storage account, and server latency is the time taken when Azure Storage received the final packet in the request until the response is returned from storage to the client.

4. Video: Azure Storage Geo-redundancy (it_cldema_01_enus_04)

See how to use the geo-redundancy features in Azure Storage to design highly available applications. Review object replication, distribution, locally redundant storage, zone redundant storage, geo-redundant storage, and geo-zone redundant storage.

utilize the geo-redundancy features in Azure Storage to design highly available applications
[Video description begins] Topic title: Azure Storage Geo-redundancy. Your host for this session is Charles Robinson. [Video description ends]
In this video, we're going to discuss Azure Storage Geo Redundancy. Now, Replicating objects between blob storage accounts in Azure can be used for achieving multiple goals if the organization is serving data to geographically disperse clients, the data can be replicated between storage accounts to be closer to them to reduce latency. Or data can be replicated to a storage account that is only used for archiving. Or from an archive to a cool or hot tear after hydrating it. So object replication is applied through a policy that defines the rules to apply the rules specify both the source and the destination storage accounts. The source and destination container and the blocks to be replicated. Object replications are performed asynchronously and do not come with an SLA for replication completion.

So source and destination can be frequently out of sync. Blob versioning must be enabled on both storage accounts and triggers the replication process when a blob is modified. When blobs are deleted, only the current version is deleted. Previous versions will remain. Now there are constraints and limitations imposed on the object replication process. So things like the following snapshots on data are not supported. Blobs in the archive tier cannot be replicated. Blobs can't be replicated from hot to cool. and cool to hot tiers. An immutable blobs cannot be replicated. So data in Azure storage accounts is always replicated for resiliency. But there are optional tiers for replication to expand on that base replication for additional cost. So regional replication is not enabled by default, but will replicate data from the source region to another region within the Azure platform. A region is set of data centers connected locally. But geographically distant. From another Azure region.

Availability zones are independent data centers within Azure region. While they are connected to the other data centers within the region. They have their own cooling power and networking to ensure continuity. If one becomes unavailable. Data replicated to an availability zone will fail over automatically if there is an outage. However, data replicated to a secondary region will require a manual failover if there is an outage. Replications between regions happen asynchronously, so there is a possibility that data in the secondary region is not up to date with the primary region at the time of an outage. The last sync time is a property attached to the blobs within a storage account that has replication enabled. That basically has a date stamp with the last successful replication to the secondary region, so this can be referenced to determine which is the most up-to-date information when services are restored, and a failback is being considered. Locally redundant storage is automatically enabled for all storage accounts. So it replicates the data three times within a single data center in the primary region.

Which means that data is replicated across different storage arrays in different aisles to remove component liability from causing an outage. Locally redundant storage offers 11 nines of durability. Durability is a measure of the likelihood of loss of data through corruption or component failure. But is not reflective of availability. So the data in a locally redundant storage configuration is written synchronously to all three locations, so a component or rack failure will not affect data consistency. Zone redundant storage expands on the locally redundant storage pattern. So data is replicated three times across 3 availability zones within the region. Recall that an availability zone is an independent data center with its own power supply, cooling equipment, and networking. So, zone redundant storage options offer twelve 9s durability over the course of a year. If a zone fails, Azure will automatically fail over to the next zone. Repointing DNS entries. Your application should support retries, and back off as the failover process is happening to enable resuming operations after the process is complete. Zone redundant storage is not available in all regions.

Microsoft publishes a list of currently supported regions that should be referenced before an account is created. Archived here is not supported in zone redundant storage. Only the hot and cool tiers can be replicated. Geo Redundant Storage is a service for applications that require high availability or are globally distributed. Data is written in the standard, locally redundant form, writing to three different locations within a single data center. Then an asynchronous process replicates the data from one of these locally redundant copies to a secondary region. The data that lands in a secondary region is written locally three times. The net result is locally redundant storage in two different regions. Geo Zone redundant Storage is an extension of the Geo-redundant storage and offers the highest level of data redundancy on Azure Storage. So data is written as zone redundant in the primary region, spanning three independent data centers.

The data is then replicated to a secondary region where it is written as a locally redundant in the zone it lands in. The result is 6 copies of the data across four different data centers in two different regions. This method of storage offers sixteen 9s of durability. This is somewhere in the region of 10 bytes of data loss to corruption or other factors per exabytes of data per year. This method of storage is only available on general-purpose, version two accounts and is limited in the regions that currently support it.

5. Video: Storage Account Disaster Recovery (it_cldema_01_enus_05)

This video examines the storage account disaster recovery options. It reviews redundancy, geo-redundant application design, circuit breaker patterns, account failover, and considerations, such as logs, failure tests, overrides, exceptions, archived blobs, Azure File Sync, and Azure Data Lakes Gen2.

describe the available options and design options for Azure Storage account disaster recovery
[Video description begins] Topic title: Storage Account Disaster Recovery. Your host for this session is Charles Robinson. [Video description ends]
In this video, we're going to discuss storage account, disaster recovery. Now ensuring service continuity and data loss prevention is a concern of all businesses. Using cloud services like Azure Blob Storage can help to mitigate the potential for data loss through a combination of functions built into the service, as well as incorporating design patterns and applications that utilize the storage. Azure Blob Storage provides high availability, high durability storage for text and media blobs. It offers multiple options for replication to ensure the data resides in multiple places. So Geo-redundant data storage replicates the data three times within the same data center.

But on different storage in the primary region is written to. Then it replicates the data out to a secondary region. So Geo zone redundant storage takes this concept a step further. Replicating the data across 3 availability zones which are individual independent data centers in the primary region before replicating it out to a secondary region. Applications utilizing Geo redundancy for continued operations can use Azure Geo-redundant storage for their data storage. However, this carries with it some design concerns. So data in the secondary region of a Geo-redundant storage deployment is in a read-only state until a failover is initiated. This ensures temporary outages do not result in split data. Situations and applications can be designed to take advantage of this by flipping to a read-only mode to continue serving the data without compromising consistency. So data is replicated asynchronously between regions, so caching and data fetches may be affected, and should be accounted for. The last sync time attribute is written to blobs and indicates when the last replication took place.

This will help to ensure data consistency when services resume after an outage. Storage Client Library provides functions for redirecting requests to the secondary region automatically. If access to the primary region times out. Initiating an account failover repoints the DNS entries to the secondary region and provides write access to the data. So should be used with caution, especially if the app is built to automatically switch to write only with data in the secondary region. Failing back and require manual data merging and cause consistency issues. The circuit breaker pattern is a design pattern used to prevent applications from continuously repeating operations that are failing, and then providing a mechanism for automatic detection of resolution and resuming operations. So when the application is in a closed state. The request is routed normally to the operation. The system has a proxy function that maintains records of recent failures. If a threshold is exceeded by the number of failures, then the application moves the proxy to an open state. Which initiates a timer.

Now when the proxy isn't an open state, the request will fail and the proxy will return an exception to the application. The proxy can assume 1/2 open state, which is initiated after the timer expires. After the open state attempts to resume a closed state. So this half open state will restrict the number of requests that can be passed to the service. That went down to protect it from getting accidentally overwhelmed and triggering another failure. So there are some items to consider when the developers are looking to implement this circuit breaker pattern. So when the proxy enters the open state and starts failing requests, it will automatically send exceptions back to the application, which must be architected to respond to the exception. The application may stop specific operations to that resource or perform alternate operations to continue functioning without complete failure. The application should log all failed requests and the exceptions thrown for both ongoing health monitoring and post-failure analysis. The application can be built to perform on its own testing against the failed service to determine its availability.

Rather than using a timer before attempting to resume a closed state. This could be pinging the service to determine if it is up or something more advanced. A manual override or reset option could be made available to application administrators to reset the failure and continue operating when they are made aware of the service becoming unavailable again. So failover for a Geo replicated Azure blob storage account is a manual process initiated by the customer. By default, data is only ever written to the primary region by your application, and all replications to the secondary region are read-only. So initiating the failover process updates the DNS entries into Azure to point the primary domain name of the secondary storage account and resumes write access to the secondary data. Now, along with the DNS update and enabling writing, the failover process

will configure the storage accounts to be locally redundant accounts. Discontinuing Geo-replication processing the customer must re-enable Geo redundancy after a failover event has occurred. Now, Geo-replication storage accounts will replicate archive blobs however, they will require re-hydrating prior to re-enabling Geo redundancy. Storage account failovers are not supported by Azure File Sync. Enabling the Azure data like Generation 2 hierarchical namespace will disable Geo replication capabilities. Premium an immutable block blobs do not support Geo redundancy failover operations. For situations where performing the failover operation is not desirable, copying data out of the secondary region into a third region for application access can help to remove the overhead of having to re-enable Geo redundancy, and failback data to the primary region.

6. Video: Azure Data Lake Storage Gen2 (it_cldema_01_enus_06)

This video examines the role played by Azure Data Lake Storage Gen2 to manage big data used in analytics scenarios. It reviews the hierarchical namespace, performance enhancements, best practices, access control, and security.

describe the role played by Azure Data Lake Storage Gen2 in managing big data used in analytics scenarios
[Video description begins] Topic title: Azure Data Lake Storage Gen2. Your host for this session is Charles Robinson. [Video description ends]
In this video, we're going to discuss Azure Data Lake Storage Generation 2. So Azure data like Generation 2 is a data Lake storage solution for centralized in the storage of structured and unstructured data. Now Microsoft has built Azure Data Lake Storage Gen2 on the Azure Storage platform. Benefits from most of the capabilities of Azure Storage without requiring additional configuration. It provides a hierarchical namespace on top of the blob storage. This namespace is used to order blobs in a folder hierarchy to maximize efficient data operations and enforce atomic operations against the directory. Accessing the data Lake instance can be done through the Hadoop distributed File System, Command-line interface or for short HDFS CLI, and through remote services like Ambari, WebHCat and SSH.

Alternatively, you can use the Azure Data Lake Storage Generation to rest APIs. To interact with the file system. The interface provides operations for creating and deleting file system folders and files. The most powerful method for interacting with the data Lake is using the Azure Blob file system driver. Also called ABFS, which is a dedicated driver for Hadoop running on Azure Storage. Making it compatible across data Lake Gen2, Azure HD Insight, and Azure Databricks, and Azure synapse analytics.

The driver presents the hierarchical file system view and enables a user to perform any action on the data, as well as acting as a source or destination for MapReduce. Apache Hive and Apache Spark. Hierarchical Namespace are on an overlay that provides various features on top of the file system. By implementing directories, the namespace can perform functions against subsets of data without having to walk through directories to get there. This also enables atomic operations where everything succeeds or everything fails. Without having to 1st process all of the data contained in the data Lake.

So access control lists for managing user authentication and authorization against the data can be implemented at the directory and file level. Throttling and bottleneck management can be performed by the ABFS driver to remove as many errors as possible, ensuring a maximum throughput of valid data. The namespace improves performance through the way data is read and manipulated, but also through removing the need for data transformation operations. Now, including the hierarchical namespace opens the door for various performance enhancements. So although Azure Data Lake Gen2 is built on object storage.

Traditionally, object storage processes folders as a virtual entity referenced by AURI. The hierarchical namespace offers a true folder hierarchy with distinct folders. This translates to various changes in how data is handled. So renaming an relocating files within directories is an operation against metadata. So only performs the operation once compared to traditional object stores that would copy and then delete the original copy, requiring two operations for every move. Queries can use partition scans to prune the data through a predicate pushdown to significantly reduce the amount of data that is processed. Operations are atomic, meaning they either basically pass or fail everything.

The system will never get stuck in a partially implemented state or result in hung operations. Now there are Best Practices for working with Azure Data Lake Gen2 that should be followed to ensure optimal performance. So use Azure Active Directory security groups rather than individual users for setting permissions within the data Lake. There is a maximum of 32 entries per access control list or ACL. So this helps ensure that you do not exceed the threshold and will significantly reduce the number of operations performed against the data when adding or remove user access. The Azure provided firewall can be enabled on storage accounts providing security against external activity.

Hadoop Distributed Copy is a Hadoop tool for moving data between locations using MapReduce jobs against the cluster. It is optimized for Hadoop clusters, so should be the tool of choice for moving big data over at network connections. Apache Oozie is a scheduling system that can initiate copy jobs on a timer or when certain data operations have a curd. Azure Data Factory can also schedule jobs but has limitations to its throughput for large data. Security and processing speeds rely heavily on the data layout when it is in the data Lake. So partitioning and directory structures should be planned in advance of data ingestion.

Now controlling access to the Data Lake and the data within it can be managed through multiple mechanisms. A shared key is a programmatic administrative key to the resource. Used within your application to perform any operation against the Azure storage layer and the contents of it. A shared access signature is a URI generated mainly and provided to partners. To delegate restrict time-controlled rights to the storage account. Role-based Access Controls provide high-level user authentication and authorization to the storage account using Azure Active Directory identities. Well, access control lists provide granular permissions to files and folders within the storage account. Now the Azure platform offers native tools for securing the Azure Data Lake Gen2 instances.

Azure Defender is a security intelligence tool that monitors for unusual behaviors, and access attempts against the storage accounts. Azure storage encryption encrypts data in the cloud using 256 bit AES and is on by default, requiring no user intervention. Private endpoints are a networking feature that securely connects your clients on a VNet to the access of the storage using a dedicated secured link rather than going over the Internet.

7. Video: Big Data Processing with Data Lake (it_cldema_01_enus_07)

Examine scenarios in which Azure Data Lake Storage Gen2 processes big data, from ingestion, to storage, preparation, and presentation. Review big data architecture, components, benefits, challenges, and considerations, such as distribution, partitions, schema semantics, and processing.

recognize the scenarios where Azure Data Lake Storage Gen2 would be applied for big data processing
[Video description begins] Topic title: Big Data Processing with Data Lake. Your host for this session is Charles Robinson. [Video description ends]
In this video, we're going to discuss big data processing with Data Lake. Azure Data Lake Storage Gen 2 is a data source for the centralizing of storage of both structured and unstructured data. It is primarily a landing zone for big data operations and analysis to take place. All architectures in a big data solution follow the same four principles in terms of processing stages. Step one, ingesting data is the process of acquiring it from the source data in big data architectures may come from sensors, application logs, generated files, or a multitude of other sources. Two data that is ingested from multiple sources require a centrally accessible storage platform.

The data Lake offers storage capabilities for mixed data types from many sources Three data preparation is the process of removing bad data. Transforming the data to a usable state and pulling samples into usable blocks. This is also the stage where training models for data science, efforts would take place, and 4th the purpose of the data analysis solution is to determine the answer to questions posed by the business. The presentation layer abstracts the answers from the querying process and generates reports and visualizations for businesses to consume. A big data architecture follows the same principles about can vary by organization and which tools are used. So data is processed in two ways, either in a batch or in a stream. A batch will queue up a quantity of data and then execute on it. A stream processes the data as it is generated in near real-time. Data is collected from various sources and is either stored in a data lake to be prepared for batch or push through it for streaming and either instance the data once processed is either stored in a new data store.

For analytical processes to execute against orders passed directly to the presentation layer where reports and visualizations can be created. All big data architectures share a common set of components. Now, these components can be distributed or tightly integrated. The source is where the data is being generated. Like application logs and data stores or databases or real-time streams like hardware sensors, and data storage requires a solution capable of ingesting large amounts of high-velocity data in multiple formats. Data lakes are more frequently deployed for big data architectures. Now, batch processing operations must filter an aggregate data to make it common for use in reporting. This processing involves an application that can read, analyze, and write data in large quantities. Hadoop HD insight and Azure Data Lake Analytics can fill this role. Once data has been processed, it is moved into the analytical Datastore, so the structured data. This is frequently a data warehouse like Azure Synapse Analytics or a no SQL service like Azure Cosmos DB.

For unstructured data this can be a blob storage account. Reporting services are used to define and visualize the outcomes derived from the data, visualization looks like power BI for toolsets and can be leveraged directly from Azure or standalone technologies can also be used. Notebooks using Python can be used externally as well. Now utilizing the big data architectures described in this video on a service like Azure can bring many benefits. Microsoft have introduced an assortment of different technologies onto the Azure platform through Microsoft built offerings to Microsoft supported open source hosted solutions like data bricks. This offers a lot of flexibility and choice for ensuring data can come from any source in any type. Now using a cloud platform offers Elasticity and growth opportunities for ensuring resourcing is there when it is required. Rather been limited to the available hardware of an on-prem solution. Performance can be scaled up through parallelism and resourcing, available through the infrastructure of the cloud.

Companies with existing solutions can leverage their investments to still gain the benefits of the big data architecture without having to retool or migrate their existing landscape. Along with the benefits that come through the big data architectures, there are still challenges that organizations may face. So big data solutions, by their nature, are dispersed landscapes and can be complex with multiple data ingestion or storage or processing and visualization efforts taking place simultaneously. Performance and application modifications may have unintended consequences and can cause backlogs and queuing for resources. Many of the tools require a specialized skills and knowledge, making resourcing them difficult for management. Big Data is still an emerging field, and tools are continuing to be added and removed as their value peaks and drops. Expecting change in the big data landscape should be part of the organizational culture.

So organizations looking to implement the big data architecture should adhere to some best practices. So distribute the workload by leveraging parallelism. Distributed filesystems, and splittable file formats, can help to increase the performance of data operations. Planning data, partitions, and structures ahead of adoption can return significant performance gains and troubleshooting failures becomes easier to resolve. The schema can be built using schema on write or schema on read. So schema on read adds flexibility to the architecture and improves performance by reducing bottlenecks through checks and validation operations. Historically, business intelligence operations require processes to perform ETL processes as they move data into the data warehouse. Big data operations against the data Lake can be for transformation in place before moving to the analytical data store for accelerating processing and enabling flexibility, in the operations.

8. Video: Planning a Data Lake (it_cldema_01_enus_08)

Examine how to plan a data lake, with consideration for platform quotas, lake placement, lake distribution, management, and silos. Study the structure of a data lake, including row, cleansed, curated, and exploratory zones. Review hierarchy, access management, file formats, and governance.

effectively plan an Azure Data Lake Gen2 deployment
[Video description begins] Topic title: Planning a Data Lake. Your host for this session is Charles Robinson. [Video description ends]
In this video, we're going to discuss planning a data Lake. Now, when it comes to planning and building a data Lake deployment in the cloud, there are various architectural and design decisions that should be considered. So, planning an enterprise data Lake due to the potential scale may require the ability to sustain high throughput, and stores significant quantities of data. So platform quotas may impact the ability to meet requirements where storage limits may be reached or bandwidth costs push the cost out of range. Distributed organizations with data collection happening in multiple geographic locations may require different regulatory compliance in different data zones.

Likewise, organizations with data collection from different regions may have multiple data lakes deployed across the globe to reduce latency but have a requirement for a central data collection for company-wise statistics and business intelligence, and depending on how the business is structured, there may be multiple data lakes operating independently of each other based on business unit requirements and funding. Although zoning data lakes is part of the process, sometimes it is optimal to deploy multiple lakes to keep zones or stages of data modification independent of each other. So zoning data lakes is the process of separating data into different states. Each state is a transformation of the data, not of the contents. It represents the processes taken to make the data usable through removing errors and taking only relevant data to the next stage. The model proposed here is just an example, and each organization may have different requirements for their data transformation.

So the raw zone is the landing area for data arriving from a source. It is unedited, unfiltered, and in its raw state data should be immutable and made available only as read only. A data lifecycle management system should be implemented to prune old data to an archive when it exceeds its useful life. The cleanse zone performs data cleanup exercises like removing irrelevant columns, data validation, and data is stored into relevant business areas instead of by application. The curated zone is where data is staged for consumption by reporting and business intelligence applications. Data is dimensionally modeled and presented it through a self serve portal. But it should be for note that a data Lake typically is not a store for dynamic visualizations or interactive reporting. Data for these operations should be extracted and stored in a data warehouse or other faster data stores. The exploratory zone offers a testing area for data scientists and analysts to perform operations without fear of affecting production and for testing new models and concepts.

So the performance of a Data Lake can be heavily affected by the folder hierarchy. So it's crucial to ensure this is properly developed. The naming convention for the Folders and the files inside them should be human-readable. Consistent throughout, and should be able to be deciphered by unfamiliar users who are new to the environment. Permission should be applied at a level that is granular enough to secure the data. But without being too low level that it affects performance whenever it changes made. Partitioning strategies should be designed to reduce hotspots within the data while still serving sufficient file sizes. High cardinality Key can lead to oversized partitions and unusable file sizes. Now, data within a folder should follow the same schema and be of the same format. Now access to data lakes should be through a multi-layered approach.

Combining management controls to manage permissions to data. So role-based access control provides coarse-grained permissions to the data Lake Orta folders inside it. These are used to allow or deny permissions to the folder structure, but typically do not dictate the ability of the user perform actions against the data. Access control lists used to define. The fine-grained permissions to the data. This is where the ability of the user to read, write, modify or delete the data is set. Security groups are performed over directly adding and removing the user principles to the data Lake contents. This will reduce the overhead operations when a user is added or removed and provides a single source for management. With larger quantities of data come larger impacts on the speed of the query. Data scientists must optimize the data to meet the query requirements and to achieve optimization.

There are many different file formats that can provide benefits and drawbacks to different situations. Some of the popular ones are Parquet oversee and Avril. So Parquet is an open source storage format for Hadoop produced by the Apache Foundation. It is a column-oriented and optimized for bulk complex data and supports data compression, and multiple encodings. Optimized row column or ORC. Converts row data into columns and stores multiple rows profile. Bringing with it the ability to perform parallel processing across the cluster. ORC has some of the highest compressibility available through data and column skipping. Avro uses Jason to define data, protocols, and types. It is a data serialization framework that serializes data. In a binary fashion rather than a text file an uses remote procedure calls for data movement.

Good governance and planning will add significant value to the organization's growth and the data lakes usability in the future. A data catalog should be created which contains the metadata for sorting and cataloging the data along with management tools and a search function for analyst to find the datasets they require. Data quality is a standard the organization can apply to ensure that data is complete, consistent and standardized. It also dictates where masking should be applied to secure personal identifying or other sensitive data. Data compliance for the data of HIPAA and where the data originates and is stored. GDP are as well as the type of data being handle. Which PCI DSS are all common concerns for any organization managing data, especially global organizations that have to adhere to multiple regulations.

These should be accounted for an plant into the design of the Data Lake. Self-service tools to control access, but allow users to access data without having to continue through data engineers for each request will improve relations and accelerate delivery to the business, and planning for growth and data lifecycle management will stop concerns before they become issues, and the expectations should always be for the environment to scale as the potential growth we have seen in recent years has been an indicator of that.

9. Video: Data Lake Storage Best Practices (it_cldema_01_enus_09)

Study best practices to design a solution that incorporates Azure Data Lake Storage Gen2. Review expectations regarding multiple sources, accessibility, and timelines. Examine architectural considerations, storage concerns, processing data storage in a data lake, data consumption, and governance.

apply best practices when designing a solution incorporating Azure Data Lake Storage Gen2
[Video description begins] Topic title: Data Lake Storage Best Practices. Your host for this session is Charles Robinson. [Video description ends]
In this video, we're going to discuss Data Lake Storage Best Practices. Now Data Lakes are a consolidated repository for organizations data in either a structured or unstructured form. Because of the nature of the link to be a central repository, it must be able to do several things. First, ingest data from multiple sources. The organizations application landscape should storage data within the Lake regardless of data type, structure, and velocity. The Lake should have sufficient throughput and writing speed to ingest the data from all sources as it is generated. The Lake must be securely accessible to anybody who needs access with access controls and data segregation in place.

To limit users to only the data they require and data must be accessible when it is needed and be kept updated to ensure its useful lifespan. Lifecycle management should be implemented to archive data. Self-service portals should be implemented to enable users to access data for modeling and reporting against without requiring datasets. Data Lake architectures are enablers of the microservices application design and should be designed in the same way. So process is should be decoupled to avoid cascading failures in event of 1 component or pipeline encountering an outage.

The data store and zones within the Data Lake should be agile, able to adapt to change and should be delivered in a minimum viable product delivery method. So data pipelines in the Lake should be built on highly available components. Removing single points of failure and providing the service continuity even when one fails. Auditing, data access, and events within the Data Lake is critical to both security and to troubleshooting issues. So the Lake and all data interfaces should provide logging capabilities. Data Storage is the fundamental purpose of a Data Lake. But with many different data pipelines and different tools and users accessing it, it will require careful planning to meet all requirements.

So the storage platform should be scalable and elastic, able to expand on-demand, well, not hitting limits that could cause the Lake to stop functioning. Durability refers to the data's ability to tolerate events causing data loss, not an inability to access it, which is availability. Their ability can be caused by failed hardware, corrupt data rights and human error. The Data Lake should provide a platform that offers high durability to avoid losing data. Now, because the Data Lake will be the repository of company data and must be secure and adhere to any related regulations like HIPAA, GDPR and PCI DSS.

Now processing data stored in a Data Lake can be complex operation with multiple stages. Determining how to perform these functions may affect the data like structure and deployment. The framework should enable big data processing at high speeds. Usually using parallel processing against multiple files in a data set. The Azure Data Lake Gen2 product is an Apache Hadoop overlay on an Azure storage account. With Azure Databricks providing Apache Spark functionality to work in concert to provide big data processing at scale.

Tools like Azure Data factory can be used to perform ETL actions against data in a Lake. The processing compute should be scalable, able to add resources like CPU and RAM as the data load increases. Azure Databricks offers scalable compute groups to bring resources when they are required. Some organizations require constant computing. So on-Prem deployments may have financial sense or financial impact. However, for organizations that deploy compute resources as it is required for operations. The operational model delivered through Azure Databricks, may be more cost-effective.

Clusters can be deployed on demand and torn down to stop during office closure times. Now data stored in the Data Lake will often be accessed by different consumers looking to achieve a different goal. The service should be designed to enable all consumers which can determine its hierarchical structure and the file formats. So data warehouses are typically stores of structured data, modeled and curated data. The business users will extract data from the Lake to insert into the warehouse for querying by other processes. Azure Data Factory can be used to perform ETL processes from Azure Data Lake Gen2 to Azure Data Warehouse or Azure Synapse Analytics.

Apache Hive another big data querying tools can be leveraged to execute queries against the data stored in the Data Lake directly for Ad hoc interactive querying against structured and unstructured data. Machine learning is used for predictive analysis and forecasting. So the Data Lake users may run mining models against the data. Artificial intelligence like chatbots interactive utilities can use the data if it is suitably structured for them to access and read from. Now governance in the Data Lake controls how data is stored and access to ensure security, integrity, and usability. A data catalog should be automated, maintained, and presented from a central point accessible to all frameworks accessing the Data Lake.

It contains the metadata and location information for data stored in the Lake that analysts can use to determine which data set to querying against. So data quality should be assured through controls and checking the quality ensures irrelevant and noisy data is reduced. Compliance ensures the data meets regulation. An organizational policy for data storage and masking practices. Self-service is going to be utilized by most organisations. But there must be controls and auditing in place to monitor its usage for attacks and abuse. Security should be a priority for all organizational practices. But for the data, like acting as a store for all company data, it's critical. So security should be approached the same whether the Data Lake is in the cloud or on-Prem and should be accessed for all data pipelines.

Data should be encrypted at rest and in transit. If the data is especially sensitive, encrypting using hardware security modules can be implemented. The network feeding data into and retrieving data from the Data Lake must be secure. For deployments in the cloud, there should be a VPN service or a dedicated link like Azure Express route to ensure data does not travel over the Internet. Access controls should be implemented for managing access and permissions to the data through identity management utilities and policy, and because applications feed data into the Data Lake, they must be secured and their data must be treated as untrusted. Unable to be executed once it is inside the Data Lake.

10. Video: Creating a Data Lake Gen2 Account (it_cldema_01_enus_10)

See how to deploy an Azure Data Lake Gen2 storage account. Search and select storage accounts. Specify a resource group. Set the kind of account, connectivity method, data protection, data tracking, and storage options.

deploy an Azure Data Lake Gen2 storage account
[Video description begins] Topic title: Creating a Data Lake Gen2 Account. Your host for this session is Charles Robinson. [Video description ends]
In this demo, we're going to show you how to create a Data Lake Generation 2 account. So I'm now in portal.azure.com logged in, and this is where you need to be to do this demo. Azure Data Lake Gen2 is an extension for the Azure Blob Storage service, gaining all the benefits from the underlying storage service like replication, scaling and throughput while adding a hierarchical file system. The hierarchical file system offers more granular security, higher performance for data operations, and atomic operations. Deploying the Data Lake Gen2 product is not a separate SKU, but instead is performed as part of a storage account instance. So we need to do first is Create a new resource. So I'm going to go ahead and click the button link, Create a new resource.

[Video description begins] A page titled: Home- Microsoft Azure appears on the screen. It contains two sections, namely: Azure services and Recent resources. The first section has various options including: Create a resource, Logic apps, and Notification Hubs. The second section has a table with the following column headers: Name, Type, and Last Viewed. [Video description ends]

 [Video description begins] The Content pane displays a page titled: New. It contains a search field. The page has two columns with headers: Azure Marketplace and Popular. The first column contains options such as: Get started, Recently created, Identity, and so on. The second column lists various options including: Windows Server 2016 Datacenter, Web App, and Function App. [Video description ends]

Now I'm going to search for storage accounts.So storage account. Go ahead and click on that and what we need to do here is

[Video description begins] He types storage account in the search field, a drop down menu appears. It includes the following options: Pure Cloud Block Store (subscription), Pure Cloud Block Store (Product Deployment), and Enterprise File Fabric. [Video description ends]

 [Video description begins] The Content pane displays a page titled: Storage account. Underneath the title, a button labelled Create is available. The page has four tabs namely: Overview, Plans, Usage Information + Support, and Reviews. Currently, the Overview tab is active. It has various options, such as: Workspace, Wire Data 2.0, and Microsoft HPC Pack 2012 R2. [Video description ends]

select the Storage account that is done by Microsoft. Alright, so there are several here, but the primary one already selected is the one I'm looking for at the very top. Just simple storage account by Microsoft. Go ahead and click on create.

[Video description begins] A page called 'Create storage account' opens . It has various tabs, such as: Basics, Networking, and Tags. Currently, the Basics tab is active. It has two sections, namely: Project details and Instance details. The Project Details section contains the following fields: Subscription and Resource group. The Instance details section contains various fields including Storage account name, Location, and Performance. [Video description ends]

Now I have to do a little bit of verification here, so I need to specify a Resource group. I'll go ahead and create a new one, Create new, DP203DLG2RG

. [Video description begins] He clicks the "Create new" option available beneath the Resource group field, a pop-up box opens. It has a field titled: Name. [Video description ends]

There we go. Alright, and I'm going to change this before I specify the storage name. I just want to change this to East to West. So that's at the very top of my list. [Video description begins] The Location field shows a drop-down list of various locations such as Central US, West US, and East Asia. [Video description ends]

OK, now Storage accounts. They have to be lowercase. I'm going to call this dp203dlg2 here we go. Now, when it comes to the account type here, we need to keep this general purpose v2. For the Data Lake for what we want to accomplish here.

[Video description begins] The Account kind field shows a drop-down list with 3 options, namely: StorageV2 (general purpose v2), Storage (general purpose v1) and BlobStorage. [Video description ends]

There is right after the Location though this Performance and their Standard and Premium, and it's a radio button. So the performance tier determines the type of drives, the storage account resides on.

Standard Tier uses traditional spinning disk hard drives and provides higher latency retrieval at a lower cost. This tier is fine for mass storage and data archiving. The premium tier offers solid-state drives for consistency. Low latency access at a slightly higher cost. Any applications or time-sensitive requirements probably should use premium tier. Note that selecting the premium tier will limit the type of blob that can be used in the storage account to page blobs. Now Data Lake Gen2 can only run as I mentioned on the storage v2 account type. I'm just going to put this back to standard here.

So because the hierarchical data namespace is an overlay of the blob storage, all replication types are supported with the Data Lake Gen2. So from my use, that will require a zone redundant storage here. So, I'm going to pick Zone-redundant storage, which is the second list ZRS,and now I'm in good shape to click on the Next Networking button. So in the Networking section, you have two configuration items.

[Video description begins] A drop-down list of the Replication field appears on the screen. It contains multiple options, some of which are: Locally-redundant storage (LRS), Zone-redundant storage (ZRS), and Geo-redundant storage (GRS). [Video description ends] 

[Video description begins] The Networking tab is active now. It has two sections, namely: Network connectivity, and Network routing. [Video description ends]

The connectivity method determines if the storage account will be accessible to the Internet or kept internal to your organization only. The networking routing method will determine if your traffic will go out over the Internet, or if it can be routed internally within the Microsoft Network. Let me go ahead and click on Data Protection because we leave everything default there.

Storage accounts offer various methods of protecting data for different scenarios. Point in time restores allow you to take a periodic snapshot of your containers to restore the contents quickly to a recent state. Soft delete is effectively a recycle bin on a timer. It allows you to retrieve accidentally deleted files without having to restore them from backup. It is available at the Blob and file share level. The ability to activate soft delete on containers is in preview at the time of this recording. Now go look at the bottom section here.

[Video description begins] The Data protection tab is active now. It has two sections, namely: Recovery, and Tracking. [Video description ends] 

Under the Tracking, so versioning can be enabled to track changes to files and revert to previous versions of data is modified, and the blob change feed is not currently supported on Data Lake Gen2. So enabling it will remove our ability to enable the hierarchical namespace. But it provides transaction logging capability to track changes made to blobs and their metadata. So that's an overview of that can go ahead and move into the next section here under advanced. So the Advanced tab storage accounts is where we will find the configuration items for modifying the storage account to perform specialized activities and for additional security items. So enabling the Secure transfer option will force all incoming connectors to use a secure protocol.

[Video description begins] The Tracking section has two checkboxes, namely: Turn on versioning for blobs, and Turn on blob change feed. He selects the second checkbox. [Video description ends]

 [Video description begins] The Advanced tab is active now. It contains various sections, including: Security, Blob storage, and Data Lake Storage Gen2. [Video description ends] 

Connections to the REST API will force HTTPS an SMB 2.13.0 and the Linux SMB client will all require encryption to be enabled. So disabling the Allow shared key access will disable connections from apps using the shared key or shared access signature methods for authentication. The Minimum TLS level can be set to enable more compatibility, but comes with the risk of less secure protocols in use. The Infrastructure encryption is in preview at the time of this video and is unavailable unless requested, but it adds an additional layer of encryption using different keys and different algorithms. So under the blob here we can see we can allow access to public blobs within the account.

If this is disabled then the ability to set the ACL to anonymous access is also disabled, and the default blob tier can be set to either write data to the Hot tier, which has slightly higher storage costs but lower access costs, or to the cool tier which has lower storage costs but higher access costs. Now this NFS version three is also in review at the time of this recording, so it's going to be disabled. However, when it is available, clients will be able to connect directly to the storage account using the NFS v3 protocol. So next section basically handles the types of specialty storage options available to Azure storage. The storage account can be turned into a Data Lake Gen2 deployment or an Azure file instance.

Now, additionally, the ability to use customer provided keys for securing tables and queries will be available soon, but it is also in preview at this time. So I'm going to go ahead and click on Enabled and the radio button here for the Data Lake Gen2 storage, and then I'm going to go ahead and click on Next to move to the tag section, and basically, Tags can be set on the account for tracking and management purposes, not needed for here. Just go ahead and click on Review and Create, and do a quick scan here to make sure everything looks good.

[Video description begins] The "Data Lake Storage Gen2" section has a sub section named Hierarchical namespace. It further contains two radio buttons titled: Disabled and Enabled. [Video description ends] 

[Video description begins] The Tags tab is active now. It has 3 options available in tabular form. The titles of the columns are: Name, Value, and Resource. Currently, the Name and Value fields are empty. The Resource field value is set to Storage account. [Video description ends] 

[Video description begins] The Review + create tab is active now. It has following sections: Basics, Networking, and so on. [Video description ends]

I have the right resource group, I'm in the right location. I have my storage account name.

I have zone redundant storage, ZRS selected I have general purpose v2 selected. My performance is standard, and as they come down here I can see my Blob Access tier is Hot and my Hierarchical Namespace, which is what this is all about, is Enabled. This was when I selected that Data Lake option, it's referred as the Hierarchical Namespace in the review section. At this time I can now go ahead and click on Create, and Azure will go ahead and create the storage account for the Data Links here. [Video description begins] The subsequent page opens. The header of the page is Microsoft.StorageAccount-20210331083342 | Overview. The page has a search field. [Video description ends] So my deployment is in progress. This will just take a moment. Ok is now done. I can click the Go to resource button, and quick verification that yes indeed all the options in settings have copied over and now set up and ready for use. [Video description begins] A page titled dp203dlg2 Storage account appears on the screen. [Video description ends]

11. Video: Course Summary (it_cldema_01_enus_11)

This course introduced you to Microsoft Azure Blob Storage and how it can be used to store a variety of file types. You explored the capabilities of blob storage, and examined how to architect a deployment for optimal performance and scalability.

summarize the key concepts covered in this course
[Video description begins] Topic title: Course Summary. [Video description ends]
So in this course, we've examined how to plan for an effective deployment for data and big data storage. We did this by exploring data storage capabilities of Azure blob storage. Blob storage performance, scalability, geo-redundancy, and disaster recovery. We looked at Azure Data Lake Storage Gen 2 and big data processing, and looked at planning an Azure data Lake, storage best practices and creating a storage account. So in the next course. We'll move on to examine how to plan a data structure for efficient storage and transactions.

 2024 Skillsoft Ireland Limited - All rights reserved.