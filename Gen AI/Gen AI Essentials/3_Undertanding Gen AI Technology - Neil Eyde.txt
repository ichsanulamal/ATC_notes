Hey, now for your next presenter. It's me. All right, so I didn't actually get to formally introduce myself but greetings everyone. My name is Neil. I am part of the NAIC, among other groups. I have been looking at my introduction of genera. I really started a couple of years ago, right in late 2021. I was asked, we put together a little working group with an Accenture to take a look at this crazy idea of something called Github, Co pilot, and other related tools. And how they can be used to generate code, generate, test unit tests, et cetera. So I've now expanded, I'm not just focused on AI assisted coding tools, but some of the examples I'm going to show you here today are going to definitely be focused on that. But the purpose here for me is to kind of give you a general understanding of generative AI, right? I have some content that I put together that I think will hopefully help connect some of the dots. And it will give us, like I said, the mental models. It'll give us an understanding for the terminology. And then once we have a common language and a common understanding of these things, then we can figure out how to apply them, to apply them to different use cases. If we could dive in the first thing here we're going to get into is understanding transformers. I know that there was a question early on about this. Now we can move on to the next slide. I'm going to be the first one to tell you that understanding transformers and how it works is not necessary, is not required in order for you to understand how to use it. It's always obviously helpful, but for example, when I drive my car, I don't really understand how a combustion engine works, right? Or watching TV. I don't really understand the intricacies of the TV. I'm able to use these things and manage them. Later on in my presentation, I'm going to take the way to think about it, I'm going to be giving you like, maybe more like the user manual for generative AI, right? And talk to how you would use it. But I did want to take a little time just to kind of help you understand how we got here. What are some, take a peek behind the curtain in terms of what generative AI does and how it works. Because I think if you have a basic understanding for this, it'll have you. I think most people, one of the things that struggle with, and I know I struggle with this as well, is like this all appears like magic, right? It's like how the heck does chat team, you know, or open or Google or whatever The L M that you want to choose, How the heck does it. Take this thing that I give it and come back with something that is really, really comprehensive. And the way that's done is really through the transformer architecture. The transformer architecture is probably the big innovation. And it was really all happened if you've listened to any presenter and I'm like, that's the obligatory slide, right? We have to all draw attention to this research paper from Google in 2017. That's called, attention is all you need if you're really interested in it. You know the great thing about research papers is it's accessible to anyone. So just Google attention is all you need. It'll take your right to the paper and you can read the 40, 50, 60 pages, whatever it is and understand it in its details. But the Need paper introduced this concept of a transformer architecture. Alongside that, there is this model called Burt, which was also from Google It. I had to write it down here, bidirectional encoder representation from Transformers, thank goodness for acronyms, because there's no way I could repeat that multiple times. But this model really was an instantiation or an implementation of this research paper, right? This attention is all you need. Research paper. There are some key things in this model that make a difference. Once again, we're not going to get into the deep, deep details here, but there's just a couple important things to understand. Here really is in the title of the model itself, which is the whole bidirectional part of it. And this is unique because a lot of times, a lot of things that we're done with natural language, we're basically, we're considering words in one direction as it helped determine the next word the transformer architecture introduced is this bidirectional context, where you're basically looking at context in both directions in order to predict the right word, or we'll talk about soon, the right token. The other part of the, of the transformer architecture is obviously the application of this attention mechanism. We're going to double click into the transformer architecture here in just a moment, so I won't talk too much about it right now. But the other key thing around this is also this idea that you can preach, train a model. And the model may be good for certain tasks or may have some baseline understanding of knowledge or a baseline behavior. And the idea that this is the other part of the other innovation here is that then I can fine tune that model by giving it additional detail, additional data. To have it be able to understand other domains, but also be able to do other tasks. For example, if I want to do most models, if you want to do Q and A like we're accustomed to with ChachiPT or with Bing or whatever tool we're using, that Q and A is not an innate ability of a model. That behavior or the ability of doing that, was something that was done through fine tuning. As you can see, these things have just led to an escalation of innovation. Obviously, I think this really started to come to light was with the launch of Chachi PT. Because now all of a sudden we had a tool that we could access directly. And this was no longer just a research paper or the domain of AI engineers or data scientists. Now we actually, from within our browsers, we could actually experience this. Of course, now what I think a lot of us are doing is we're saying, okay, this is really cool. Now let's reverse engineer this and help me understand how all these things work together. We'll spend a little time talking about that here today. The last thing I'll mention here is just we don't have to switch back, but All right. We cannot hear. You can't hear, Okay. I think we lost AI for Microsoft teams. Yeah, we can hear. All right. Yeah, exactly. I got GPT. Yeah, exactly. I just mentioned GPT and I guess maybe Microsoft did. Whatever. The point here being, I got kicked out automatically. Thankfully, I was able to cut back in if it happens again, bear with me, the GPT, a generative pre trained model. It is basically just another implementation of another variation of the implementation of transformers. So, let's move on to the next slide here. How does generative AI work here? I am borrowing from the work of some slides that Paul Nelson has put together and if you haven't had a Chancellor hear Paul Nelson's presentation, he's going to do 1 million times better than I'm do. But hopefully I can do a sufficient model has once again, our goal here is not to make you deep experts in the transformer architecture. This is about the level of understanding that I have of transformers architecture. And it's very sufficient for me to be able to apply general or to figure out how to apply general in a variety of use cases. But the idea here is really we take some text input here, which is represented by this bar on the left hand side, this bar. You can think of it for now, think of it as words. We're going to talk about real quickly how this is really tokens, right? But we're going to take these words, we're going to basically pass these into a model. This model, this is where the magic happens, right? This model, we're going to take a look behind the curtain. Right? This magical model is going to predict the next token. And then what we do is we then take that token and we drop it at the bottom of our list. And then we take that whole process and do it all over again. And we keep doing this over and over again until we get the output answer that we're looking for, we get the code completion that we're looking for. That's generally how the process works. Now one of the things these models do is their aim is really to say, you know what? We're going to go through this process and we're going to figure out what's the right next word or token, as I'll explain in just a moment. Often it will come up with a handful of options and they're all going to have different degrees of likelihood of being the next word. What this means is that there's this thing called temperature, which you'll hear about when you talk about Generate. The temperature is a way that you can basically control how creative the generate the transformer architecture is. If I had said it to the lowest setting, it's going to always only give me whatever the highest probability token is. But if I change that temperature just a little bit, what it'll do is say, you know what? Let me introduce some randomness to this. And rather than maybe picking the best one, let me pick among the top three or the top five best ones in order to introduce some randomness into the output that gets created. And of course, if you start introducing randomness early in the process, that randomness will then feed into more randomness. And that's how you start to get the AI models to kind of the NAI models or the LMs to be able to kind of show some predictability. But I've been talking about these things as words, but they're really not words. The way general I works is they use this concept of tokens. So here's an example of, of a token, right? Accenture is superly awesome Coolness IT service company, right? Each of these different colors represent the different tokens that are actually used. Generative represent a couple things. They represent words in the case where there have fine meanings. Sometimes they represent parts of words. For example, in accenture it actually two tokens involved there. And then sometimes even when you're making up words like super awesome, coolness is not a real word in the dictionary, right? But the generative is able to understand this because it's taking that word and it's breaking it up into its components and it's treats to treating all those components independently. We'll talk about why that's important in just a moment. For every 1,000 words. Well, actually about every 750 words represents usually about 1,000 different tokens. And the reason why tokens are important is because the tokens are really what the language models under. They don't understand words, they understand tokens. Anytime you're going to be interacting with an element, sometimes it's automatically, you don't even know that it's doing it. But what it's really doing it's taking your text and it's converting it into a token format that before it actually does something with it internally. But now let's double click into this predict next token box a little bit. If we can go to the next slide here, we start really getting into the transformer architecture, right? We don't want to get super detailed here, but when I talked about converting these things to tokens, what actually ends up happening is that every model has a dictionary of all of these different vectors. And these vectors can be anywhere from like ten to 20 million unique tokens that it has. Part of the process, as we already talked about, is taking the input text and converting it into a token and converting it to really an embedding vector. The reason why we need the embedding, embedding vector is basically a long list of floating point numbers. Depending on the model you're using, the list can be a couple hundred floating point numbers. More powerful models will actually have thousands of floating point numbers, but the point of these floating point numbers is to produce a mathematical representation of the token. On why is it important for us to have a mathematical representation? Ultimately, what we're doing in these transformer architectures is math. And we have to have things in a mathematical representation in order to be able to apply the math to them. Now that's a bit of an oversimplification, but that is in fact, I can't explain the math that's in the transformer architecture, but that's in essence how it works. Now once we take these embeddings and we feed them into the transformer, the transformer has these different layers. Gpt, for example, has 96 layers. Here, we're only showing six of them. These layers represent there are a series of transformers that learn about the tokens at increasing abstraction levels, The lower level abstraction or the initial layers of the transformer architecture. What help the LM recognize character patterns or understand word forms like run versus running or cat versus cats and things like that. The mid level ones are abstractions. These are really where the language model starts to take what it has from the earlier transformers and starts to understand syntax and starts to understand common phrases. And then as we kind of continue into the higher level abstractions, this is where the LM starts to understand semantics, right? Because all of that other work has already been done in previous Transformers. It takes the output of the previous Transformers and starts to kind of apply more knowledge like semantics or understanding textual relationships. And then finally, at the end, this is actually the abstraction layers where we start to introduce like knowledge. It could be world knowledge in the case of like jet GPD or it could be domain knowledge or whatever the case. This is also where the Transformers start to introduce the nuanced understanding of different languages and different domain and different terminology. All of this is basically one little box that I showed in the previous slide, all going through, all with the purpose of just predicting the next token and then what happens after that. This process just feeds right back in of itself to generate the next token, in the next token until it provides the output that you're looking for. Anyway, that is the essence of the transformer architecture. And if you take a look at the attention is all you need. This is a simplification of what they're trying to describe there. Do you mind on this, just real quick speaking to the cost of tokens? Because I think this is where the use case on NAI starts to kind of come to life. Is understanding the cost, right? Yeah, I'll talk about that a little bit later here. We're just want to really focus on the transform architecture. But yeah, there are imp implications. Kind of goes back to what we talked about before in terms of like what is it that I'm trying to accomplish and what's the best way to do it? And of course I've got to account for cost complexity security and all those things. I'll be talking about that here in just a moment. Let me just get through this next slide or the next two slides and then we'll take a pause. We can, we can answer some more questions on this, can ask just on the slide to understand that for the explanation the look up to a embedding vectors. The second column we process all the tokens. So if we have ten tokens all at the same time, or it's done one by one, well, is one of the values of the transformer architecture is actually part of the innovation, I guess is the way that these jobs can be parallelized. And I have always had trouble with that word, but parallelized, parallelized. So I can take these and I can actually, part of it is not just that I'm doing the embeddings, but I'm able to do all of this processing in parallel. And so to answer your question, no, it's not done sequentially. This process here is done sequentially, right? Because it kind of needs to be. But the process of actually, you know, looking up the embedding token, that is something that is something that doesn't need to be done sequentially. That can be done in parallel. Okay. Next slide? No, this is the next slide, sorry, training. All right. So the training, this slide should look very familiar to the last one. And it's the same information, but we're just going to focus in on a different part of it. Once again, we take our input, we generate the tokens, we grab the embedding of those tokens, We run it through these generated layers, and we use that to predict the next token. Now what we do is we evaluate, hey, did we get the right prediction? When you think about how we train these models, what we're doing is we're taking whatever content is available to us. Many of the models, it could be trained from text on the Internet or maybe maybe trained from GPT, I'm sorry, Wikipedia for example, or whatever it is. Whatever the dataset that the creator of the LM chose to use, You take this text, you run it through this process, and then you look at the right side. You take a look at did the network produce what was expected? If it didn't, then what we need to do is we need to fine tune. We need to change the weights or the parameters of the model in order to try to get to that desired output. And we continue that process at a massive scale to continue to adjust the weights. When I think of weights, the way I think about it is about it, like in a recording studio, right? You know, you get these big, huge, you know, mixing consoles, 1,000 buttons or tens of thousands of buttons all over the room. And so what we're really doing is, you know, if we're trying to get a certain sound, what we're trying to do is we're trying to adjust all of those buttons in just the right way to produce the desired output. And if we don't get the desired output, what do we do? We go back and repeat the process. And we continue to change the models until we get the sound that we're looking for. Right? That's very much what's happening here is we're going through, we're evaluating the output and then we're determining through a process that is technical term is back propagation. We're basically taking the output, we're feeding that back through the model to kind of adjust the weights in a thoughtful and intelligent way to adjust the weights to try to get us a little bit closer to the desired outcome. And once we get close enough that once we meet a certain threshold, then we know that, hey, those weights are where we need them to be and we can move on to the next token. This is why, as indicated here, the process can take months and months until learns that. Okay, Then finally, the last thing here that often comes up, and this is something I think it's important for everyone to have a good mental model for, is this idea of embedding vectors. It'll come up later on in our discussion. If we can go to the next slide. I already mentioned that an embedding vector is basically a mathematical representation of some text, right? It's the floating, a good way of thinking about this. I love the way Paul describes this. Could think of an embedding vector as basically sticking 1,500 plus probes in your head which are going to measure the electricity. Right when you see something, it's going to fire off certain electrical signals in your brain connectors. These probes are going to basically measure the voltage of the different parts of your brain right now, If you took all of those voltages that you generated, for example, when you were thinking about your grandmother, all of those voltage would basically be a long sequence of numbers that basically represents the embedding vector for your grandmother, right? And then if you start now switching and you think about something else like your grandmother's Pi, it's going to be firing off different neurons in your brain. And you're capturing different voltages. And those voltages are going to be different than the first one. Because in the one you're thinking of your grandmother, the other one you're thinking about pi, right? That's essentially the purpose of embedding vectors. It's a mechanism that enables us to compare similar things. But to do it in a mathematical way, right? Not in a visual way. It's really about, it's not exactly mimicking the way the brain works, but it is inspired by how people think the brain works. And that's essentially what we're doing here, is we're saying, hey, anytime we want to compare content, a common technique is to use embedding vectors. And we'll talk about this here in just a moment, in more detail, like in a business context, for how we will apply it to actually use general for some. Purposefully analogy. The idea here is just to give you a basic enough understanding of Transformers so that you can have mental model for how things work, you understand some of the basic terminology. We're not done talking about this stuff, but now we're basically start switching, looking behind the curtain to actually essentially like the user manual for how you actually use degenerates for productive purposes. Let me take a pause here. Let me see I got any comments here. I'm going to ask one clarifying question on that last slide. Wouldn't vectors not only be related but literally one would be longer and built off the other. Right? Because aren't those two sentences identical through to the R and grandmother? No, they'll be similar in the sense that they have some similar concepts. But they're not going to be identical because one of them is going to have, is going to be more focused on the Pi. Right? And if you're looking for similarity in terms of I'm looking for like my grandmother's Pi. That's the essence of my query. Going into a system. Yes, it'll find these two things. Okay. These things are in sort related, it's probability, right? It's going to basically be doing a check across an entire dataset and it's going to bubble up to the top, the things that are closest related to what it is that you're looking for. And so that's how it does it. This whole generative eye is all right. And so the way that it does that is by doing these comparisons with one another, okay? But the vector input analysis is complete. I'm thinking of my grandmother. It doesn't take the input until, you know, you have version one which stops at grandmother and the other one that continues with a post. Yes. Okay. And just to be clear here, so the input, it was not the text. This is the person thinking that I was exactly I was. So thank you for pointing out. Yes's. Not like we fed this text into the embedding vector. We're trying to come up with a representation for your thoughts, right? And that the text is just there to tell you what this person is actually thinking about as we're doing these calculations nil. Can I ask you a question quickly on slide the 29? Yeah. And we're going to go back can we go back to 29, please? Okay. Yeah. Yeah. Okay. Just to understand, the input of the transformers is a vector. Yeah. And the output is a token or it's a refine vector. Yeah, it's a token. Yeah. Okay. And the prediction is based on the previous token that they've been processed by the transformer. It is not exactly, it is a special neural network Uh huh. That is responsible for lighting up the token. Because you remember these models have ten to 20 million lines tokens. Right? And so you think about it is like ten to 20 million layer layer. It's a T million a layer that has ten to 20 million different decision trees. And what we're trying to do is we're trying to figure out which, which token is the right token in that list. And the mental model that I use for this, for better, for worse, is I have this huge wall of LED's. And I have an LED for every token it is out there. And as I'm running my queries through this, I'm running my training through this, I'm trying to figure out which LED is turning on, which will only be one, right? There's only be one best one, whichever LED turns on, that represents the actual token, that is the next prediction. Right, so in the case of accenture, that next token might be ACC, right? And then it's going to go back through the process and what I'm hoping is that the next thing that's going to light up is the right, the latter part of accentures name. And so that neural network really is responsible for distilling everything that went through the transformer architecture and coming down to the one token, that is the highest probability of completing the prediction. Thank you. Okay. Deciding fact, transformative, the model structure, the architecture, which I shared earlier was from the research paper itself. Attention is all you need, which is being referred over here. Yeah, exactly. Exactly. I saw that. But every time I look at that diagram, it. Complex and there's a lot of details. And I'm sure there's many that understand it at a low level detail. I don't understand a low level detail, but I also know that I don't need to. This is helpful for me because this demystifies it, right? It makes it less magical. Right now I'm like, okay, now I know my head, I can figure out, I can trust, I have a very basic understanding for how this stuff works. Now I can move beyond the magic and just say, hey, now let me figure out how I can actually twist and tweak and bend this thing to my will to do whatever it is that I wanted to do. This adequate question on the training process itself. When we say we train the foundation model, we're actually using billions of words and trillions of words from Wikipedia and the Internet. How do we evaluate the accuracy, how do we say whether the next token is the correctly predicted token? Because the way I understand it, these are not instructions where we have a right answer here. Keep in mind we're looking at this at a very granular level here, right? But we know if we take a statement, right, and we have an expectation for what that next word is going to be in a statement. And let's just pick a statement on here. Okay. Run it through a Let me pick on how does Jen, I work question mark. Right. This is some amongst a whole lot of other text that's on Wikipedia. I'm just picking Wikipedia because it's a very popular source text that can be used for training models. Right? There's nothing special about Wikipedia, it's just that it's voluminous. As a lot of examples, I have this Wikipedia arc that says, how does I work? And it's going to be using that data to train this network. And it's going to go through and it's going to say, okay, the first word is how I want to, I want to adjust the weights of this model so that the next recommendation is does. It goes through that first process and adjust those weights accordingly. Then it's going to repeat this process for the next word and repeat this process for the next word. You have the answer that you're looking for. It uses that answer as a verification for did you get the output that you were looking for when you ran it through the model? If you didn't, it means that you need to adapt it. It's also important to be clear here that every answer is not ever going to be 100% right. It's all about probabilities. And we have some degree of, of control through this temperature to adjust. Like, hey, how precise do we want it to be? Thanks, I have a consideration I want to share with you about this point. I think it's clear for us that the generative I is not true about one topic, about one question, because it is based on the huge amount of information that we are using in order to provide training to the generative engine. We can also identify very a particular scenario if there is a sort of a cyber attack that introduce in the network a lot of wrong information about a specific specialized topic. The risk is that our generative I will provide a completely wrong interpretation of specific question topic argument. This means that the true is not inside the generative, but the true's inside the huge amount of information that were used for the draining. Do you agree with this consideration? Yeah, absolutely. I mean, the models are not garbage in. Garbage out, Right. So for sure. Okay. Okay. Thanks. Yeah, Yeah. And I think this brings to maybe my question as well, because I see this reminds me a little bit of the machine learning models. Our prediction is going to be as good as the data we have, right? And for that we need to trade our foundation models. It's called for AI, and I believe this is going to be where our major efforts will be spent and where we need to manage our client expectations as well. So the question here I would have high level, how did we manage this with existing client if we have done this already, how we convince the client of the data needed, what kind of assessment did we do about the data we need? And of course, a little bit the process of training the foundation models. We'll touch a little bit upon that here. There's a lot to cover. A lot of details that would be too difficult to cover in my segment. I think part of it will be covered in the respective segments tomorrow. But this is a launching off point, right, for you guys to dive deeper into the topics that you think are most relevant. Once again, it's going to depend on the use case, like what are you relying? A lot of the work that's being done with generated by today, we're going to show some of the patterns here at just a moment. Is not reliant on what? Is not reliant on the knowledge that, well, I shouldn't say it's reliant on the knowledge that the models have on language, not necessarily knowledge on the facts that it might have about a particular domain. We'll architect our solutions in a way like hey, we're going to provide the knowledge through prompt engineering and things like that we're going to provide or fine tuning. We're going to provide the knowledge and we're going to use the general AI for its understanding of language. That's a very practical and very reasonable way of addressing a lot of this, right? And avoiding hallucinations, right, is by making sure that, hey, we're feeding it with the data that we wanted to process. I'll show some examples of that here in just a little bit. State tuned. Just a quick question on tying it back to the by Boost build talked in the previous and then how JNI works if we go back to the previous slide with the buy be that that's a predetermined input prompt and predetermined transformers and so on, Whereas boost customizing certain aspects of this particular process and then build, you're doing it from the ground up. It would be I think once again we, this is very focused on the transformer architectures. I'm going to cover, let me cover that a little bit later because now as we start, this is all. I don't want to say it's not hypothetical because it's actually really done. How many of these models are either built or it's inspired how many of the new models are being built. But let's, I don't think that's the right conversation to be having at this level when we're talking about the transformer architecture. Let's save that when we actually consider how to use it as part of the decision process there. It's a great question. I'm happy to answer it. I just don't want answer it prematurely. That sounds great. Thanks. All right. All right. So we got one more hand there. All right. We'll do one more and then just in the interest of time, I want to make sure that I stay on track and that I don't cut you guys short on any content. Is there any Last question, Neil, this is one question. Whenever you talk about Jeni, all our clients think about chat GPD, right? So what we've been working with, Bard and Palm two and all those things. How is this different when they say we have LLM for business, where they would train it on your data, the organization data, and then build the models on top of it. We will use the same structure. It will be prebuilt. We train it on the data instead of Wikipedia. No, it can be. There are probably some scenarios where you want to do that, but at this point it is not the primary scenario. We'll cover that. There's a technique called retrieval augmented generation, where basically it's a way for us to take our data and to use it in conjunction with the power of the LM to be able to do any of those patterns that Teresa talked about. Whether it's summarization, synthesis, generation Q and A, or whatever the case may be. We'll get into that here. Thank you. I think all roads are leading to moving forward, so let's move forward. So applying foundation models post training. All right, so we kind of give you a little bit of view what training looks like. Let's bring it up a level, right? Let's get out of the details of the transformer architecture. And now let's just think more broadly here. When we think about Teresa had a different representation of this. But you know, when we think about an application that is applying generative, we've kind distilled it down to like four main layers. And a little bit later on we're going to kind of double click even a little bit further onto showing how these different layers interact with each other. But at a high level, we have four layers of a Generate. We have the application itself, this is the user experience layer. My intent here is not for this to be a chat PT show. The only reason why I'm really bringing chat PT to light here is because it let me close this window here. It's a context that we all have available, right? There's no barrier for us to go and take things that you may have already done with chat PT as a model for how to think about what else you can do with generate AI. But when we use chat PT, the chat is our experience layer. This is how we provide inputs into chat PT, and this is how we get the outputs provided back to us. The chat pot itself is pretty dumb. It's just a chappot, right? It's just the front end. The real power lies on the other layers. One of those being the L M prompts. These are the prompts that are used. The Chatbot manages the creation of these prompts When you're typing something in Chat GPT, what you type in there is not verbatim and exclusively what goes into the foundation model to generate an answer. There is a lot of processing and preparation of that data and other things that we'll get into that go into the prompt that actually goes to the GPT 35 or the GPT four model, the L M. This is where prompt engineering is really done in the application itself. We are providing context to chat PT and Chat BT is responsible for creating these prompts for us and ultimately figuring out what data it needs from our chat history, from what we asked for, and from any filters that it's applying to actually, you know, generate the prompt that goes to the foundation model. And of course, there's application data and this is really important. We're going to dive into this in a little bit more detail about like how do I actually start bringing in leveraging my own data? Do I have to create my own model train on that data Or is there a way that I can leverage the existing models and be able to feed it the data that it can be used to answer those questions. And an example of feeding that data might be you have an article that you want summarized in chat GPT, right? If you asked chat PT just to ask you to summarize an article for you, it's going to be like, well, what article are you referring to? So you have to feed the article. And most people what they can do is they can just copy and paste the article, throw it in the chat PT. Now you've provided it with the data that it needs to actually do the processing. So that's just one example. Another common example that many people have experimented with is Github copilot, right? It's just one of the many different IDE tools that are out there. And once again, its purpose is to adapt the power of a foundation model to the use case at hand. Developers, they don't think about foundation models, they don't think about prompts. They think about hey, I need to write code, I need to explain code, I need to generate test cases. And what the IDE extension that they provide does is basically adapts the LM to those types of tasks that they're performing. And it's ultimately Github copilot that's generating the prompts. And of course you have some input into it, but your input is only part of it. The get up co pilot will generate the prompts that it will and combine any data. In this case here, copilot currently does not allow you to use external data. But it takes those things, feeds it into the foundation model to produce an output there. Let's go, let's go on to the next slide, please. Here we start getting into the different approaches of tailoring a foundation model depending on what you need. These are three different approaches that they're not mutually exclusive, right? But we're going to dive into these, each in a little bit of detail. The blueprint is the same, the flow is still the same, right? Ultimately, you have a prompt that goes into a model and that model produces an output. And we talked about earlier on when we talked about how transformers work, we have a little bit of idea of how that actually technically works. Now let's dive into the prompt engineering first. So if we can go to the next slide, we're going to cover all three of these here. You will see this slide again. Prompt Engineering here. What we're basically saying is, in order for us to tailor this foundation model, we're going to put all of our focus into the questions that we're asking. When you ask a question, you're providing three things. You're providing it the instruction, right? What is it that you actually want the LM to do for you? You're providing it with data. If there's data that you want to actually apply those instructions against. And then you're going to provide examples. And those examples are important because those examples are ways for the model to understand like what kind of output are you looking for? How do you want me to answer this question? Do you want me to generate a white paper for you or do you want me to give you a very precise answer? Or do you want me something in between? So this is really where prompt engineering is really focusing your effort on that prompt. And that's a bit of an art. I mean, if you guys have looked at things like mid journey or Dolly and you see some of the prompts that people create for creating images. I mean, there is a lot of creativity and a lot of detail that can go into those prompts in order to get in order to extract the exact output that you're looking for from given someone's access to a Thank you. Thank you. If you focus on the prompt and then ultimately you're using the existing models as they exist, whether it's burt, although most people aren't using burt anymore these days, but that doesn't preclude you from using it or if it's PT 35 or GPT four or Google code a two. Basically, all of these pre trained models, you're taking your input, you're feeding them through there, and you're generating outputs. The idea here is that whatever is being generated is being fed back. Tailor the foundation model. Otherwise, like this prompt engineer, how's that helping? The prompt engineering is order. You've created this beautiful model, you've trained it off all this Wikipedia data or whatever domain data or whatever the data is, and now you actually want to use it for a productive purpose, right? So we have this model, we instantiate it, we put an API in front of it, and now we actually want it to do something for us. How do we actually interact with it? How do we actually tell it to do something? For an example that a key approach to tailor a foundation model. I thought that's the part I'm trying to understand. So you're not tailoring model, you're, you're tailoring the output of the model. You're tailoring the use of the model. You're not tailoring the training of the model. There's no training involved here. This is a pre trained models trained by someone else. They spent the money and the days and the millions of dollars to actually create this model. Now you want to use this model to do something productive. So how do you tell the model what it is that you want to do? Well, you provide it with some instruction. You can kind of think of this a little bit like chat PT, right? How do you get something out of chat PT? It just sits there, the icons just blinking. It's waiting for you to kind of provide some input for it to do something on your behalf. That's exactly what we're talking about here. We're, we're specifying the prompt. The prompt might be, I use an example like summarizing a news article, right? The article could be like, hey, please summarize this new article for me and give me the top three bullets. Okay? That in of itself is not enough because the model is not going to know what news article you're referring to. I'm also going to paste in the news article and I'm also going to give you examples for how I would like the output formatted. And I take those three things, I feed that into the model. And then ideally, the model will produce the results that I'm looking for in here. But all of my effort is on generating that prompt. And if I didn't get what I was looking for, like I often don't get, it's an iterative process. I work it a couple of times until I get the output I'm looking for. And then once I've gotten what I'm looking for, I save that prompt so I can use that as a model or as a blueprint for other times I want to do something similar. Code summarization, same thing, right? Just a different example, right? I take, I want to use this LM to summarize a block of code, right? So what I'm going to do, and I'm going to use strictly a prompt engineering approach here. So what I'm going to do, I tell the LM, hey, please summarize this code for me and give me a description, give me a functional description of each function in the code. I'm going to tell it what I wanted to do. I'm going to provide it the code that I want it to actually apply this instruction to. And I'm going to give it examples for how I would like those descriptions to actually. I'm going to give it some example of the output that I would like so that it will try to produce something similar to that. There's also like from a data engineering standpoint, right? I'm going to tell the prompt I would like to, I have this data set, I have a data set as a CSV file. I would like you to tell me what are the top five products that were sold in the month of August. Here's an example of how I'd like you to format that, output it. Basically, I'm focusing on in the prompt providing the instruction, the data, and the examples of what I need. We go to the next slide. This is actually a summarization of that, just more textual. I don't want to get people distracted on the text quite yet, but it's basically what what I've already been talking about, which is focus on the content. In this case here, it's focusing on the content first. Then the instruction, the examples, the elements don't care what order you put things in. But this is a good example. All right, we're going to get into prompt engineering a little bit more, but there's a couple of other things to consider. There's this idea called in context learning. It's a concept where a model learns and makes predictions based off of the context that's given during it. During inference, that means basically what's given to it. In the prompt itself, you're going to hear these terms like zero shot learning, one shot learning, and F shot learning, just to help you visualize. I'm a visual learner, I visually oriented, I like building diagrams. Right? Which is why Accenture is perfect for me, right? But the idea here is, okay, in zero shot learning, I'm basically, look, I'm leaving the data out for now, right? But in zero shot learning, I'm really saying, hey, here's what I want you to do. I'm not going to tell you what the output should look like. Go ahead L M, figure it out for me. And give me something that you think is reasonable right now that's really useful. It's great for like simple examples where I'm trying to, I see the audience is asking for a break. Is there a natural time to whoever is the main facilitator? Is there a time that we want to take a quick break here? It's Lisa. It's fine. I know there's a lot of really good conversation and questions. We want to take a quick break. Maybe I'll lean on you, Neil, based on how you see the next few slides in your section. But I will try to keep it to like 10 minutes so that we can keep going. There's a lot of good contents I don't want to lose sight of. Yeah, exactly, about three more slides in this section and I think that would be a good natural break. So if we can. 58 more minutes. I think that would be a good time. All right, thank you, Neil. No problem. No problem. Okay. Zero shot learning is basically where I'm just letting the LM hey, figure out, guess for me, hey, what the output should be like. One shot learning is basically where I give it one example of the output in order to try to steer it towards the output that I'm looking for. A few shot learning is where as like what, steering it with one example is not enough. I'm going to actually give it multiple examples of what I would like to, how I would like it to produce the output and to really get a very precise output from the LM itself. Zero shot learning is really great just for like ad hoc, where I don't really know what I'm looking for. But I'm just experimenting and ideating and just trying to get a general understanding where I don't even really know what kind of outputs I'm looking for. But once I start getting a better understanding for the type of outputs that I'm looking for, and I want to apply this prompt at scale to a broad dataset. That's where I don't want that variability in the output. I want the output to be a little bit more precise. And the way that I can do that is by using one shot learning or two shot learning techniques. Where I actually embed in the prompt what it is that I'm looking for here. Next slide please. This is an interesting topic here. Even within Prompt Engineering, there are different ways that I can provide those examples. I said how providing those examples 01, multi shot, learning how they're important here is really interesting how you can use those examples to drive the model in terms of how it reasons and provides its output. In these two examples, one of the left one is just standard in context learning and the other one is more of chain of thought prompting. Before I talk about the differences, let me talk about the things that are the same. Both of these, the instruction is the same. The question that's being asked is the same. The LM is the same. The, the answer that ultimately comes out of both is the same. The difference is in how the LM was actually used to get to that answer. On the left hand side, what we're doing is that we're basically just providing a bunch of question and answer examples. We're relying on the models ability to infer from these examples what it is that we're trying to do. On the right hand side, this is what really gets interesting. You're actually, not just the examples that you're giving it are actually instructions on how to solve the problem. And these examples are the hints that the LM will use to reason the answer. When you're not getting the type of that we're looking for from an LM. That's where you start to switch to this chain of thought, prompting. And this is where you'll really start getting better results. The classic example, you'll hear headlines, problems like hey, you know what? This model can't even do math right? It's often because they've taken this approach to it where it's basically you're reliant more on the memorization of the LM rather than the ability of the LM to reason. If you take that thing question and reform, you provide it with different examples in terms of how it should approach answering these types of questions. That's when you get the output that you're looking for. It's a much deeper topic, but I just at least wanted to introduce it here. Then I'll tell you what, let's take a pause here for fine tuning. Lisa, what do you suggest Like a five minute break? All right. I will say Lisa's not going. I'll say let's take a five minute break. It'll give me a chance to kind of take a look at the questions that have come through and make sure that I embed those into the rest of my presentation. So for me it's 11 44 right now. Let's just say we come back at 11:50 So I'll give you 6 minutes and then we'll proceed. All right. Thank you everybody. Thank you. What's the preferred way in context or chain of thought prompting? Well, we're on break but I don't answer. Don't fall for it. Don't answer every time. Okay. But the training material is incredible. That's off. That's a so I wouldn't say. All right. We've got about a minute before we get started here, Keep you. Okay. All right. I think you're on near. All right. So I just got a a generate time stamp of how much time we have left, and it's about 20 minutes, and then we have 50 minutes for Q. D. I'm going to wait and I'm going to continue. So hopefully everyone is back from their breaks. If we can move on to the next slide, please. This is where we start to get into a little bit on fine tuning here. Whereas previously we're focusing on the prompt, now we're going to focus on customizing the output based on fine tuning. I'll tell you right now, it's never going to be exclusively on fine tuning. There's always going to be prompt engineering involved. When we look at fine tuning, it doesn't mean that we're minimizing or that this is an alternative to prompt engineering, because ultimately it's going to be a combination of the two. Now, there are reasons why you might want to fine tune a model. First of all, we talked about prompting and all the power and everything you do with prompting. But there's some challenges with prompting. First of all, most of the L M's, they have a limit for how much size, what the size of the prompt can be. Some of them could be 1,000 tokens, and some of them could be 8,000 or more tokens. And the size, there are token limits in terms of how, how much data I can actually put in the prompt. That's one constraint that we have to work with. Fine tuning becomes a way that we can address that constraint. Another constraint is cost, right? Sometimes the way that many of these as a service models work. When we are buying a model, we're using open AI or we're using Google. We're often paying for what's being metered is our prompt size or our token size. And the larger our prompts are, the more expensive it will be to actually process that request. There's also performance. The larger the prompt, the more resources, technical resources that are needed, the longer it might take to actually get an answer to that prompt. And then of course, we're dealing with complexity. I talked about how we have to combine this instruction data and examples all into the prompt. That is not, that's something that's often done in code and that can be a very cumbersome and it can be very difficult not combining these things, but how do we actually make sure that we have the right data and that we're pulling in the right data and we're pulling in the right examples for what the end user is actually asking for or trying to do. So there's a lot of reasons why prompt engineering by itself is not going to work. Many cases it will work. For the cases where it doesn't work, that's where we start getting into fine tuning. And what we're saying here is basically, listen the layman's way. I love layman because I am a layman. The layman's way of understanding this is I can take my data and my examples that I would have put into the prompt and instead I'm going to put those into the model themselves. So I'm going to take an existing model that has been trained on a certain dataset, has certain weights, and it's been evaluated and tested for what it was trained on. And now I'm going to adapt that model. I'm not going to create a new from scratch, I'm going to basically adapt that model by providing it with more data and more examples of what I'm looking for. The data in the examples is basically like the knowledge that I would like it to understand, whether it's a programming language or maybe it's a behavior. I want it to understand how, how to do Q and A. For example, I wanted to understand how to do advanced math or I wanted to understand how to do summarization of annual statements or whatever the case may be. I'm going to provide those inside the dataset that I will use to fine tune the model so that I don't have to have all of that in the prompt itself, maybe for the reasons I already say I want repeat myself a very simple example of this, if we go to the next slide of fine tuning is on the left hand side. I'm going to use Co generation because I had told you that's the area that I got introduced to, this's the area that I'm most comfortable talking about. We're going to do co generation with a pre trained model and I've got this new programming language called I just made it up Lang XYZ it doesn't exist. Right. And I'm going to go ahead and I'm going to ask any model out there to hey generate for me the persistence code for this language that it wasn't trained on. What I'm likely going to get out of that model is I'll get an output. Maybe it'll tell me it doesn't understand if I'm lucky. But more than likely it will produce an output for me and it'll be incorrect because it's completely made up. Or as they talk about it, it was hallucinated, right? How can I address this? Right? How am I going to actually take this model which has no knowledge of this new programming language? How am I going to take this model? How am I going to use this model for helping me generate code for this new programming language? Well, I know that I can't possibly fit all the knowledge of that programming language in the prompt itself, the really only viable option here is for me is to use a fine tuning approach. And on the right hand side, this is the fine tuned model. What I did is I took the pre trained model from the left, and then I added a whole bunch of examples of lang XYZ code so that it could understand the patterns, it could understand the terminology, it could understand the connections between the different Components of that language that when I now ask it to generate data persistence code, it has been further trained on the data that was used for fine tuning so that it can produce output that is actually meaningful to me. This is the other lever that's available to us. This is an area that often people, either they jump to thinking they need a custom model and then sometimes we can convince them like, well no, at a minimum you might need to fine tune model. But more often than not, a lot of initially what they want to do can be done with a pre trained model. Of course, I'm making generalizations there on. It's always still going to be dependent on the use case at hand. But hopefully shows a little bit for how we would use this. There was a question here, let me see. Could you please explain what the prompt is? The prompt is the question that I passed in to the L M, it's the text, if you recall. The prompt is usually going to be a combination of things. It's going to be the instruction, what is it that you want it to do? In this case, the instruction is here is to generate data persistence code for this language. The data, what I want that instruction to apply against, and then the examples for the output. Those are generally the three things that go into a prompt. All right, and so if we go to the next slide, sorry, quick question following on the prompt to bring it to life. Like for example, if I'm using chat GPT is the prompt what I type into the chat PT UI Or is the prompt what's being fed to the LL model in the background that I don't see. It's what's fed to the LM in the background. Okay. Right. Which includes, among other things, what you typed in, but it also includes the appropriate snippets from your history, your chat GPT history. And it also can include other data which I'll I'll give you some I'll describe here in just a moment. Okay. So the last one here is the last approach. We're not going to go into the details on this because this is required. This is a whole nother area of study. But it's like how can I actually forget about fine tuning model? I want to create a custom model from scratch, right? So this is basically ML engineering, where you are defining the architecture, you're defining all of the data. Obviously you're going to need a lot more data for this than you would with fine tuning. And you're taking that approach that we showed when we were training and transform architecture. You're taking that approach on whatever data sets you want. You're creating this model for whatever the domain is, whatever the intended purpose is. We'll get. I still have a little bit more to say on that when you're doing this. This is really where ML ops really starts to come into play, right? Because now you start creating models and there's a whole life cycle for creating models and this would follow that life cycle by life cycle. You can also say this is also where there's cost and complexity has to be really clear what the value proposition and what the benefit is going to be of having this custom model versus the cost of actually creating it. Just creating. Just technically creating it. Creating it's going to be a couple lines of script, but it's all the data curation that has to go into getting the data, the right data and getting it into a format that it can be used to actually train the model. Okay, moving right ahead here. Let's now immediately jump into the next slide here. I think you guys will find this worthwhile. All right. Teresa showed a variation of this slide earlier. This is, I showed you the four layers. Now we're going to double click into that a little bit more. And we're not just going to show the layers, we want to show like the interactions between the different layers. But here are the main components for any application that is leveraging generative AI. This does not represent the entire application, but this would represent how generative AI is being used by that application. On the left hand side, we have this application. I already talked about this before. This is to use your experience layer, right? This is how you actually interact with the application. Is it an API? Is it a Chatbot? Is it some other orchestrated process? Is it my mobile phone, text app, whatever the mechanism for interacting with that? That's what's covered by the application here. Then I have a variety of patterns. Now, pattern is the different ways of getting the type of output. Pattern could be like, am I trying to do generation, summarization, Q, and a synthesis, et cetera. So there are different ways of applying those patterns. And before you even generate the prompt, you have to understand what pattern it is you're applying to the question at hand. And then possibly retrieve some data that's needed to support that question or to support that request. And then passing that to the L M, the LM can produce the output that's being looked for. All right? I don't want to dwell on this too much. This was already introduced and this is just really high level. I'm going to provide you some more concrete examples of this here in just a moment. All right. Moving ahead to the next slide please. What do we have here? Yeah. Okay. A lot of focus is on how do we actually take enterprise data. We're not interested in generating a new model. We're trying to figure out how we can take our enterprise data or our enterprise software systems, and how can we integrate them, or how can we make them a part of a generative AI solution? This is where we start talking about our four key archetypes. Let's move on to the next slide here. This is where we'll actually start talking about them in a little bit more detail. I'm a boxes and arrows guy. As I said, this is how I visualize it here. But we'll see examples of these here in just a moment. But the first archetype is where we're only application can think of it as a chat bot if you want to right to chat PT. But recognizing that that's just one of many different applications, ways that you can interact with general. But in this example here I have a that query goes to the LM and I'm reliant on the LM's knowledge and the LM's understanding of language to produce an output. Here there's really the corpus of knowledge that is being brought to bear for this archetype is the training data that went into the LLM. All right, the second archetype here is saying, okay, we have some enterprise data stores that we want to actually bring into the mix here. The flow, the way it works is I have an application. That application, I have my query. My query goes to the LM. And the LM does two things here. The LM says, okay, let me take a look at this query. And let me figure out what type of data that I need and what data store I'm going to go retrieve it from, from the list of data stores that are available to me. Then the data store, there's a retrieval that happens at that data store. This is where we start getting into retrieval. Augmented generation or the acronyms rag. But we start retrieving this data and then this data then goes back into the LM. And the LM says, okay, I got the original question. I have the relevant data that I need. Now, I'm going to go ahead and synthesize it or generate or predict the answer from that information, right? It could be summarizing a article like I talked about. Or generating new code or, or generating summaries of code or whatever the case may be. So this is really where it's more of like a data store, right? It's a database and there's a variety of different databases that could be brought to bear here. We're going to talk about vector databases here in just a moment, which is a term you may have heard about. The third archetype is for data where user data is being used with external processing. Where basically I don't have, it's not a data store, but I have an API for a system that I need to bring into the mix. For example, maybe it's a HR system and I need to be able to pull someone's work history. I'm probably not going to have that in a data store, I'm probably going to have that as an API endpoint. I need to pull that into this. And then the fourth one is really just saying, hey, sometimes it's a combination of these different things. So let's move to the next slide. Let me show you some kind of concrete examples. I'm going to use chat PT here, not because I'm trying to promote PT. We all understand that, you know, we have restrictions with an accenture to use chat PT. We can never use it for uploading our own private, our company data, our clients data, et cetera. But this is a tool that is immediately available to you. Anyone can go to Chat PT, sign up for it, and can start experimenting with this, right? But I want to show you each of those archetypes just and chat PT as a way to kind of reinforce these concepts and so you can kind of experience them firsthand. So in this first one here, I'm going to go and ask chat PT just I'm using GPT four here, I'm going to ask it a basic questions. Who's the first President of the United States? I'm not providing any other context, I'm not providing it with any other data. I'm reliant on that first one on what does the LM understand about the history of the United States and who the first president was. Right. And it turns out it has actually a lot of that knowledge in there. So it was able to provide me with a good answer. So let's move on to data retrieval. Data retrieval here is a little different here. I'm saying, okay, who won the tennis finals of the US Open? Now we all know that, well, maybe we don't know the LM models. They were trained a year ago. They're not going to have any understanding of what, they're not going to have any understanding of the finals of the US Open. Now, I could have gone to the US Open web website and copied that content and put it in my prompt, and they could have figured that out for me. But I wanted something more automated and that's where I'm using retrieval in chat PT. There's this concept of plug ins, and I enabled the Wikipedia plug in and that Wikipedia plug in exactly. I knew the answer, but so I could verify it. But that Wikipedia plug in is something that's being used by chat PT that says, hey, when someone asked me a question that's current information, I'm going to go to Wikipedia and see if I can find that information over there. And it turned out it did find it. It did a look up, it did a retrieval of an external system. In this case, it was Wikipedia, It the summary page that had the information needed. And then it took that along with my original question in order to provide me with the answer here. In this case, here, this is a very basic example of retrieval. We can do this on our own with our own private enterprise data stores as well. Next slide please. The next one here is around external processing. I can go to chat PT for example. And I can say, hey, what are the flights from Tampa to Boston on the morning of September 15? When I took the screenshot, it was in the future, now it's in the past. What I also did is, prior to asking this question, I enabled the Expedia plug in on chat PT. When I asked this question, PD says, hey, listen, this person is asking about travel. Let me take a look at the list of plug ins that they've enabled. Oh, they've got to plug in for Expedia. Let me use that one. And it went ahead and invoked the API on Expedia. Came back with some suggestions as to flights that were available to me. Once again, this information is not in the LM itself, GPT 435. It doesn't have this type of information in it. This is an example where I needed to retrieve it not from an external data store, but I actually needed to retrieve it from an external system via an API endpoint. It was all done automatically for me, right? I was able to enable that here, it was just a click by enabling a couple of plug ins. Then the last one here is I want to show you an example of these things in combination, right? In this case, for this last scenario, I asked as like hey, enable the Wikipedia, the Expedia plug in. And I asked the questions like, I live in St. ⁇ Petersburg, Florida, what are the weekend air travel options to visit the city of this year's NCAA Men's Final champions? There's a lot of details in there and it had to actually do a couple of different things in order to provide me with the travel options. First of all, it had to figure out, okay, where was the basketball champions actually held? It used Wikipedia to figure that out, and came back and found the city that I needed to travel to. And then it said, okay, the next step of the process is for me to go to the Expedia and query the travel options for going to Expedia and then generating. I couldn't show here, but a little lower down it showed me the travel options for that. So these are those basic four archetypes brought to light using this kind of Chachi PT idea. All right, Neil, a question, Neil, if you don't mind. In the archetype overview you showed a couple of minutes ago, you had the arrows going from the LLM to the external source or to the data retrieval. Is it the LLM that's doing the query or are we having the prompt doing these queries and then send this to the LLM, the policy manager. Let's go to the next slide and we'll go into details on that. I'm going to take the liberty here, like we have 1 minute left of my allotted time, but I have 15 minutes for questions. I'm going to take the liberty here of taking five more minutes to get run through this content real quick. I think this is really important here. This is actually now double clicking into that in a little bit more detail, right? This is also a pattern that is actually implemented by accentures, right? You can go here and experiment with accenture content, but the idea here is that the LM is being used in several different stages. That previous slide, I tried to show it all in one, just very simplistic. But what's actually happening here in a Chatbot is when I take that I submit the query. This query we're dividing and conquering here, right? We're saying, okay, let me take a look at this query. Let me go and search against a vector data store which has this content that I'm looking for. I'm using the LM here to do some of the embedding work. The LM is used here. The LM is also used here to actually generate the final response. The LM comes into play in multiple parts, but to answer your question, it's not the LM that's making the decisions about going to retrieve this. It's the system, the software system that I'm developing that's making the decision. And it's orchestrating the process of retrieving the data and returning the results and combining it with the original prompt, et cetera, in order to generate the output that I'm looking for. Our focus should not be just on the LM or even just be on the prompt because it's often going to be a software system. And there are frameworks like Lang Chain Semantic Kernel from Microsoft and others that are really providing you with the abstractions to be able to build these types of things even more easily. Of course you can just use standard Python code or whatever programming language you prefer, but they provide frameworks that make this even easier for you to do this particular pattern. I'm going to have to skip a couple of slides here, I realize because I'm going a little long, actually go back one more second. Pattern. This pattern is really important here. This pattern is probably the predominant pattern that's being immediately applied in a lot of use cases. Because what this is basically saying is, listen, I have a whole bunch of data. It can be code, it can be documentation, it could be legal papers, it could be architecture. I have all of this content and I need to get this content into a format where I can do Where I need to get it into a vector format so that I can do embedding search against it. And we already talked about what embeddings are right now. I'm trying to figure out like between thinking about my grandmother or I'm thinking about Pi. This is where that magic is happening. I'm having this query be embedded and I'm doing a search against that vector store so that I can then find the most relevant content from all of my unstructured content that I fed into this vector database. I'm going to find all the content that I have and I'm going to try to then take the top results of that content and use that in the prompt. Now know prompt includes instruct in the instruction the data and the examples. This is how I'm retrieving that data that's going to get fed into the LM to produce the output that I'm looking for. When I go to eccentric Aspot and say, hey, I need to know, I need to find a POV on this topic. How is Eccentric Aspot doing? Well, basically what they did is they took all of the eccentric content. Not all of it, but whatever was in the scope of their application, they took the content, they put it all into a vector database, and then they put a chat. They built a system to put a chat bot in front of it for people to interact with this system that is going to go and basically be searching against this vector to find the relevant content to ultimately answer the question that you're asking from, from Accentures portal. Let's move on to the next slide here. Here is basically how I'm showing how that same thing applies to code completion and co generation. I talked about this a little bit. I'm not going to be able to go into details now, but you'll have this as reference material. Next slide please. This one here is an example that we're using as a code advisor to understand your legacy system, right? What if I took this pattern? This pattern should start looking familiar to you, right? I've got these vector data stores here. And what am I going to do? What assets do I have from my existing or my legacy systems? I have code, I have documentation. I'm going to press all of that stuff and I'm going to use the LM to do some of that pre processing. By the way, I'm going to press that information, get it into a format where I can then do vector based similarity search to be able to support a chat bot that's going to enable a developer or a quality engineer to be able to ask questions against their code base and define snippets of code using natural language. Or to understand the data flow through an application by creating queries using the chatbox. Next slide, please. Yeah, at least I think starting to play the music hates Neil, it's time to get offstage. Okay. This is just a double click into some of the details around that previous example where he actually, what are the techniques for actually taking my legacy content and getting it? This is just for that domain, right? You could apply this to any other domain. How do I take that legacy content and get it into a vector database? Next slide, please leave this here. There are other resources where you can double click in this, but this is the standard capability model. When you're going to do generative I, what are the different capabilities that you need to have? First of all, you need to a model, sometimes they're going to be customized models, you're going to need infrastructure, you're going to need prompt engineering capabilities. You're going to need NAI Ops which is like basically a rebranding of ML Ops but for generative I use cases and then you're going to obviously to have responsibility and security embedded into that capability as well. And then the next slide really just summarizes some of the logos as you start thinking about different vendors that you might hear about. Like where do they play in this capability map? This gives you some idea as to where they fit in. This is useful, is constantly evolving, right? Because they were seeing a lot of, a lot of new vendor space. Then what else do I have here? I'll go, if we can just jump to, I'll wrap up on slide 51. If we can go to slide 51, keep going. Obviously someone changed the slides from when I looked at it last. Okay, here this is really just starting to say, okay, there are orchestration is really important here. I talked about how frameworks like Lang chain and Santa kernel and there's many others that are emerging. They're all really targeting this middle layer here, right? Which is how can I create the right abstractions to be able to pull together the capabilities of an LM to deliver on a particular activity or task. Some of them are going to be very static on the left hand side of the agent, less ones. The orchestration is going to be done in code and it's going to be very deliberate in the code that's written. Some of the interesting things that we're seeing right now is really doing these things in a more autonomous way, basically enabling generative AI agents with tools and capabilities that Very similar to what I showed you with chat chiPT, right? Where I asked it a very general question and it automatically came up with a plan for how it was going to solve my problem for getting to the NCAA basketball city. Right. It decided on its own that, hey, I'm going to go look up Wikipedia to get that information, that part of the information. And then I'm going to go to Expedia to actually get the travel information. All of that was done. I didn't direct it to do that. It figured all that stuff on its own autonomously, be on the lookout for that. Probably an area that we're going to see a lot of innovation over the next couple of months. All right. I've done a poor job of managing my time, but I'll blame it on the break that you guys had to take. But anyway, that's where I'm going to stop. Of course, I probably could have spent another hour here. But I want to make sure. I believe we have until 12. We have about 7 minutes for questions. What can I answer for you? The architecture that is showed you have the vector database in the D. If I'm trying to do a code generation, my first option would be like if I have a reusable piece of code, the enterprise before I hit the LLM, can I vectorize my existing code base? And that would be hit first and then it goes to LLM. How does that work? Yeah, let's go back. Let's go back. If we can scroll back a couple slides, I think I think you get the slides. Yes, you do. Go back. Go back one more. Couple more. Okay, go back here. What we're basically trying to do is next slide, What you're basically trying to do is you're trying to address this use case here. And you're saying like, okay, how do I go about taking this stuff and getting it into a format here? I'm literally taking code, but let me go to the next slide here and describe this in 1 minute, next slide. Thank you. All right, so here's an example, okay? If I want to understand my code, what are the assets that I have available to me? I have a code, right? One of the things that I can do is I can take my code itself, I can embed it, and I can store the code and the embeddings in a vector database. This then will enable me to answer questions like, hey, find me the code that uses in this library, or find me the code that is similar to this code. So those are the types of queries that it's going to be able to answer by using this technique. Another something else that I can do is I can say, let me take this code, let me run it through an L M and have it create a natural language summary of that code, a functional summary of what that code does. And then I take that functional summary, I embed it, and I store it in a vector database. Now I have a vector store that I can use for answering questions like, hey, find me the code that enables end users to set application preferences, right? I'm asking more natural language questions. I'm not asking code specific questions because I generated natural language descriptions using the LM. The next thing I could do is I could take my documentation, my training documentation, transcripts, whatever information I have available to me. I can embed that. Now we all understand what embedding is, right? I can embed that, store that in another vector data store That's going to enable me to ask questions like, hey, I don't want to know how the code was implemented. I want to understand from an end user's perspective, how should they be using this code or how should they be using this application? I won't go into every technique here, but the point here is like these different techniques exist for getting this information into a store that can then be used to support that code advisory system that we talked about previously. Once again, I don't want the takeaway of this meeting to be, hey, it's all about code advisor and that's the only use case this applies. That just happens to be the domain that I'm showing you examples for, but it can apply to any domain. Okay, other question, this is Annie, I just quick question on industry specifics, right? Like do you see a similar kind of use cases by industry like what I mean is whether it's a, utilities or banking, or insurance. A similar kind of use cases, that's part A and B is do we plan to have a custom trained LLM models which are ready to sell to the C? Yes. Yes. I think it's inevitable that we will have custom trained models and I think there's some work going on with the extension right now to understand what we think those might be. You know, nothing definitive. But I think like many organizations, I think, you know, we're large enough where there are things that we can do that are very unique to Accenture, for example, right? Accenture has a software business. We have a lot of software that we've developed. We have a lot of code that's available to us. What if we created a LM for coding that was trained off of all of the ecrire code? And why would that be important? Well, because that eccentric code is going to represent accenture best practices and it's going to represent eccentric standards and it's going to represent Accenture libraries and things like that. Those are things that the LM's are not going to learn from looking at Github repositories, right? So That's just a very simple example of something that we could possibly do for creating a custom code LM from a domain perspective. Absolutely. I think there's all kinds of different things that we can do there, even within co generation. And you can tell where my preferences are and where I'm comfortable talking about, right, Which is co generation. But even within co generation itself, like we have systems, we have assets or accelerators that are being built right now that are going to enable you to do more end to end co generation. Uh, I think you're going to probably hear about one of them tomorrow. It's called Gen Light. And what this accelerator does is it basically brings general through the entire process from requirements all the way to deployment. And there's an element of that around domain, right, where we are using a domain knowledge base to be able to drive the generation of the Epics into features. And the features into designs, and the designs into code, et cetera. So there's all kinds of opportunities there to create specialized models on accent domain knowledge, technical knowledge, et cetera. Thank you, Neil. Hi. If we can go back to the architecture for vector search right the first grade, whenever we probably envision a road map to build an application like this, is it the enterprise data retrieval side that should be the focus? I mean, we should have probably a mature enterprise oral mechanism in place in order to have an effective NAI and App work, which will be effective. Before you answer, just one more question. What do you mean by memory here and the layers in under? Generative is L, the only part which is off the shelf, available to us. Everything else needs to be built. Probably touch upon what needs to be built. This architecture memory is just a fancy name for like chat history, right? When you ask, when you talk to chat GPT, you don't have to constantly be asking the same question or providing the same context over and over again in a given thread. It's basically where these tools, the store the information that it based on the interaction that you've had. Your question. I think that everything here on the left Accenture has built many, many times. And we already have starter code or even reusable code that can be already leveraged to do the things on the left hand side here where we really need to focus our efforts on is finding the use cases and ingesting pre processing that domain knowledge into a format that we can use for doing similarity search against. That's I think where the bulk of the work is. That's really the left hand side. You can just consider that to be done right there. Rest is to go on Azure right now where I can probably build this system a couple of hours. Obviously it's not enterprise and all fully secure and all that. But creating that stuff on the left hand side is easy. What's not easy is figuring out what data do I need, how do I prepare that data, and how do I get it into a format that I can use it? Yeah, when we build a roadmap for creating an application, the right hand side is something that we need to invest as probably laying out data foundation, data segregation, the trial mechanisms. Then the middle layer is something which is probably easier part, right? Yeah, it is easier because we've already done it 1 million times in different domains. 