Now I'm going to go forward and talk about some patterns that we're seeing with generative and how we're applying them, right? Generative, the ability to generate this content. There's a couple of key patterns, right? You can use it to generate the content. You can use it to maybe synthesize and analyze and summarize content. You could use it to have a question and answer type format. But these patterns, in our case, we're applying it now to this technology delivery life cycle, TDLC. And in this case, we're looking at the most impactful parts of the TDLC that these generative patterns could impact actively. Many of our partners talk about code and development. Code generation. Code analysis, possibly documentation of the code unit test, case generation. That's where a lot of these tools in technology delivery, things like co pilot, things like code whisperer Cody. That's where a lot of the tension in the market has been. But as we look at how generative AI applies across the end to end life cycle, you see there's a lot of other parts where this ability to generate content and this ability to predict would also apply even at the beginning of a project when you're designing your requirements. If you need to also summarize your requirements, it could be through even the knowledge transfer, right when we start a client project. There might be, I might have lost the screen share, I don't know if that's happened for anybody else, but when we start a beginning of a project, there's a knowledge transfer phase. Or when somebody is coming new, in that case, with the knowledge transfer. An example of using NAI might be that you're going to be able to summarize and guide that new person coming on board. How you might have done knowledge transfer before would be starting a project. I might have someone like a lo do a voice call for me. He's going to walk me through the documents. Then I'm left to look through the documents and learn the assignment on my own. With generative AI and with some of the tools that we've built in Jed Wizard, we're able to now one go from that initial call to voice to text conversion and from that voice to text. I can combine this with a lot of the other documents to create using generative AI systems of understanding document. Right. For I'm sorry to interrupt, we lost the screen share quality. Yeah, I know I mentioned that as well but I'm not driving the screen. Who is because we're missing what you're trying to show us? I'm actually telling another story because the screen is not there. We are working to solve it will be Vacs. I didn't continue. I was just, you could see it. Okay, so I was still on the slide and I guess I was saying that a lot of the focus has been on develop, right? We see the end to end, giving an example within knowledge transfer as one of the things that generative AI is going to make a big impact on, right? So what you might have done before, you might have to walk through the documents on your own. With generative AI, I can take the documents, take this voice call that we convert to text, put it together with generative I and the documents and generate a custom systems of understanding document for me, right? I can also act as a Q and A as I'm learning the system to be able to understand what I need to do, right, or to really guide me as I'm trying to understand. That's a different type of work that we see that's much more common and quite impactful. In addition to things like the co generation, that same capability, the ability to synthesize and to guide and to really direct the user towards what's important, also applies within BPMSright. You can't imagine any of these functions as I'm looking at a resume or I'm looking at something within my supply chain and demand management. I want to be able to have summarized at my fingertips any questions that I might have about the scenario. It's not really just about productivity improvement, similar to the co generation example, right? A lot of this is giving the information to the human user so that they can just be much, much more efficient at solving the task at hand. And even really directing them to exactly what needs to be done, right? So there's a lot of new value that's created in addition to what we talk about, again when we think about co generation. This is when we do business process as a managed service. We might manage these processes on behalf of our clients. If we now talk about these approaches for enabling foundation models, I mentioned that there's a lot of services that already exist where these models are available. But we can think about three general types, by a boost and a build. I like to think about them a little bit like in Cloud, where I think about software as a service, platform as a service, and infrastructure as a service. We know how, um, are choices that are made in terms of being able to apply something out of the box versus the ability to customize but take on more responsibility right within the by category. You may be able to buy a pre existing, not just model but application, something like Microsoft Co Pilot. This is something where the application is even presented. The end user is not really working with the foundation models on their own. They're not even writing prompts, right? It's embedded within the tools that we see. A lot of the tools that we see 365 co pilot or within our Github, co pilot for coding. That's an easy way to apply and it's low customization though. The middle category, this boost capability, this is a capability where we might want to add our own data, our own workflows, our own ways of interacting with the model. This one would be where we would be able to take a pre trained model or an existing model and fine tune it. We're going to now be able to have more custom accenture experience. And that's a lot of what we're starting to do with our own tools. But again, this has a trade off that's more responsibility for us then eventually we might need to build from the ground up. Possibly. But more likely it would be taking a pre trained model, open model with open source data. We can now create a custom center model as an example or a custom model for our clients. As we do this particular one, we're going to need data for it. I've heard from co pilot Gi Hub, that they think that we should have at least 10 million lines of code and not just random bits of code. If you were going to do something for software, right, you would have things within the same good standards that illustrate what we want to do. And in this case, again, it's the most work to do this. But the positives are that you really can control exactly the data that's being used. You could control exactly when it's being versioned. You could control and have a lot of your standards in your systems built within the model itself. As we think about these three patterns, by the boost in the build, there's a lot of vendors that already exist, especially within this technology delivery life cycle space, right? Most of the tool, again, code specific tools that you could apply, those are going to be in this category, there's not a lot of customization. These tools are using the data within the IDE when it's software development to be able to better provide context and advice about what you should be doing. The boost again, I mentioned that's where Accenture tools are applied a lot. This is where we're bringing our own knowledge corpus of the client at hand. Right, not just their code but their system architecture, their business process, their logs. And bringing that together alongside with our own IP around what we believe is needed for that step in the process to create this own custom type tooling. Then all the way through to this final one with the build. And we have run at least the technical build using an open source model that we were able to build. And again, just show that we could now have something that has a hub, co pilot like development experience. But it would be within our own control of exactly the data that's being used there. If I go forward one slide, I'm going to walk through each of these categories a little bit more detail, but as we think about the, by really thinking about the types of coverage that. Um, these tools would offer out of the box. Right. If we look at the languages that eccentric trains are people on, there's about 17 of those in those languages. We could see right away that it's not equally covered. Right, And in some cases, these languages are not fully covered. A lot of the modern application development languages, things like Javascript, Python, Java, have good coverage. And again, this goes back to the corpus of data on which the model is trained. As soon as we get into things like Q, C sharp, and then definitely into things like, we see that the coverage is not as complete, right? In this case, the filled box is something that has more complete coverage. The lighter the box, the less of the coverage of those particular languages. We do also have other unique tools coming out besides code in the space, things around service now for case management, things around data, AI data breaks their house IQ. And then prophecy specific to ETL pipelines. But we see here that the coverage is just not as complete as soon as we look at some of these different types of tools. If we go forward one more slide, now if we think about the boost category, and this is where, again, a lot of our own custom tools are applying. And as we talk to our clients, for them to think about the most benefit of NAI, this might be a place where they also look at, in addition to buying some of these tools where it does apply. Right in the middle we have our large language model, so many of our tools we're using as your open AI. Then what we're trying to do, again, is to generate, in addition to code, other things like a ticket resolution scripts that Q and Q and insights for somebody in the knowledge management example I was giving earlier. To make this happen, we need to really focus on creating a knowledge base, right? The knowledge base that now has that client specific context that we're going to use with this retrieval, augmented generation pattern, with that LLM in the middle. That now can use the LLM to answer a generic question. And then we're going to be able to add that client specific to the answer the question to be able to generate some of these outputs. If we go forward one more slide we talk about the last case is that I mentioned that we might even build task specific models. The future that many envision is where you have different models that do different things. As you're open AI, it's probably still the best for things like a broad, general set of questions. But engineering to have these more smaller sub 10 billion parameter LLMs that are going to be more fine tuned for the task. If we go forward one more slide, I want to finish with how to scale, right? We do think about these three phases and thinking about having them happen a bit in parallel, right now, use and buy the tools where it makes sense, right, to be able to apply that right away the next. And this is where we're going to boost and creating a very customized client specific co pilot for that client context. A lot of the work here is about bringing that data together to create that knowledge corpus. And then creating the application specific to the end user. Then finally, the AI powered development, right, to think about how to change the experience, right? That it's not just doing the process the same way that we did. But we might be using generative AI as much of the development as possible, right? Starting even with that's generate the requirements. If I see how somebody is using the application, I might be able to generate the requirements that are needed to the plan to bill, generate parts of the bill, generate the tests, right? You could generate all those and begin able to automate all those to be able to change. So some of what we're looking at here is also the ability to even create our own models. I'm going to pause here. I think I saw a lot of questions in the chat. We could either have people come off, I don't know if that's possible, or should I could try to scroll up and take a look. He sa can you hear me? I can. Cool. We can come off mute and talk to you. By the way, I know Jason raised the question. Maybe I can wait and go after Jason. Go ahead, Tanita. Thanks. So this is really good information. Thank you. And wondering one question is you mentioned about we can generate the test cases out of the code and using generate AI. Is that something we have done in the lab or have we done that with any client? Any examples that you can show us to generate the code from our own LLM, we have both the ability to generate, actually I'm interested more in generating the test cases right from the code that we are building. Or if we have code existing already and modifying it. How can we generate the test cases out of that? Is that possible using NAI? Yes, I think some of the unit test cases are appearing in some of the vendor provided tools like with Co pilot. I think our ability to generate some of the test cases. We have some demos on that. Some of the work that we're doing is really to come from code. And I mentioned some of the powers in the summarization. Could we, as part of an existing code base, try to generate the requirements Right, coming out of it? And then you can use those requirements then to generate code. Yes, we're starting and we have some demos with that. I'm sorry, I'm going to need to step out. I'm feeling a little ill. Neil, do you mind actually taking some of these questions? Problem. I'll do my best. Apologize. No problem. I'm not a Teresa, but I will do my best. I just to follow up to the last question. So using generative AI to generate tests from code, it's very common use case. A lot of the products Git Hub, co pilot tab nine as your code whisper. I mean all of these products are already enabling you to do that kind of functionality. And of course that you'll hear a term regularly that's a augment versus automate, right? That's very much, there's a couple of different approaches to go about this. One approach is to take the kind of like the augmentation route, where you're basically giving developers or practitioners in any given domain. You're giving them some tools to do certain part of what they do more quickly. Basically, a co pilot, someone, a virtual assistant, that's looking over their shoulder. Helping, helping them out on a task by task basis. At the same time, there's also plenty of opportunities and extent is doing some really interesting things around this. As well as taking more of like an end to end view of how do we go about testing and how do we actually then apply generated to automate more of the process. We're doing a lot on both fronts and there's a lot there. All right. I can't see how I see one person would end up. Is that Jason? Yeah. Question for me. I really like the last slide. There's a ton of possibility here with these tools. And as technologists, most of us want to go, go, go and try to quickly POC, and get something going with our teams. But one of the things that I think about, and I know our clients think about, is IP protection. When we start to use some of these tools that generate code and suggest code snippets and things like that, right? There's a lot of concern that either we're going to snag somebody's code that's proprietary or we're going to generate code that somehow gets back into the model and gets out there. Right. It's a big concern. The slide listed now next and all of that, I'm just curious. I saw a lot of heads nodding, so I'm assuming this is a concern that's fairly broad, but I'm curious how you've seen clients or even Accenture. Right? We start to adopt some of these tools and how we're building in the right safeguards to protect IP. Yeah, absolutely. It's a great question. And it applies in every domain, and I'm going to speak to it in a domain that I'm familiar with, which is with just code generation and Github, copilot, and Tab mines and things like that. And so you know, you bring up really good points. Those are definitely kind of considerations and sometimes they are also, you know, obstacles to getting started. The good news is that there's a variety of Options available, right? Some vendors do. Some vendors, the way that they provide their service requires that you are sending fragments of your code or bits of your code, or more generally your data. You're sending it to that provider in order for it to do some sort of inference. In the case of co generation, that inference is to generate code for you or maybe generate a code summary for you. So none of that stuff's really happening on the developer's workstation that's all being sent remotely behind the scenes to a remote server. And then of course, there's some concerns like, okay, what is happening with that code that's happening on that remote server? Is it being collected, is it being used to train the model, et cetera. So, so for those cases maybe that particular vendor is not the right vendor. Or you have to double click and really understand what that vendor is doing and understand what type of certifications they have and what type of assurances they can give you that, hey, they are in fact not going to be using that code in the purposes for the purposes that you don't want them to use it for. The other aspect of that is that there's other vendors out there that give you more options. There are commercial vendors. For example, we with an Accenture. We have an instance of another tool very similar to get copilots called tab nine. We actually have our own private hosted instance of it, right? So none of our data when our developers, none of it's leaving our, our fence, right? It's all staying on our servers. And so that gives us an additional assurance that, hey, we're protecting the code that is being sent behind the scenes to this remote service for co generation. And then there's also a variety of open source models as well where we've demonstrated we can run these models on our own infrastructure in the cloud and be able, and be able to use those as code completion tools in the ID. So there's a spectrum in there. And as we start to kind of double crick into even just something as simple as code completion. It sounds so basic. And it would seem like, hey, all these tools, it's just basically they all do the same thing architecturally. They're a little bit different and those different architectures are basically reflect their opinions on how they want to provide safety and assurances to two organizations. So yeah. Yeah. I appreciate the answer. I mean, it kind of takes me back to another slide that Teresa talked to which is the approach. Right. You know, do you buy it right If it's something that's really not proprietary at all. Right. I mean, it's just, you know, I mean it's just a service you're going to get to transcribe something or summarize something or whatever, right? Versus something that's really near and dear to you that you probably want to keep in your four walls. I think there's complexities with that as well, right? I mean, just if you take a large learning model and you customize it and tweak it, then how do you get updates? You know, what benefit do you get? And so I mean, there's all kinds of things to think about there, but I appreciate appreciate we're going to dip into this a little bit more in my portion of the presentation which is coming up after this. But yeah, good input into kind of how I will describe some things, so All right, let me see what other questions here is. Anybody else have a question they want to ask live? Otherwise, L directs me the side. Just a very quick question. As soon as you start working with the clients or you start identifying the use cases, potential use cases, the very first thing which hits you as a technologist is that there are 100,000 ways of solutioning it. There might be ways which might be more cost effective. How do we enable the people around us, the team around us in terms of finding out that whether LLM is the right approach to a particular use case or not. I think trainings like this will go a long way in helping that right demystifying a little bit and giving a little bit more detail and context and understanding. We're going to be talking about several different ways that you can customize an L M. An example I'll give you, people will often jump to the conclusion that I have, my use case is so specialized that I'm going to need at L M. And they just everything else they just kind of ignore. They're like we need a custom LM. And anything short of that is just not going to work for us. Well, when in reality when you really double click and you really understand what are the other options available like prompt engineering and fine tuning, You're like, okay, I don't actually need to create a model from scratch. And those represent three different ways that I can go about tackling the same problem. And within those three different ways there's considerably differences in terms of cost and complexity, right? And so it's around understanding that at a level of detail. I think that will enable you to kind of navigate, you know, what is the right pattern to use for the task at hand. I hope that helps me. The sessions like this are going to start. I think that part of the purposes of go back to what Elkind goals were, I think part of the purpose is, is for us to kind like internalize this, right? We aren't going to be able to accomplish this through a decision tree, right? It's going to need to be something, we're going to have to have an understanding of the technology to be able to To navigate the myriad of options that exist. And guess what? The options are just exploding, right? So we're part of the challenge right now is just there's almost too many options, right? And so we have to kind categorize them. And then once we categorize them, then we kind of double click into, hey, what are the specific tools, or solutions, or approaches, or patterns that I want to use for this particular use case. Which is why often when I'm starting a conversation, it's like when people are asking like, well, what's the best way? I'm going to always give the same answer. What are you trying to do, right? What are you trying to accomplish? Because that will drive what you pick or what you fine tune or what you build from scratch. All right. I see another, I can't see the entire, I see a hand up under my view all here, but I don't know who it is. Okay. I will go next at this side. So my question is that in multiple client discussion, right, it is coming up again and again. They want to go through the app modernization process. And their biggest challenge today is because of the legacy application system, the legacy code base. They want to do the code migration from the legacy system to the new system. For example, if they have something in C and they want to bring something back in the Python. So is there a possibility or solution available that we can help client to code conversion? Yeah. Okay. I'll start with anything that involves language, there is an opportunity to use general. You just have to accept that the answer is always going to be yes. I think the more specific question that you're asking is probably like, how would we actually do this, right? I mean, I'm going to assume most people in the audience have already at least experimented or tried chat PT, SDLC developers. What's probably one of the first things that they do is they take some code they pasted in chat PT and they ask it to client code or eccentric code, but just sample code. They take it, put it in a chat PT and they ask for a description, or they ask it to translate and you're going to be, hey, you'll get some output right now that's not scalable, right? So the question is like, how do you do it and how do you do it at scale? I'm going to touch upon this a little bit as we talk about some of the patterns. One of the patterns that I will bring to light is originated from looking at how we might apply this to modernization. But before we maybe dive in deeper into that topic, I'd like to have the opportunity to kind of present my content because that'll give the background and then we can double click into that short answer is yes. And I can assure you that our kind of our modernization practice with an Accenture is already looking at this. They're already figuring out. Okay, let's take a look at how we do modernization. Whether it's, I think you said C sharp or Ball mainframe modernization. How do we take our modernization approach? Where in those set of tasks and things that we do, where can we apply generative AI to help with some part, make some part of that process easier? And then the next thing is like, okay, not only how can we make it easier, but is there a way that we can completely reimagine the way that we would do this with generative AI in a more automated way. So those are all things that are being actively worked upon. Area is one of the areas is really the biggest challenge areas. And this is something I'll go into just a little bit later on this morning is really around understanding the legacy system. How do you use generator to get insights? Because a lot of times these legacy systems aren't very well documented or there isn't good comments in the code. So how can we leverage it? Just to start off with, before we think about modernization, we have to have a good understanding of the legacy system. Is there a way that we can use generated I, not to generate new code, but is there a way that we can leverage to take existing code and give us an understanding and a description of what all this code does, what the interdependencies are, what's the test coverage and things like that. I want to be if we have a keeping track of the time. So I want to make sure that we stick to our agenda. I'm happy to continue talking here, but I want to just make sure that someone other than me is being mindful of the time. Neil, I'm just going to one question because a lot has been answered by the questions by Jason and sank. Thank you. These tools that you have mentioned that Accenture has built, is it possible to probably deep dive into those and look at what has really gone behind as part of coding and the efforts that probably go into creating a tool like this. Yeah, if you can share contact or details where we can do a deep dive. I think part of it I think you have a couple of questions. First of all, this first session is a lo could mention is really around just getting us on the commonalities in place. Tomorrow is where we kind of start diving into a little bit more operations versus technology. And each of those groups, each of those sessions I'm not a part of, by the way. But each of those sessions is going to be kind of double clicking into the assets and the approaches for that particular domain. I think what you're asking is probably part of just getting general awareness for, hey, what assets do we have and what's on the horizon. Correct. So I think that's definitely going to be covered. I think what you're asking is like I'd also like to understand a little bit how those things were implemented. Right. I presume because you want to earn or you want to be able to maybe figure out how can I take this pattern and apply it in a different context. So today I will be talking about patterns and so I think maybe you'll get some of that. And I think tomorrow you'll, as you're introduced to tools, you'll start seeing, hey, oh, this tool is actually implementing this pattern that Neil talked about yesterday. Okay. And it's like now that you have a mental model of the pattern or the archetype, now you can figure out how do I apply this in different domains. So I think that will be covered. And of course, when the respective people talk about those tools, they can give you contacts. Or who are the points of contact for those respective assets, or accelerators with an accenture. 