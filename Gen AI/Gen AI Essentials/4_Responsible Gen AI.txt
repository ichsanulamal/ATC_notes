Hello, my name is Ray Eitel Porter and I'm Accenture's global responsible AI lead. As you know, generative AI is one of many applications of AI. And while it does present the same risks as traditional AI, it also brings new ones. Some of these risks are driven by the fact that foundation models and the data they're trained on, opaque and not available. Third party scrutiny. Other risks relate to the legal status of training data and liability and ownership considerations for outputs from the model. And furthermore, we should note that while traditional AI is typically only developed by a relatively defined a team of data scientists. Generic div ai can be available via the internet to every employee of an organization. So it's much harder to ring fence and control. When considering generative AI risk, it's important to understand the level of impact a given use case. We'll have how third-party tools will be used. So if an application will have limited impact on people, society, or the environment, then lighter touch controls will be appropriate. Some of the risks associated with generative AI looked like this. Unintentional bias and harmful or toxic content may be created by generative AI. And this can include misinformation. The outputs of generative AI may be highly accurate, but on the other hand, they may be highly plausible and yet inaccurate. And since foundation models provide little or no explainability, it's harder to check for accuracy and for consistency. Foundation models may undermine prophecy because confidential information which is entered as a prompt, may be shared with the developer of the foundation model and might be leaked to others. And there are ongoing legal disputes about the fact that foundation models may have been trained using copyrighted data. Derivatives may constitute copyright infringement. And similarly, the legal status of outputs created by a generative AI is unclear. So organisations may not be able to claim IP ownership over things that they create. And it's often unclear whether liability for any harms resulting from outputs would be attributed to the developer of the foundation model or to the organization building applications or services on top of it. We should also be aware that bad actors may use generative AI, e.g. making phishing attacks more realistic or helping to perpetrate types of fraud and to spread disinformation at scale. So as you see, the risks associated with generative AI are very real and they will in one way or another, impact all of us. As we're showing onscreen, we're dealing with a multilayered nature for generative AI solutions, which makes it more difficult to mitigate responsible AI risks than width traditional AI. This is because organizations building solutions, leveraging a proprietary foundation model, have no access to the training data that was used or to the model itself. So in these cases, we must focus on restricting the outputs of the solution to use only fine-tuning data, which is within our control. And to apply responsible AI and mitigation techniques in the fine tuning and application layers. These would include minimizing any bias in the fine-tuning training data itself and removing any sensitive content which shouldn't be shared with users of the solution. Using prompt engineering to enforce accuracy and potentially competence levels for the output. And introducing as far as possible explainability so that users can see how the outputs were generated. In the application layer itself, it's important to ensure that users are made aware that they are interacting with an AI system. And in addition to these technical mitigations, It's critical to develop governance and process approaches to mitigate risk from generative AI systems. These include ensuring that the organization's responsible AI principles, policy, standards, and governance procedures. I've been modified to account for new risks introduced by generative AI. Including the fact as I mentioned before, that solutions such as chat GPT can be easily accessed by anyone in the organization and are no longer in the hands of trained data scientists. Adapt a risk assessment and controls for generative AI, including introducing manual human in the loop or automated reviews as appropriate. Ensure that the legal department has approved for terms of use, for any generative AI foundation model or managed service solution. And provide firm-wide guidance and training on generative AI risks. Specific technical guidance and standards for relevant data science teams. Hi, my name is done actually be I am Managing Director and lead for responsible AI in both markets. We are focused on making our AI solutions fear, explainable and robust so that it is trusted and actionable. And this helps our clients to adopt our AI solution with confidence. You have just heard from our global responsible AI lead Ray Porter about responsible AI in generative AI. Let me give you a few examples. One of the most exciting aspects of generative AI that fascinates me is its ability to generate creative content. So the very first example that comes to mind is that of marketing. Knowledge language models can be used to instantly generate hundreds of camping ideas based on a single prompt. This helps marketers to do more personalization, better contextualization, and create exciting content at scale. Whether that was about text generation, but even more intriguing is generative AI capability to generate software tools. Large language models can complete a code snippet, understand the text prompt to convert it into quotes. It can create test cases to assess the software's functionality, identify and fix bugs in the code, and even help developers implement machine learning models quickly and easily into software applications. These two are industry agnostic use cases. Let me cite some examples that are specific to a particular industry, e.g. health care, where generative AI is widely applicable to many use cases. One of them is about summarizing the doctor-patient conversation that takes place during a consultation. Though are very highly skilled skills. That trade-off is productivity gain by enabling doctors to concentrate more in the conversation without the need to switch attention from the patient to the envelopes. Another type of customer service use case is integrated omnichannel, customer care. Generative AI's ability to discover and integrate customer data in real time can significantly improve the service outcomes, including better and faster resolution, as well as proactive assurance. As a result, customer service agents can discover customer data from multi-model sources, including usage, data, documents, images, jet, social media, and so on, all from a single interface, thereby significantly enhancing customer experience. This contrasts with situations where one agent transfers the customer interaction to another agent for fork where it is. So what you're seeing is that generative AI is a very promising technology, but we have to be mindful of its potential social and ethical risks as well. We need to select our use cases carefully because generative AI is not good at everything, contrary to what the hype might suggest. And SPSS and choose appropriate platforms and architectures are fine tune and embed controls into the application layer. It will be essential to provide as much reliability and safety as possible. Then for high-risk use cases, it will be important for a human to check outputs, at least for the foreseeable future. My final point is that we should start with a few experiments and gradually build our generative AI capability. Incorporating the principles of responsible AI innovate that effectively reduces the potential risks of this technology and makes it more adoptable across different industries and functions.