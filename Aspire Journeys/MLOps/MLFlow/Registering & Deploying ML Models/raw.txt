Registering & Deploying ML Models
The MLflow Model Registry enables easy registration and deployment of machine learning (ML) models for future use, either locally or in the cloud. It streamlines model management, facilitating collaboration among team members during model development and deployment. In this course, you will create classification models using the regular ML workflow. You'll see that visualizing and cleaning data, running experiments, and analyzing model performance using SHapley Additive exPlanations (SHAP) will provide valuable insights for decision-making. You'll also discover how programmatic comparison will aid in selecting the best-performing model. Next, you'll explore the powerful MLflow Models feature, enabling efficient model versioning and management. You'll learn how to modify registered model versions, work with different versions of the same model, and serve models to Representational State Transfer (REST) endpoints. Finally, you'll explore integrating MLflow with Azure Machine Learning, leveraging the cloud's power for model development.

Table of Contents
    1. Video: Course Overview (it_mlflowdj_03_enus_01)

    2. Video: Visualizing and Cleaning Data (it_mlflowdj_03_enus_02)

    3. Video: Creating an Experiment from the MLflow UI (it_mlflowdj_03_enus_03)

    4. Video: Running a Classification Model and Viewing its Metrics (it_mlflowdj_03_enus_04)

    5. Video: Analyzing Model Insights Using SHAP (it_mlflowdj_03_enus_05)

    6. Video: Running Multiple Classification Models (it_mlflowdj_03_enus_06)

    7. Video: Comparing Models Programmatically (it_mlflowdj_03_enus_07)

    8. Video: Registering an MLflow Model (it_mlflowdj_03_enus_08)

    9. Video: Modifying Registered Model Versions (it_mlflowdj_03_enus_09)

    10. Video: Registering Another Model and Viewing the Registered Model (it_mlflowdj_03_enus_10)

    11. Video: Serving Models to a Local REST Endpoint (it_mlflowdj_03_enus_11)

    12. Video: Creating an Azure Machine Learning (Azure ML) Account (it_mlflowdj_03_enus_12)

    13. Video: Registering a Model on Azure (it_mlflowdj_03_enus_13)

    14. Video: Accessing Models through Azure REST Endpoints (it_mlflowdj_03_enus_14)

    15. Video: Course Summary (it_mlflowdj_03_enus_15)

    Course File-based Resources

1. Video: Course Overview (it_mlflowdj_03_enus_01)

In this video, we will discover the key concepts covered in this course.
discover the key concepts covered in this course
[Video description begins] Topic title: Course Overview. Your host for this session is Vitthal Srinivasan. [Video description ends]
Hi and welcome to this course, registering and deploying ML Models. My name is Vitthal Srinivasan, and I will be your instructor for this course. The MLflow Model Registry enables easy registration and deployment of machine learning models for future use, either locally or in the cloud. It streamlines model management, facilitating collaboration among team members during model development and deployment.

MLflow Models provides a powerful solution for model version control and accessibility, allowing for sharing models, deploying them for production, and reproducing experiments in different environments. In this course, first, you'll create classification models using the regular ML workflow, visualizing and cleaning data, running experiments, and analyzing model performance using SHAP will provide valuable insights for decision-making. Programmatic comparison will aid in selecting the best-performing model.

Next, you'll explore the powerful MLflow Models feature, enabling efficient model versioning and management. You'll learn how to modify registered model versions, work with different versions of the same model, and serve models to REST endpoints. Finally, you'll explore integrating MLflow with Azure Machine Learning, leveraging the cloud's power for model development. In conclusion, this comprehensive course will provide you with a strong foundation in using the MLflow Model Registry effectively.

2. Video: Visualizing and Cleaning Data (it_mlflowdj_03_enus_02)

Find out how to visualize and clean data.
visualize and clean data
[Video description begins] Topic title: Visualizing and Cleaning Data. Your host for this session is Vitthal Srinivasan. [Video description ends]
In this demo, we are going to work with a new dataset, build a new model, this time it's going to be a classification model and then we are going to see how we can use MLflow in order to evaluate many different candidate models. We will programmatically find the best of this candidate models and then deploy it to production.

Initially, this deployment will be a local one and we will use this locally deployed model for inference using a curl call. Moving along, we will then take things onto Azure the Microsoft cloud platform. We will deploy our model from MLflow directly to Microsoft Azure, and then see how we can use that Azure-deployed model for inference.

[Video description begins] A Jupyter notebook titled ModelsRegistrationAndDeployment appears. It contains an address bar at the top. Under this, there is a toolbar with options like File, Edit, View, Insert, Cell, Kernel, Widgets, and Help. Adjacent to this, there are 2 buttons labeled Trusted, and mlflow_venv. [Video description ends]

This is going to be an exciting journey, so let's kick it off. As usual with the simple import statements, nothing particularly new here. You can see that we have all of the usual suspects pandas, numpy, seaborn, matplotlib.pyplot, and then from sklearn, we have the RandomForestClassifier, ColumnTransformer, Pipeline, SimpleImputer, StandardScalar, OneHotEncoder, train_test_split and LogisticRegression.

[Video description begins] The input cell 1 is highlighted. Line 1 reads: import pandas as pd. Line 2 reads: import numpy as np. Line 3 reads: import seaborn as sns. Line 4 reads: import matplotlib.pyplot as plt. [Video description ends] 

These object names give you a good full taste of the experiments that we are going to run ahead. Next, let's read in our input dataset into a pandas DataFrame.

[Video description begins] Line 6 reads: from sklearn.ensemble import RandomForestClassifier. Line 8 reads: from sklearn.compose import ColumnTransformer. Line 9 reads: from sklearn.pipeline import Pipeline. Line 10 reads: from sklearn.impute import SimpleImputer. Line 11 reads: from sklearn.preprocessing import StandardScaler, OneHotEncoder. Line 12 reads: from sklearn.model_selection import train_test_split. Line 14 reads: from sklearn.linear_model import LogisticRegression. [Video description ends]

Here, we do this with pd.read_csv, and the input data has been sourced from Kaggle. This is a dataset which has to do with Travel Churn Prediction.

[Video description begins] The input cell 2 is highlighted. Line 1 is highlighted. It reads: travel_churn_data = pd.read_csv('./datasets/customer_travel.csv'). A URL appears below that reads: Travel Churn Prediction from Kaggle, https://www.kaggle.com/datasets/tejashvi14/tour-travels-customer-churn-prediction. [Video description ends] You can see the Kaggle URL on screen now. In this, there is a column called Target, you can see that way over on the right. Target equals 1 implies that the customer churns. Target equals 0 implies that the customer does not churn.

In order to predict whether the customer churns or not, we have information about the Age, FrequentFlyer status, AnnualIncomeClass, ServicesOpted for, whether the account has been synced to social media, and whether that customer has booked a hotel or not. You can imagine that data like this could serve as the basis for a customer retention program. Also notice that the columns FrequentFlyer, AnnualIncomeClass, AccountSyncedToSocialMedia, and BookedHotelOrNot, all currently contain string values. These are all categorical variables, and we will need to encode them by converting those string values into numeric forms and then use one-hot encoding before we use them for prediction.

Let's begin with a little exploratory data analysis. We begin by printing out the columns in our DataFrame. Here they are visible on screen now.

[Video description begins] The input cell 3 is highlighted. Line 1 reads: travel_churn_data.columns. [Video description ends]

Next, let's try to get a sense of how balanced or imbalanced the dataset is. We can see that we've invoked sns.countplot, passing in the Target as the x variable.

[Video description begins] The input cell 4 is highlighted. Line 3 is highlighted. It reads: sns.countplot(x = 'Target', data = travel_churn_data). [Video description ends]

We can see from this that there are 700 odd rows where target is equal to 0, and about 200 rows where target is equal to 1. A countplot like this one gives us a good sense of whether we need to prioritize precision, accuracy, or recall in our classifier. We'll have more to say on that subject.

For now, let's move on and construct a boxplot.

[Video description begins] The input cell 5 is highlighted. Line 3 is highlighted. It reads: sns.boxplot(x = 'AnnualIncomeClass', y = 'Age', data = travel_churn_data). [Video description ends]

Here we are interested in the relationship between AnnualIncomeClass and Age. So we invoke sns.boxplot. x is equal to AnnualIncomeClass, y is equal to Age, and when we plot this chart, you can see that on the x-axis, we have three boxplots, one for each of the categorical values of income class. To jog your memory on how to interpret this plot, the box represents the interquartile range.

So, for instance, for the Middle Income boxplot, the lower end of the box which is at 30 represents the 25th percentile, the upper end of the box which is at 36 represents the 75th percentile. The horizontal line that you see within that box represents the median. The whiskers which extend out in each direction go out to 1.5 times the interquartile range. So, here that extends up to 38 on the upside and less than 28 on the downside. If there were any outliers, that is points which lay beyond this 1.5 times interquartile range on either side, they would be represented as individual points within the boxplot, there aren't any in this data.

One point worth keeping in mind about this annual income variable is that it is categorical, of course, but it is a specific type of categorical variable. It is an ordinal categorical variable. You see the values Low Income, Middle Income, and High Income, these values have an order. They represent a certain hierarchical or ordered relationship. This is something that we'll keep in mind when we perform the label encoding that is, when we encode these values as numbers, the numbers that we pick must represent or must contain the same information about order as we implicitly pick up from the labels.

We will need to label encode these such that the value for Low Income is the smallest, the one for Middle Income is in the middle, and the High Income is the highest. That's what we do in the next code cell. There are three string values and in the next code cell, we are going to replace them with corresponding numeric values.

[Video description begins] The input cell 6 is highlighted. Line 1 reads: travel_churn_data['AnnualIncomeClass'].unique(). [Video description ends]

We do this with a dictionary called mapper. The keys correspond to the values in our categorical data and the values correspond to the numeric encoding. We invoke the replace method, passing in this dictionary on the column AnnualIncomeClass. Then, at the end of this process, we invoke the info method on our DataFrame one more time and we can satisfy ourselves that by this point, the AnnualIncomeClass appears as being of type int64.

[Video description begins] The input cell 7 is highlighted. Line 1 reads: mapper = {'Low income': 0, 'Middle Income': 1, 'High Income': 2}. Line 3 reads: travel_churn_data['AnnualIncomeClass'] = travel_churn_data['AnnualIncomeClass'].replace(mapper). [Video description ends]

[Video description begins] The input cell 8 is highlighted. Line 1 reads: travel_churn_data.info(). [Video description ends] 

Previously, it would have been of type object because it contains strings. Those have now been replaced with numeric values. However, we can see that the column FrequentFlyer still has Dtype of object. That's also true for AccountSyncedToSocialMedia and BookedHotelOrNot. We had previously discussed how the AnnualIncomeClass was a categorical ordinal variable because the different values there had an ordering inherent in them. These variables, FrequentFlyer, AccountSyncedToSocialMedia, and BookedHotelOrNot are not ordinal variables, these are known as categorical nominal variables. Because these are nominal, we cannot encode them using integers such as 0, 1, and 2, and instead, we'll rely on one-hot encoding. You can see the code for this on screen now.

We have a list called categorical_features and this holds the column names FrequentFlyer, AccountSyncedToSocialMedia, BookedHotelOrNot. Then on line 4 of this code cell, we construct a Pipeline. This is a scikit-learn pipeline and we've got to specify the steps and the steps in a scikit-learn pipeline can be specified as a list of tuples.

[Video description begins] The input cell 9 is highlighted. Line 1 reads: categorical_features = [. Line 2 reads: 'FrequentFlyer', 'AccountSyncedToSocialMedia', 'BookedHotelOrNot']. Line 4 reads: categorical_transformer = Pipeline(. Line 5 reads: steps = [('encoder', OneHotEncoder(handle_unknown = 'ignore', drop = 'first'))]). [Video description ends]

Here that list, which is called steps, has just the one tuple in it. The first element in the tuple is the name of the step which is encoder and the second is the transform. That transform is the OneHotEncoder. We are also telling it how to handle unknown values. They should be ignored and how to decide which of the values should drop. We just drop the first.

This categorical_transformer will help us to one hot encode the categorical features. We then pass that into the ColumnTransformer which serves as a general preprocessor for all of our data. Within this ColumnTransformer, we have just one transformer in the list of transformers and that's the categorical_transformer which we defined on line 4. Which columns is this going to work on? Well, those are the categorical_features which we have defined starting line 1. For the remaining columns on line 9, we just specify a StandardScaler. That StandardScaler will standardize each feature, effectively removing the mean and scaling to variance of 1.

[Video description begins] Line 7 reads: preprocessor = ColumnTransformer(. Line 8 reads: transformers = [("cat", categorical_transformer, categorical_features)],. Line 9 reads: remainder = StandardScaler(). Line 10 reads: ). [Video description ends]

All of this preprocessing that you see in this code cell is going to effectively become a part of the model and that's because our model is going to be a Pipeline in which we have two steps.

The first step will be the preprocessor and the second will be the classifier. And that's the model which we will be logging and using for deployment. The implication of all of this is that later we can just pass in raw data for inference to our model, all of the preprocessing steps will be built in. This helps to combat something known as training-serving skew. If the data preprocessing steps differ on the training data versus on the inference data, the model results might be a lot worse in production than during training and testing.

By using a Pipeline object in this form and then serializing or saving the model along with the preprocessing step, you can go a long way in mitigating this. In any case, let's keep going. In the next code cell, we extract the X and the y variables. The y variable on line 3 is simply the target column from our data. The X variable on line 1 is obtained by simply dropping that one Target. That's why we invoke .drop on travel_churn_data and we specify labels equal to Target. Then on line 5, we invoke train_test_split passing in the X and the y data, and we specify the size of the test dataset as 20% that's 0.2. The random_state is initialized to 124.

[Video description begins] The input cell 10 is highlighted. Line 1 reads: X = travel_churn_data.drop(labels = ['Target'], axis = 1). Line 3 reads: y = travel_churn_data['Target']. Line 5 reads: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 124). [Video description ends]

Let's then quickly examine the shapes of X_train and X_test. As you can see, we have 763 rows in the training data and 191 rows in the test data. There are 6 X variables that we will be using in our classifier. We are now done with the basic exploratory data analysis. It's time for us to move on to the MLflow portions and we'll get to those in the next demo.

[Video description begins] The input cell 11 is highlighted. Line 1 reads: X_train.shape, X_test.shape. [Video description ends] 

3. Video: Creating an Experiment from the MLflow UI (it_mlflowdj_03_enus_03)

In this video, find out how to create an experiment from the MLflow user interface (UI).
create an experiment from the MLflow user interface (UI)
[Video description begins] Topic title: Creating an Experiment from the MLflow UI. Your host for this session is Vitthal Srinivasan. [Video description ends]
We ended the previous demo by performing some simple exploratory data analysis and we also coded up the preprocessing required for our training data into a pipeline. Now, it's time for us to log our model using MLflow.

[Video description begins] The Jupyter notebook titled ModelsRegistrationAndDeployment appears. It contains an address bar at the top. Under this, there is a toolbar with options like File, Edit, View, Insert, Cell, Kernel, Widgets, and Help. Adjacent to this, there are 2 buttons labeled Trusted, and mlflow_venv. [Video description ends]

Please note that we have not run any of this code yet and I'll explain why in a moment.

Please focus on the third code cell where we invoke mlflow.set_experiment and we choose experiment_name equals customer_churn_prediction. Now this experiment name does not exist as of now and we are going to create that experiment by switching over to the MLflow UI.

[Video description begins] The input cell 14 is highlighted. Line 1 reads: mlflow.set_experiment(experiment_name = 'customer_churn_prediction'). [Video description ends]

However, while doing so, there is one little quirk which we have to be mindful of. Let's see what that is.

We switch over to the MLflow UI. We are in the Experiments tab. We click on the plus button over on the top left. This opens up a dialog where we can type in an Experiment Name, and we go with customer_churn_prediction. That's the same Experiment Name that we had specified from our Jupyter the quirk has to do with the second input that you see here for Artifact Location. Right now, that is blank. However, if we do not specify a value of the Artifact Location and just click on Create at this point, then an exception is going to result down the line. By default, when we create an experiment using the MLfow UI, MLflow is going to assume that the Artifact Location is not going to be on the local file system. It will assume that that Artifact Location resides on a proxy server somewhere.

Now, the MLflow UI is usually used on a local installation of MLflow and that's why this quirk may actually be considered a bug. But in any case, let's just fix it. All we need to do is to specify an Artifact Location on the local machine. We switch over to a terminal window and run ls -l in our top-level MLflow directory. You can see here that we have the virtual environment, mlflow_venv, we have the mlruns directory, and then we have the datasets and the Python notebook.

Now in this directory, let's create another directory called artifacts and then we rerun ls -l to confirm that the artifacts directory does indeed exist and that's what we will now specify. So, we switch back to the MLflow UI, and in the Artifact Location, we now paste in the full path. This is the absolute path to that artifacts directory that we just created from the terminal. Now, it's actually OK for us to hit Create.

[Video description begins] A Terminal window appears. The following commands are highlighted: ls -l, mkdir artifacts, and ls -l. [Video description ends] 

Please remember this while creating your experiments from the MLflow UI, if you wish to have your artifacts logged to a local file location, then you've got to specify that directory right here while creating the experiment. That's gone through successfully and we now see the familiar Experiment view for this experiment customer_churn_prediction. Now we can switch back to Jupyter. We begin by importing mlflow and we also invoke the method mlflow.get_tracking_uri. This confirms that the mlruns directory, which we looked at a moment ago in the terminal is indeed the correct location for the tracking store.

[Video description begins] The input cell 13 is highlighted. Line 1 reads: import mlflow. Line 3 reads: mlflow.get_tracking_uri(). [Video description ends]

Next, let's invoke set_experiment to set customer_churn_prediction as the currently active experiment and this goes through successfully. Very importantly, we also see in the returned experiment object that the artifact_location points to the correct local file path, mlflow/artifacts. Had we not created this directory and associated it manually, then later on while attempting to invoke log artifacts, an exception would have resulted. On screen now, you can see the URL to a bug report which someone has filed on Git.

[Video description begins] The input cell 14 is highlighted again. A URL appears at the bottom that reads: https://github.com/mlflow/mlflow/issues/7819. Below, the following text appears: More details on this issue - possibly "a bug in mlflow ui". [Video description ends] 

Moving on, by this point, we've created our experiment from the MLflow UI, and we've set that experiment as the active experiment from here in Jupyter Notebook. It's time now for our first run and for this run, we do not use autologging, that's because we would like to explicitly save the model signature and some other important details. We begin with the requisite import statements. On line 1, we import accuracy_score, precision_score, and so on.

On line 2, we import the function infer_signature from mlflow.models.signature. The with block starts on line 4. We invoke mlflow.start_run. The name of the run is logistic_model and of course, we have that as the variable logistic_run. On line 6, we instantiate a LogisticRegression model object, this is an sklearn model object.

[Video description begins] The input cell 15 is highlighted. Line 1 reads: from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score. Line 2 reads: from mlflow.models.signature import infer_signature. Line 4 reads: with mlflow.start_run(run_name = 'logistic_model') as logistic_run:. Line 6 reads: lr_model = LogisticRegression(). [Video description ends]

On line 8, we create a Pipeline. This pipeline takes in a list of steps. Each step is a tuple. The first element in the tuple is the name of that step. So, the first step is preprocessor, the second is classifier. The second contains the transform. So, in the first step, the preprocessor is the transform, in the second, it's the lr_model that's the transform. Once we have our pipe_lr, we can invoke fit on it.

We do that on line 9, passing in X_ train and y_ train, and then we can use this train model for predictions. We do that on line 11 by invoking .predict. We then can also get the probabilities of the different labels using the .predict_proba passing in X_test. This is going to return the explicit probabilities rather than just the yes-no labels. On line 15, we calculate the training accuracy score, and then on lines 17 through 21, we compute various metrics, all on the test data.

[Video description begins] Line 8 reads: pipe_lr = Pipeline(steps = [('preprocessor', preprocessor), ('classifier', lr_model)]). Line 9 reads: pipe_lr.fit(X_train, y_train). Line 11 reads: predictions = pipe_lr.predict(X_test). Line 13 reads: predictions_predict_prob = pipe_lr.predict_proba(X_test). Line 15 reads: train_accuracy_score = pipe_lr.score(X_train, y_train). [Video description ends]

So, on line 17, we call accuracy_score on y_test, comparing it to predictions. Remember that accuracy is simply what proportion of the predictions are correct. Precision can be thought of as the accuracy for when the classifier actually flags churn. So, the precision tells us, of all the cases where the classifier has predicted churn, how many of those cases were actually churn? The recall can be thought of as the accuracy for when churn is actually present. So, for all of the cases, where the customer actually churned, how many of them were correctly predicted by our classifier?

Now, there's an inherent trade-off between precision and recall. You probably know this already, but it won't hurt to jog your memory. Imagine a classifier which always predicts churn. The recall of this classifier is going to be a 100%. However, the precision is going to be very low because most passengers do not actually churn. Likewise, let's say we have a classifier which predicts node churn under all circumstances. In such a case, the recall is going to be very low, but the precision and the accuracy will both seem pretty high because the majority of passengers do not churn.

Anytime we have two metrics which are inversely correlated to each other like this, a good way to get a composite metric which compromises between them is using the harmonic mean, and that's what the f1_score is. You can see that calculated on line 20, the f1_score is the harmonic mean of precision and recall, and it will end up being closer to the smaller of the two. As a result, if you pick a classifier which has a high f1_score, you are picking a classifier which has a good balance between precision and recall.

The auc_score that you see on line 21 is another interesting way of evaluating a classifier. This function takes in two input arguments. It takes in the true y values, that's y_test as well as the predicted y values. Then it has an internal procedure on the basis of which it returns a score. If the returned score is 0.5, well then, our classifier is no better than random. A perfect classifier would have an auc of 1. What is the score?

[Video description begins] Line 17 reads: test_accuracy_score = accuracy_score(y_test, predictions). Line 18 reads: test_precision_score = precision_score(y_test, predictions). Line 19 reads: test_recall_score = recall_score(y_test, predictions). Line 20 reads: test_f1_score = f1_score(y_test, predictions). Line 21 reads: auc_score = roc_auc_score(y_test, predictions_predict_prob[:, 1]). [Video description ends]

Well, the way the score is calculated, it effectively builds a graph where on the y-axis, it has the true positive rate and, on the x-axis, it has the false positive rate. And then using a numeric integration procedure, it tries to find how well our classifier does as compared to either a random classifier or a perfect classifier.

In a perfect classifier, the true positive rate would be a 100%, the false positive rate would be 0, and the area under curve as calculated by it would be 1. On the other hand, for a completely random classifier, the area under the curve would be 0.5. We now have a good intuitive understanding of how to evaluate a classifier, that's done on the basis of the code on lines 17 through 21. Then starting line 23, we extract the parameters of our model and then log them using MLflow. Let's describe that in more detail in the next demo.

[Video description begins] Line 23 reads: model_params = lr_model.get_params(). Line 24 reads: mlflow.log_params(model_params). [Video description ends]

4. Video: Running a Classification Model and Viewing its Metrics (it_mlflowdj_03_enus_04)

Learn how to run a classification model and view the metrics.
run a classification model and view the metrics
[Video description begins] Topic title: Running a Classification Model and Viewing its Metrics. Your host for this session is Vitthal Srinivasan. [Video description ends]
We ended the previous demo midway through the code of our mlflow run. We had discussed the code up to line 21, that's where we have computed the auc_score of our classifier. Let's pick up the action from line 23, where we begin by invoking get_params on our lr_model.

[Video description begins] The Jupyter notebook titled ModelsRegistrationAndDeployment appears. It contains an address bar at the top. Under this, there is a toolbar with options like File, Edit, View, Insert, Cell, Kernel, Widgets, and Help. Adjacent to this, there are 2 buttons labeled Trusted, and mlflow_venv. [Video description ends]

Then we log the parameters of our model to mlflow that's done on line 24, using mlflow.log_params. Please note that when logging the parameters, we are only focused on the model, we don't care about the pipeline. However, the pipeline is going to come into play while logging the model, which is something that we will do down below. Then starting line 26, we construct a metrics dictionary with all of the training and test metrics that we computed up above. We invoke mlflow.log_metrics on line 35.

[Video description begins] The input cell 15 is highlighted. Line 23 reads: model_params = lr_model.get_params(). Line 24 reads: mlflow.log_params(model_params). Line 26 reads: metrics = {. Line 27 reads: 'Train_accuracy_score': train_accuracy_score,. Line 28 reads: 'Test_accuracy_score': test_accuracy_score,. Line 29 reads: 'Test_precision_score': test_precision_score,. Line 30 reads: 'Test_recall_score': Test_recall_score,. Line 31 reads: 'Test_f1_score': test_f1_score,. Line 32 reads: 'AUC_score': auc_score. Line 33 reads: }. [Video description ends]

So, by this point, we've logged the parameters as well as the metrics but now the most important bit lies ahead of us and that's logging the model.

[Video description begins] Line 35 reads: mlflow.log_metrics(metrics). [Video description ends]

Please remember that we have not turned on autolog and we are going to log the model ourselves. The code for this starts on line 38, where we pull in ModelSignature from mlflow.models.signature as well as Schema and ColSpec from mlflow.types.schema.

This process is best understood from bottom to top, so let's actually start with line 55, where we invoke mlflow.sklearn.log_model. The first input argument is pipe_lr, so please note that we are logging the entire pipeline and not only the Logistic Regression Model. So, the first input argument, pipe_lr is going to be pickled and that's what we will find in the pickle file inside the model directory on the artifact server. The second input argument on line 57 is the artifact path.

[Video description begins] Line 38 reads: from mlflow.models.signature import ModelSignature. Line 39 reads: from mlflow.types.schema import Schema, ColSpec. [Video description ends] 

This here is preprocessing_pipeline_with_logistic_regression_model. This is pretty verbose, but this is going to be the location on the artifact server where this model is going to be logged. The most interesting input is the third one, the signature. That signature is constructed on line 52.

[Video description begins] Line 55 reads: mlflow.sklearn.log_model(. Line 56 reads: pipe_lr,. Line 57 reads: 'preprocessing_pipeline_with_logistic_regression_model',. Line 58 reads: signature = signature). [Video description ends]

It's an object of type ModelSignature and notice on line 52, that we specify the inputs as well as the outputs.

The inputs point to the input_schema object which is defined starting line 41. The output_schema which is a lot simpler is constructed on line 51. Let's first deal with the output_schema.

[Video description begins] Line 52 is highlighted now. It reads: signature = ModelSignature(inputs = input_schema, outputs = output_schema). Line 51 reads: output_schema = Schema([ColSpec('integer')]). [Video description ends]

You can see that this is an object of type Schema. It takes in a list of ColSpec objects. ColSpec, of course, is short for Column Specification, and the type of the output is integer. In similar fashion, the input_schema is also an object of type Schema, and it too takes in a list of ColSpec objects.

Now, of course, the inputs are more numerous than the outputs, and that's why we have all of those individual ColSpec objects on lines 43 through 48. For each, we have the type as well as the name. In case, you're wondering why we do not have a name for the output_schema ColSpec, well, that's because the output_schema corresponds effectively to the return type from a function, and that's not expected to be a named parameter. The inputs are expected to have names that will allow us to specify the parameters while invoking the function.

In any case, we have here ColSpecs for Age, FrequentFlyer, and so on, and the corresponding types, long, string, etc.

[Video description begins] Line 41 reads: input_schema = Schema(. Line 42 reads: [. Line 43 reads: ColSpec('long', 'Age'),. Line 44 reads: ColSpec('string', 'FrequentFlyer'),. Line 45 reads: ColSpec('long', 'AnnualIncomeClass'),. Line 46 reads: ColSpec('long', 'ServicesOpted'),. Line 47 reads: ColSpec('string', 'AccountSyncedToSocialMedia'),. Line 48 reads: ColSpec('string', 'BookedHotelOrNot'),. Line 49 reads: ]. Line 50 reads: ). [Video description ends]

So, we've now gone through this model logging code from bottom to top. Let's quickly go back from top to bottom. On line 41, we constructed the input_schema. On line 51, we constructed the output_schema. On line 52, we combined those into the ModelSignature, and then on line 55, we logged the model passing in that signature.

This model signature is extremely important in the MLflow context. When we deploy our model and use it for inference, MLflow is actually going to enforce the schema. We'll discuss this in detail while talking about auto-log and the warning that arose then. So, if we end up with a mismatch between our model signature and the actual function that's got to be used for the prediction, then runtime exceptions will result.

In any case, let's now hit Shift+Enter and this run goes through successfully. Let's switch over to the MLflow UI and see what exactly has been logged as a part of this run. Here we are in the customer_churn_prediction experiment. We click the Refresh button over on the center-right and the logistic_model run appears down below.

[Video description begins] A page titled mlflow 2.3.2 appears on the browser. The title bar at the top offers 2 tabs: Experiments and Models. The Experiments tab is active now. The navigation pane on the left displays 2 items: Default and customer_churn_prediction. The customer_churn_prediction item is open on the main pane. It shows the header called Description. Below, a search bar appears along with several filter options such as Table view, Chart view, Sort, Columns, and so on. A table appears next with 5 column headers: Run Name, Created, Duration, Source, and Models. [Video description ends]

Let's click through into that. We see that we have 15 Parameters, 6 Metrics, 1 Tag, and of course, the model which has been logged under Artifacts.

Let's click through into the Parameters. These are all of the Parameters corresponding to the Logistic Regression Model. These aren't that interesting to us. However, the Metrics are more illuminating. You can see here that the accuracy on the test data is 0.743, which is quite good. However, both the Test_precision and the Test_recall are significantly worse.

The recall in particular is just about 21%, this means that of the cases which actually churn, our classifier is only able to correctly spot about one in five. So, that's clearly a lot of false negatives. Four and five of its predicted churn candidates are in fact false negatives. The precision, on the other hand, is about 37.5% this means that in the cases where the classifier predicts churn, about 37.5% of them actually go on to churn. All of the rest correspond to false positives.

One heartening feature of this model performance is that the training and the test_accuracy are both quite close to each other. The test_f1_score which is the harmonic mean of the Test_recall and the Test_precision is 0.269 and the AUC_score is 0.769, just about midway between a random classifier and a perfect classifier. So, at least it doesn't seem like this is a badly overfit model.

That would be a model which does really well in training, but really poorly on the test data. That does it for the Metrics, we have just then 1 Tag in there and then we come to the Artifacts. This is the most important. While invoking logged model from our Python code, we had specified preprocessing_pipeline_with_logistic_regression_model as the Artifact path and that indeed is the name of the directory which we see in the Explorer over on the top left.

You can see here that we've successfully logged out our model and over on the left in the Explorer, we see all of the files that we expect. At the top level is a summary created for us by MLfow. We have the Model schema over on the left and the sample code snippets for how to make predictions over on the right. This Model schema of course corresponds to what we wrote out while invoking log model. Let's click through into the MLmodel file. It's interesting here to note the signature section down below.

Here the inputs as well as the outputs are taken from the column specifications which we logged out from our Python code. We can satisfy ourselves that this schema matches what we had in Python. The other files written out here also all make sense. There's the conda.yaml, the model.pkl file, the python_env.yaml file, and finally, the requirements.txt. And that gets us to the end of this demo, in which we demonstrated a successful invocation of log model in which we explicitly wrote out the model signature. Let's switch back to Jupyter and try out a few more candidate models.

5. Video: Analyzing Model Insights Using SHAP (it_mlflowdj_03_enus_05)

During this video, discover how to analyze model insights using SHapley Additive exPlanations (SHAP).
analyze model insights using SHapley Additive exPlanations (SHAP)
[Video description begins] Topic title: Analyzing Model Insights Using SHAP. Your host for this session is Vitthal Srinivasan. [Video description ends]
How does a machine learning model arrive at its prediction or conclusion? This is not just a technical question, these days questions around bias in AI and ML are becoming more and more important and therefore it's becoming more and more important for a model to be able to explain itself. As someone using the model, you need to be able to figure out why that model made its prediction. One way of getting an answer to this question is through a technique known as SHAP values, and Python has a library called SHAP, which can be used for model explainability.

MLflow has an integration with SHAP and that's what we will be working with in this demo. On the first code cell, you can see that we've run a pip install of shap and at this point, the requirement is already satisfied.

[Video description begins] The Jupyter notebook titled ModelsRegistrationAndDeployment appears. It contains an address bar at the top. Under this, there is a toolbar with options like File, Edit, View, Insert, Cell, Kernel, Widgets, and Help. Adjacent to this, there are 2 buttons labeled Trusted, and mlflow_venv. [Video description ends] 

[Video description begins] The input cell 16 is highlighted. Line 1 reads: pip install shap. [Video description ends]

Moving on to the next code cell, we start by importing mlflow.shap, that's the integration between SHAP and MLflow. Then on line 3 of this code cell, we have our with block, where we kick off our run using mlflow.start_run. The name of this run is logistic_model_with_explanation, you see that on line 3. And then we have a lot of code which should be very familiar.

[Video description begins] The input cell 17 is highlighted. Line 1 reads: import mlflow.shap. Line 3 reads: with mlflow.start_run(run_name = 'logistic model with explanation') as logistic_run:. [Video description ends]

We've just gone through all of this in detail, so let's scroll down. The first point worth noting is that this time we've simply inferred the signature. On line 36, we run the infer_signature method, this is from mlflow. We pass in the X_train data, the pipe_lr.predict function, and we get back the signature object, which we then write out while invoking mlflow.sklearn.log_model, you see that they are on line 52. This use of infer_signature is different from the previous run where we explicitly constructed the signature from the input and the output schemas and also built the column specifications.

We have encountered an implicit use of the infer signature before and that was when autolog was turned on. So, that's one new bit about this run. Then starting on line 38, we get to the Shapley values.

[Video description begins] Line 36 reads: signature = infer_signature(X_train, pipe_lr.predict(X_train)). [Video description ends]

On line 39, we get the preprocessor from the pipe_lr with the name of the step which is preprocessor, and we then transform the training data. This gives us the transformed observations. These are then converted into a pandas DataFrame, that's done on line 41, and that pandas DataFrame is then passed in to the function mlflow.shap.log_explanation on lines 46 and 47.

So, just like MLflow has methods like log_model, log_param, log_artifact, and log_metrics, it also has a log_explanation, which you see right here on line 46. These explanations will be viewable in the artifacts, and we'll get to that in a minute. But first, let's just understand what these SHAP values are. The SHAP library relies on concepts from game theory, apparently, and it takes in two inputs. It takes in all of the X values which are to be used for prediction, and it takes in a predict function which can be used to generate predicted y values. Then it applies different combinations of the X values and sees how the predictions vary.

The SHAP model will also compare these predictions to certain base predictions which would just rely on the average or the median of the original y values. Then the model will output SHAP coefficients for each X variable.

[Video description begins] Line 38 reads: # Compute Shapley values. Line 39 reads: observations = pipe_lr.named_steps['preprocessor'].transform(X_train). Line 41 reads: observations_asframe = pd.DataFrame(. Line 42 reads: observations , columns = preprocessor.get_feature_names_out()). Line 44 reads: print(observations_asframe). Line 46 reads: mlflow.shap.log_explanation(. Line 47 reads: pipe_lr.named_steps['classifier'].predict, observations_asframe). [Video description ends]

These SHAP values are not constrained to any specific range, so, for instance, they will not always be between 0 and 1, they can be positive or negative. The magnitude of the SHAP value tells how important the factor is. So, the greater the magnitude, the more important that particular factor or X variable.

The sign of the SHAP value tells the direction of the influence of that particular factor on the ultimate prediction. Finally, please note that as you can see here, the SHAP values are associated not with the model itself, but rather with the predictions. So, the SHAP values will vary based on the X data and the y data over which they are being computed. Let's now go ahead and run this code.

[Video description begins] Line 49 reads: mlflow.sklearn.log_model(. Line 50 reads: pipe_lr,. Line 51 reads: 'preprocessing_pipeline_with_logistic_regression_model',. Line 52 reads: signature = signature. Line 53 reads: ). [Video description ends]

 [Video description begins] Lines 46 and 47 are highlighted again. [Video description ends]

A warning results. We were unable to find the cause of this warning. There weren't any good online sources, so we just take it as it is and find that the output seems to be perfectly fine. Hopefully, this warning will be fixed in a future release. In any case, our run has gone through successfully.

We now switch over to the MLflow UI. We hit Refresh and the new run appears.

[Video description begins] A page titled mlflow 2.3.2 appears on the browser. The title bar at the top offers 2 tabs: Experiments and Models. The Experiments tab is active now. The navigation pane on the left displays 2 items: Default and customer_churn_prediction. The customer_churn_prediction item is open on the main pane. It shows the header called Description. Below, a search bar appears along with several filter options such as Table view, Chart view, Sort, Columns, and so on. A table appears next with 5 column headers: Run Name, Created, Duration, Source, and Models. [Video description ends]

This is the logistic model with explanation. Let's click through into this run. The first thing that caches our eye is that in the Artifacts section, we now have two directories. We continue to have the model directory, which has been expanded, but right above that we also see the model_explanations_shap. We could click through the model directory, but there's nothing particularly new here, everything is as before.

So, let's quickly focus on the shap explanations. In this directory, you see that there are three files. Two of these are NumPy files and the third is a png file. The NumPy files are the base_values.npy and the shap_values.npy. As we previously discussed, the base_values correspond to the predictions based purely on the y values, sort of an average, without taking into account any of the X values in the model prediction. The shap_values NumPy file, on the other hand, contains the output of the shap model in numeric form, that's not available for us to preview here.

Let's click through into the png file and here you can see that we have a horizontal bar chart, and on the X-axis, we have the SHAP score, which is given as the average impact on the model output, and each of these bars corresponds to one of the X variables. By the way, those names appear slightly mangled because of the preprocessing step in our sklearn pipeline. Remember that we had applied one-hot encoding to all of the nominal categorical data, and we had applied the standard scalar to all of the numeric columns.

By looking at these model explanations, it clearly seems like the FrequentFlyer variable is the most significant. The age and whether the passenger has booked a hotel or not are also pretty impactful. On the other hand, the AnnuallIncome and the FrequentFlyer record seem to have very little impact. In this way, we've successfully demonstrated how we can use MLflow in order to add in SHAP model explanations. One final word about these SHAP model explanations, these are model agnostic. We can compute SHAP model explanations for any kind of model.

Now, there are some types of models which have built-in factor importances. In the case of a decision tree, for instance, the importance of an X variable can be measured from how close to the root of the decision tree it is. If it's very close to the leaves of the decision tree, it's less important than if it's very close to the root. And there's another important aspect in which the SHAP model differs from the factor importance in a decision tree. Remember that these SHAP values are a property of a set of predictions.

In a decision tree, the factor importance is a property of the model itself. So, with the SHAP values, if you change the X and y values that you use the model on, the SHAP values will change. Now these files are all written out as artifacts in this run of our experiment. Let's switch back over to Jupyter and see how we can access these explanations from within Python. The code is visible on screen now. On line 1, we import the os module. We'll need that for some path manipulation, you see that on line 19, where we invoke os.path.join.

On line 2, we import MlflowClient.

[Video description begins] The input cell 18 is highlighted. Line 1 reads: import os. Line 19 reads: dst_path = os.path.join(dst_path, artifact_path). [Video description ends]

We instantiate that client object on line 4. We specify the artifact_path on line 6. Please note that this is the model_explanations_shap, i.e., this is the name of the directory on the artifact server. Then we have a list of artifacts where we invoke client.list_artifacts.

[Video description begins] Line 2 reads: from mlflow import MlflowClient. Line 4 reads: client = MlflowClient(). Line 6 reads: artifact_path = 'model_explanations_shap'. [Video description ends]

This method requires us to specify the run id and we get that from logistic_run.info.run_id. We also have to specify the artifact_path. In this way we get a list of artifact paths, we print those out on line 14, and then on line 17, we load back the logged explanation. That's done by invoking mlflow.artifacts.download_artifacts and then we've got to pass in an input argument which is the run_id.

[Video description begins] Line 9 reads: x.path for x in client.list_artifacts(. Line 10 reads: logistic_run.info.run_id, artifact_path). [Video description ends] 

A little bit of path manipulation follows which will allow us to load the image files, but more importantly the NumPy files. We were not able to preview those from within the MLflow UI. So, you see that on lines 23 and 24, we construct the paths of the base_values and shap_values.npy files and then load those in as NumPy darrays and we print out those values on lines 27 and 29. Let's run this code. 

[Video description begins] Line 14 reads: print(artifacts). Line 17 reads: dst_path = mlflow.artifacts.download_artifacts(run_id = logistic_run.info.run_id). Line 19 reads: dst_path = os.path.join(dst_path, artifact_path). [Video description ends]

[Video description begins] Line 23 reads: base_values = np.load(os.path.join(dst_path, 'base_values.npy')). Line 24 reads: shap_values = np.load(os.path.join(dst_path, 'shap_values.npy')). Line 27 reads: print(base_values). Line 29 reads: print(shap_values). [Video description ends]

The first bit of the output has a list of all of the artifacts in this particular directory.

There are three files here, base_values.npy, shap_values.npy and summary_bar_plot.png. Each of these is in the same directory, model_explanations_shap. Then we see the values of the two NumPy files. The base_values is about 0.17. This base_value is simply the proportion of data in the test dataset which was passed into SHAP where churn was equal to 1. Then come the actual shap_values, and interpreting these requires an understanding of the SHAP algorithm, which is beyond the scope of this course. But again, do keep in mind that the basic idea is that the SHAP algorithm will vary the inputs in the order as well as which rows are included and see how the outputs change.

With that, we've now thoroughly explored the SHAP integration that MLflow supports. It's time for us to return to more candidate models, this time using autolog. We'll get to that in the next demo.

6. Video: Running Multiple Classification Models (it_mlflowdj_03_enus_06)

In this video, you will learn how to run multiple classification models.
run multiple classification models
[Video description begins] Topic title: Running Multiple Classification Models. Your host for this session is Vitthal Srinivasan. [Video description ends]
By this point, we've successfully added two runs to our experiment. In this demo, we will add in four additional runs in quick succession.

[Video description begins] The Jupyter notebook titled ModelsRegistrationAndDeployment appears. It contains an address bar at the top. Under this, there is a toolbar with options like File, Edit, View, Insert, Cell, Kernel, Widgets, and Help. Adjacent to this, there are 2 buttons labeled Trusted, and mlflow_venv. [Video description ends]

The first of these is visible on screen now. This is a logistic model, but this time with autologging turned on. Let's hit the important differences from the previous run. On line 1, we turn on autolog. On line 4, we've got to reimport all of the functions.

Remember how autologging works? MLflow is going to attach function hooks or callbacks to a large number of the standard sklearn functions, and so, if we don't reimport these then invocations of these functions will not be logged. On line 6, we have the with block. The body of the with block is now a lot simpler because we don't have all of those calls to log_params, log_metrics, and so on.

[Video description begins] The input cell 18 is highlighted. Line 1 reads: mlflow.sklearn.autolog(). Line 4 reads: from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score. Line 6 reads: with mlflow.start_run(run_name = 'logistic model with autologging'):. [Video description ends] 

The one exception is the roc_auc_score.

[Video description begins] Line 8 reads: lr_model = LogisticRegression(). Line 10 reads: pipe_lr = Pipeline(steps = [('preprocessor', preprocessor), ('classifier', lr_model)]). Line 11 reads: pipe_lr.fit(X_train, y_train). Line 13 reads: predictions = pipe_lr.predict(X_test). Line 14 reads: predictions_predict_prob = pipe _lr.predict_proba(X_test). Line 16 reads: test_accuracy_score = accuracy_score(y_test, predictions). Line 17 reads: test_precision_score = precision_score(y_test, predictions). Line 18 reads: test_recall_score = recall_score(y_test, predictions). Line 19 reads: test_f1_score = f1_score(y_test, predictions). Line 21 reads: # Needs to be logged explicitly since it is not autologged. Line 22 reads: auc_score = roc_auc_score(y_test, predictions_predict_prob[ :, 1]). [Video description ends]

You can see that on line 23, we explicitly invoke mlflow.log_metric for the AUC_score and that's because the function roc_auc_score is not instrumented by MLflow's autolog feature. The model will be autologged and on line 25, for good measure, we also call infer_signature. Let's go ahead and hit Shift+Enter and you can see here that we get a warning.

[Video description begins] Line 23 reads: mlflow.log_metric('AUC_score', auc_score). Line 25 reads: signature = infer_signature(X_train, pipe_lr.predict(X_train)). [Video description ends]

This is the same warning that we had gotten in the past when we used autolog, and it has to do with the fact that we have no missing data in some of the integer columns in our training dataset. As a result, we get the warning telling us that if those particular variables happen to be missing values at inference time, a model schema enforcement exception will result.

You can see that in the message UserWarning: Hint: Inferred schema contains integer column(s) and then there's a lot more detail which we've already discussed. In this case, we are fine with this setup, but we've also seen how to get rid of this warning if we so choose. So, let's switch over to the MLflow UI, hit Refresh, and we now have a third run in there, this is the logistic model with autologging.

[Video description begins] A page titled mlflow 2.3.2 appears on the browser. The title bar at the top offers 2 tabs: Experiments and Models. The Experiments tab is active now. The navigation pane on the left displays 2 items: Default and customer_churn_prediction. The customer_churn_prediction item is open on the main pane. It shows the header called Description. Below, a search bar appears along with several filter options such as Table view, Chart view, Sort, Columns, and so on. A table appears next with 5 column headers: Run Name, Created, Duration, Source, and Models. It contains 3 items now: logistic model with autologging, logistic model with explanation, and logistic_model. [Video description ends]

Let's click through into this. We see that a lot of information has been logged, 43 Parameters and 12 Metrics.

We expand the list of Metrics, and we can see that everything is in there, including the roc_auc score which we had explicitly logged. It's 0.842. Moving on into the Artifacts section, in addition to the model directory, we also have many additional files. Here, for instance, is the estimator.html. This is a really nice visual representation of our model. It's a Pipeline object and within that, we can see the steps for the preprocessing as well as the LogisticRegression itself. All of the parameters of all of these objects have been autologged and that's why the number of parameters increased so dramatically.

Previously, we were only saving the parameters of the Logistic Regression model object. Here the parameters of the preprocessing pipeline are included as well. Next, let's click on the metric_info.json, this has all of the functions which were autologged. Notice that the roc_auc_score is not included in this JSON dictionary, that's because we call that function explicitly ourselves. MLflow has also very considerately created a training_confusion_matrix.

You can see that it's normalized, we have True labels on the y-axis and Predicted labels on the X-axis and there's a nice heatmap with the scale. There's also a training_precision_recall_curve. Here, on the y-axis, we have the Precision, and, on the X-axis, we have the Recall. The curve indicates a tradeoff between these two metrics. Every point on this curve has an X coordinate corresponding to its recall and y coordinate corresponding to its precision, and every one of those points is obtained from a different choice of threshold in the logistic regression classifier.

Remember that logistic regression relies on an S-curve which is fit to the data and then different thresholds are used to determine on what side the particular point lies. The usual value is 50%. However, this curve tells us how the precision and recall vary for different choices. Let's also examine the training_roc_curve. Here on the y-axis, we have the True Positive Rate, we'd like that to be as high as possible, and on the X-axis, we have the False Positive Rate, we'd like that to be as small as possible.

Every point along this curve is obtained from a different value of the threshold. We'd like to pick a threshold which maximizes the area under the curve. Here you can see that that choice gives an area under curve of 0.84. A perfect classifier would have area under curve of 1 and a random classifier would have area under curve of 0.5 or 50%, like a coin toss. We've now explored autologging with this logistic model in some detail. Let's return to Jupyter and try out a couple of more candidate models.

On screen now, you can see that we continue to use autologging. This time the model is a random forest. Remember that random forests are constructions of many decision trees, each of which is trained using a different subset of features or different subset of data. Those trees are trained in parallel. We run this code and once again, we get the same warning which we previously discussed, that's because autolog is turned on. Then let's try out two more models.

[Video description begins] The input cell 19 is highlighted. Line 1 reads: mlflow.sklearn.autolog(). Line 6 reads: with mlflow.start_run(run_name = 'random forest model default parameters'):. [Video description ends] 

On screen now, we have a support vector classifier. Very broadly speaking, support vector classifiers will try and represent the X data in an N-dimensional hyperspace and then they'll try and find a plane, more precisely a hyperplane, which cleanly separates those points into two groups based on the y label. We hit Shift+Enter and add this run as well successfully to our experiment and then let's scroll down and add one last candidate model. This one is a GradientBoostingClassifier. Like random forest classification, gradient boosting is also an ensemble method, and it too relies on constructing a number of decision trees.

[Video description begins] The input cell 20 is highlighted. Line 1 reads: # Does not support prediction probability. Line 2 reads: from sklearn.svm import SVC. Line 4 reads: mlflow.sklearn.autolog(). Line 6 reads: # Need to re-import these after turning on autolog. Line 7 reads: from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score. Line 9 reads: with mlflow.start_run(run_name = 'SVC model default parameters'):. Line 11 reads: svc_model = SVC(). Line 13 reads: pipe_svc = Pipeline(steps = [('preprocessor', preprocessor), ('classifier', svc_model)]). Line 14 reads: pipe_svc.fit(X_train, y_train). Line 16 reads: predictions = pipe_svc.predict(X_test). Line 18 reads: test_accuracy_score = accuracy_score(y_test, predictions). Line 19 reads: test_precision_score = precision_score(y_test, predictions). Line 20 reads: test_recall_score = recall_score(y_test, predictions). Line 21 reads: test_f1_score = f1_score(y_test, predictions). Line 23 reads: signature = infer_signature(X_train, pipe_svc.predict(X_train)). Line 25 reads: mlflow.set_tag('Classifier', 'SVC default parameters'). [Video description ends] 

However, there is an important difference. Random forest train a large number of decision trees in parallel, and those decision trees are built on a randomly chosen subsets of the data or the X variables. In gradient boosting, the decision trees are not built in parallel, they are built in sequence, and each tree seeks to build on the weaknesses of the preceding tree. For this reason, training a gradient boosting model is more time-consuming, although the hope is that the performance will also be correspondingly better. We hit Shift+Enter and run this code through successfully as well, and that's the fourth run that we've added to our experiment in this demo.

[Video description begins] The input cell 21 is highlighted. Line 1 reads: from sklearn.ensemble import GradientBoostingClassifier. Line 3 reads: mlflow.sklearn.autolog(). Line 5 reads: # Need to re-import these after turning on autolog. Line 6 reads: from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score. Line 8 reads: with mlflow.start_run(run_name = 'gradient boosting model default parameters'):. Line 10 reads: gb_model = GradientBoostingClassifier(). Line 12 reads: pipe_gb = Pipeline(steps = [('preprocessor', preprocessor), ('classifier', gb_model)]). Line 13 reads: pipe_gb.fit(X_train, y_train). Line 15 reads: predictions = pipe_gb.predict(X_test). Line 16 reads: predictions_predict_prob = pipe_gb.predict_proba(X_test). Line 18 reads: test_accuracy_score = accuracy_score(y_test, predictions). Line 19 reads: test_precision_score = precision_score(y_test, predictions). Line 20 reads: test_recall_score = recall_score(y_test, predictions). Line 21 reads: test_f1_score = f1_score(y_test, predictions). Line 23 reads: auc_score = roc_auc_score(y_test, predictions_predict_prob[:, 1]). Line 24 reads: mlflow.log_metric('AUC_score', auc_score). Line 26 reads: signature = infer_signature(X_train, pipe_gb.predict(X_train)). Line 28 reads: mlflow.set_tag('Classifier', 'gradient boosting default parameters'). [Video description ends] 

Let's confirm that by switching over to the MLflow UI. We hit Refresh on the center-right, and indeed we now have a total of 6 matching runs. In the next demo, we will compare these runs and see how we can find the best of these candidate models and proceed with deploying it to production.

7. Video: Comparing Models Programmatically (it_mlflowdj_03_enus_07)

Discover how to compare models programmatically.
compare models programmatically
[Video description begins] Topic title: Comparing Models Programmatically. Your host for this session is Vitthal Srinivasan. [Video description ends]
By this point, we have 6 matching runs in our customer_churn_prediction experiment. It's time now for us to compare these different runs, each of which encapsulated a different model. We are in the MLflow UI, let's click on the Columns button and customize the view. From the drop-down which opens up, we select the AUC_score and then scroll down a little further and add in the f1_score_X_test.

[Video description begins] A page titled mlflow 2.3.2 is displayed on the browser. The title bar at the top offers 2 tabs: Experiments and Models. The Experiments tab is active now. The navigation pane on the left displays 2 items: Default and customer_churn_prediction. The customer_churn_prediction item is open on the main pane. It shows the header called Description. Below, a search bar appears along with several filter options such as Table view, Chart view, Sort, Columns, and so on. A table appears below with 5 column headers: Run Name, Created, Duration, Source, and Models. [Video description ends] 

The UI updates and if we scroll over to the right, you can see that these columns are indeed visible in there. The AUC_score is highest for the gradient boosting model with default parameters, it's 0.939 and by glancing at the f1_score_X_test, we see that there too it's the gradient boosting model that does best. The f1 score is 0.658. Note that not all models have both metrics, for instance, the SVC model does not have the AUC_score recorded, and the first two logistic model runs don't have the f1_score_X_test.

Next, let's select multiple runs from this table and when we do so, the Compare button will get enabled. You can see that we've selected four models, the gradient boosting, the SVC model, the random forest model, and the logistic model. Let's click on Compare, that takes us into the Parallel Coordinates Plot which we've encountered before. Let's simplify this a little bit by removing some of the Parameters that have been chosen over on the left. At this point, we have just one parameter that's the classifier, and one metric that's the training_score. Now this plot is a little easier to interpret.

The SVC classifier has a training R square of just under 0.9, the RandomForest has the highest training R square of 0.95282, and the LogisticRegression has the lowest of 0.81913. Now that we've made sense of this parallel coordinates plot, let's add in some more metrics, this will make the comparison a bit more meaningful. From the drop-down over on the left, we add in the f1_score _X_test and the accuracy_score_X_test.

So, now we have 3 Metrics and the 1 Parameter, that Parameter is just the name of the classifier. As soon as we add in the test metrics, we see that the gradient boosted model jumps to the top of the Metrics chart. You can see that it has a lower training R square than the SVC. However, when it comes to the f1_score on the test data as well as the accuracy_score on the test data, it's the GradientBoosted tree which is the highest.

The f1_score is 0.65823 and the accuracy_score is 85.864%. Remember that in this model comparison pane, we have all of the details for the Parameters, Metrics, and Tags of these 4 runs which we have selected, and you can go through these and manually inspect all of the properties of the corresponding runs. By visually inspecting the Metrics, it seems like it's the gradient boosted tree that did best. Now we are going to try and promote this particular model for use in production.

The catch, however, is that we are going to do so in an automated fashion from the Jupyter Notebook. So, let's switch back over from the MLflow UI to Jupyter and get started. In the first code cell on screen, you can see that we've instantiated a client object. This is of type mlflow.tracking.MlflowClient. Then we invoke the search experiments method on this object. Let's print out the experiments_list.

[Video description begins] The Jupyter notebook titled ModelsRegistrationAndDeployment appears. The input cell 23 is highlighted. Line 1 reads: client = mlflow.tracking.MlflowClient(). Line 2 reads: experiments_list = client.search_experiments(). [Video description ends]

We can see that there are two experiments currently. The second of these is the default experiment. How do we know? Well, because the experiment_id there is 0.

The first element in this list is what we've been working with all along. The name is customer_churn_prediction, and we can use that name as well as its index position within this list of experiments to get a handle to the object. On-screen now, we've invoked mlflow.get_experiment_by_name, and what we pass in is the name of the 0th experiment in the experiments_list. This returns the one experiment which we are interested in, which contains the 6 runs. Remember that the experiment is effectively a container for those runs, and let's get those runs in the form of a search result.

[Video description begins] The input cell 24 is highlighted. Line 1 reads: experiment = mlflow.get_experiment_by_name(experiments_list[0].name). [Video description ends] 

On screen now, you can see that we invoke mlflow.search_runs. The first input argument is the experiment_id. We pass in experiment.experiment_id. Note that that has to be specified as a list because it's possible to search for runs across multiple experiments, that's why the square brackets are around experiment.experiment_id. The second input argument we pass in is the order_by.

Here again, we specify a list. That list has just the one element that's metrics.accuracy_score_X_test. Notice also the qualifier DESC, this will return all of those runs in descending order of the accuracy score on the test data. The return type of mlflow.search_runs is a pandas DataFrame. This is very convenient because then on line 4 of this code cell, we are able to access a subset of the columns run_id, tags.Classifier and metrics.accuracy_score_X_test. We hit Shift+Enter and we do indeed have all 6 runs in descending order of the metric.

[Video description begins] The input cell 25 is highlighted. Line 1 reads: df_run_metrics = mlflow.search_runs(. Line 2 reads: [experiment.experiment_id], order_by = ['metrics.accuracy_score_X_test DESC']). Line 4 reads: df_run_metrics[['run_id', 'tags.Classifier', 'metrics.accuracy_score_X_test']]. [Video description ends] 

You can see that we have the highest value 0.8586 right up top and the missing values NaN at the bottom. We can also confirm that the model which gave the highest accuracy was the gradient boosting with default parameters. Now that we have this nice DataFrame, it's easy enough for us to programmatically access the best run_id.

All of the Run IDs are available in the first column of this pandas DataFrame and because these are sorted in descending order of the accuracy, we can get the run_id at row 0, that's what we've done in the next code cell where we have df_run_metrics and then we employ the loc operator, passing in 0 as the index. The Run ID is displayed out the screen, it's an identifier which starts with 4e9 and ends with ea0f.

[Video description begins] The input cell 26 is highlighted. Line 1 reads: best_run_id = df_run_metrics.loc[0, 'run_id']. Line 3 reads: best_run_id. [Video description ends] 

Let's just confirm that this run_id is correct. Let's switch over to the MLflow UI and look for the run for the gradient boosted tree. It's listed at the top of this table. We click through into it, and right up top we see the Run ID. Indeed, it's the same identifier. It too starts with 4e9 and ends with ea0f. So, we indeed have programmatically gotten the Run ID for this run, and this indeed is the run which had the highest accuracy score on the test data.

Our next step is to load the model from this particular run. Because we've yet to register this model, we are going to have to do so by accessing it at the file URI. You can see here that we've not registered the model because the Register Model button is visible plainly over on the top right. Never fear, we can always access the model directly using a file path. We click on the model, and we see right up top that the Full Path is displayed, that Full Path includes the Run ID. The Run ID, which is that same identifier, which ends with ea0f, is then followed by a /artifacts because that's the name of the directory in which all of these artifacts are stored and then we can specify the model URI as a combination of the Run ID and the model directory name.

Let's switch back to Python, where we have indeed confirmed that we have the best_run_id. Let's use that Run ID in order to print out the classifier name and the run name. In the code cell at the top of the screen now, we access the tags.Classifier and tags.mlflow.runName from the 0th position in df_run_metrics.

[Video description begins] The input cell 27 is highlighted. Line 1 reads: print('Classifier_name :', df_run_metrics.loc[0, 'tags.Classifier']). Line 3 reads: print('Run_name:', df_run_metrics.loc[0, 'tags.mlflow.runName']). [Video description ends]

The Classifier_name is gradient boosting default parameters and the Run_name is gradient boosting model default parameters.

The next step is where we construct the model_uri. Notice how we start with the word runs, followed by a colon and a / then comes the run_id, which here is in the variable best_run_id, and then that's followed by / and model. This is the model_uri and this is what we will be using in order to load our model.

[Video description begins] The input cell 28 is highlighted. Line 1 reads: model_uri = 'runs:/' + best_run_id + '/model'. [Video description ends]

This URI will be a lot easier to construct once we've registered our model and deployed it, but for now, the Run ID is a handy workaround. We can see the model_uri displayed to screen. And now finally, we are ready to actually load in that model.

You see that we import mlflow.sklearn and then call mlflow.sklearn.load_model and specify just the one input argument, the model_uri. We hit Shift+Enter and this code runs through successfully.

[Video description begins] The input cell 29 is highlighted. Line 1 reads: import mlflow.sklearn. Line 3 reads: best_model = mlflow.sklearn.load_model(model_uri = model_uri). [Video description ends]

We do indeed have the best_model displayed out to screen. The nice HTML representation is displayed here, and we can see from the last block in that block diagram that this is indeed the GradientBoostingClassifier. That gets us to the end of this demo, in which we successfully searched for the best model, that is, the model that scored best on a specific metric. We did this first using the UI and then programmatically using the MLflow client APIs and we then constructed the model URI using the Run ID and loaded the model in. In the next demo, we will work with this model and ensure that it gives the same results as the original model that we wrote out into the run.

8. Video: Registering an MLflow Model (it_mlflowdj_03_enus_08)

Find out how to register an MLflow model.
register an MLflow model
[Video description begins] Topic title: Registering an MLflow Model. Your host for this session is Vitthal Srinivasan. [Video description ends]
Let's pick up right from where we left off at the end of the last demo. There we had loaded a model from a specific run that was the run which had the best performance on a specific metric. We had then constructed the model URI using the run_id and now in our code cell, we have the best_model. 

[Video description begins] The Jupyter notebook titled ModelsRegistrationAndDeployment appears. It contains an address bar at the top. Under this, there is a toolbar with options like File, Edit, View, Insert, Cell, Kernel, Widgets, and Help. Adjacent to this, there are 2 buttons labeled Trusted, and mlflow_venv. [Video description ends] 

[Video description begins] The input cell 29 is highlighted. Line 1 reads: import mlflow.sklearn. Line 3 reads: best_model = mlflow.sklearn.load_model(model_uri = model_uri). Line 5 reads: best_model. [Video description ends]

This has been displayed out to screen. You see the nice HTML representation, which is the same as what we had in estimator.html in the model artifacts directory. This is also the pretty standard representation of any scikit-learn pipeline. And what's even better is that this is interactive. Let's click around through this pipeline and ensure that this does indeed have everything that we had set up in the original model before logging it to MLflow. Indeed, everything does seem to be in order.

And notice again that this is the GradientBoostingClassifier that we have loaded into our notebook. Let's now turn to the prediction using this model. You can see here that we pass in the X_test variable which we previously had created into the predict method on best model.

[Video description begins] The input cell 30 is highlighted. Line 1 reads: predictions_loaded = best_model.predict(X_test). Line 3 reads: predictions_loaded. [Video description ends]

On screen now are all of the predictions. Again, these are the predictions from the model that we got back from MLflow. Let's compare these to the predictions from the model that we wrote into MLflow that was in a variable called pipe_gb. The gb stands for gradient boosting. This runs through as well.

[Video description begins] The input cell 31 is highlighted. Line 1 reads: predictions_original = pipe_gb.predict(X_test). Line 3 reads: predictions_original. [Video description ends]

This is a variable called predictions_original and let's now test the predictions_loaded and predictions_original for equality. We'll do this with the assert that you now see on screen and when we hit Shift+Enter, there is no exception or error.

This proves that the predictions from the loaded model are identical to those from the model that we wrote out. We're feeling pretty good about this model now and so it's time for this model to be taken to the next stage. We are going to register this model and that will take us out of the ambit of MLflow Tracking and into the MLflow Model Registry. We pick a model name that's churn_prediction_model and then we invoke mlflow.register_model and we pass in the path of the model in the runs directory. So, please note that we are not passing in the model that we loaded in from MLflow, rather we are passing in the model_uri. That's the first input argument, the second is the model_name. 

[Video description begins] The input cell 32 is highlighted. Line 1 reads: assert(np.array_equal(predictions_loaded, predictions_original)). [Video description ends]

[Video description begins] The input cell 33 is highlighted. Line 1 reads: model_name = 'churn_prediction_model'. Line 3 reads: model_version = mlflow.register_model(f'runs:/{best_run_id}/model', model_name). [Video description ends]

We hit Shift+Enter and we see that this goes through successfully. Successfully registered model 'churn_prediction_model'. And you can also see that this has been Created as version 1 of that model.

This step of registering the model is something of a coming-of-age ceremony. Now, this registered model is going to exist in the Model Registry in MLflow. It will have a unique name. It can support versions as well as stages such as production, archived, and so on. It also will support model lineage and additional metadata. It will also be a lot easier to invoke this model. As you can see down below, towards the bottom of the screen, the model_uri is now going to begin with models: and include the model_name rather than being hacked together from the run_id.

[Video description begins] Line 5 in input cell 36 is highlighted. It reads: model_uri = f'models:/{model_name}/{stage}'. [Video description ends]

However, before we get to the next cell in this Jupyter Notebook, we've got to switch over to the MLflow UI and there we'll find some exciting changes in how our model appears. Now when we switch back to the MLflow UI, it so happens that we are on the page for the best run.

You can see that this is the gradient boosting model default parameters.

[Video description begins] A page labeled gradient boosting model default parameters appears within mlflow. It displays various details at the top in fields like Run ID, Date, Source, Git Commit, User, Duration, Status, and Lifecycle Stage. Below, 5 collapsible sections appear: Description, Parameters, Metrics, Tags, and Artifacts. [Video description ends]

It's the model from this run that we just registered. And now if you look towards the center-right of the screen, you no longer see a register model button. Instead, you see a message which reads churn_prediction. The rest of the model name is redacted. But then there's the v1 and we also see that this model was registered on the 27th of June 2023. There's also a little green check mark next to the model name. This confirms that the model registration has gone through successfully. Let's double-check this. Let's click on the experiment name up top. Let's again click on the run gradient boosting model default parameters and come back. And indeed our eyes have not deceived us.

The model has indeed been registered. Now let's click on this link and this opens up a new tab. For the first time, you see that we are in the Models tab. Everything that we've done so far was in the Experiments tab, but now we are in the Models tab. And here we have information about the model that we just registered. We'll explore this in more detail but in a simpler fashion. Let's switch back to the first pane. Here we are still in the Experiments tab. Let's scroll back up to the top and click on Models. That will take us back to where we had the registered model information. You can see that we can also create entirely new models here by clicking on that Create Model button in the top left. However, for now, it's the table that we are interested in. Within that table, we have just the one model, the churn_prediction_model.

That's a clickable link and we will click through in just a moment. But first, let's look at the Latest Version. That's Version 1. There are also columns called Staging and Production. Notice that for each of those two named stages, we can have different versions. So, they can be the latest model that's in Staging and the latest model that's in Production and these need not be the same. In any case, let's now click through into the details of this churn_prediction_model. In the view that opens up, we have the Description, Tags, and Versions. There's only one version here right now that's Version 1. We click through into that and now we get the details of this particular version. Notice right up top the Source Run. This is a clickable link which will take us back to the Experiments page and the run which was used to create this model.

Then in the Schema section, we have the Inputs as well as the Outputs, and you can see that the Inputs include the Names as well as the corresponding Types. Let's also expand the Outputs. There's just the 1 output, there's no name, but the type is Tensor dtype: int 64. Next, let's scroll to the top and look at the stages associated with this version. Right now, the Stage is set to None, but if we click on that, you see that we can Transition it to either Staging or Production or Archived. Indeed, these are the four accepted values for the model stage, None, Staging, Production, and Archived. Let's click on the Stage transition. So, we'd like to transition this model from Stage None to Stage Staging.

A dialog opens up and you see here that there's also a little check box which is checked by default. It asks whether we like to Transition existing staged model versions to Archived. We don't have any such right now so we go ahead and click OK. The displayed stage changes. It's now yellow for Staging. We click on Registered Models over on the top left. That takes us back out and we can see now that our churn_prediction_model has an entry under the column Staging. That version is Version 1, Version 1 is also the Latest Version.

There is nothing which dictates that all transitions have to only be forward. We have now clicked through into our model and gone back into the versions and we are going to change the stage back from Staging to None. So, in effect, we are going to demote our model. MLflow is totally fine with this. There's no further prompt. We can either Cancel or hit OK. Right now, let's hit Cancel, and click back into Registered Models. Our stage is set to Staging and with the model in this stage, let's switch back to Jupyter and see how we can access this model and make some predictions.

9. Video: Modifying Registered Model Versions (it_mlflowdj_03_enus_09)

In this video, find out how to modify registered model versions and read models into Python.
modify registered model versions and read models into Python
[Video description begins] Topic title: Modifying Registered Model Versions. Your host for this session is Vitthal Srinivasan. [Video description ends]
We are now back in Jupyter where the last operation that we had performed was to register our model. At that point, it switched over to the MLflow UI and then explored the Model Registry and seen how to change the stage of the model to staging.

[Video description begins] The Jupyter notebook titled ModelsRegistrationAndDeployment appears. It contains an address bar at the top. Under this, there is a toolbar with options like File, Edit, View, Insert, Cell, Kernel, Widgets, and Help. Adjacent to this, there are 2 buttons labeled Trusted, and mlflow_venv. [Video description ends]

Let's now pick up the action with the next bit of code that you now see on screen. We instantiate a new MLflow client. For that, we first import MlflowClient from mlflow.tracking, and then, we invoke the function get_model_version_stages on this client object. We specify the name of the model as well as the version and we get a list of all of the accepted version stages.

[Video description begins] The input cell 34 is highlighted. Line 1 reads: from mlflow.tracking import MlflowClient. Line 3 reads: client = MlflowClient(). Line 5 reads: client.get_model_version_stages(name = model_name, version = model_version.version). [Video description ends]

These are None, Staging, Production, and Archived.

Next, we are going to try and transition our model from its current stage which is Staging to the stage Production. The client API method has an intuitive name transition_model_version_stage. We've got to specify the model_name, the model_version, and stage that we would like to move it to. Here the stage is Production. We hit Shift+Enter and this process goes through successfully as we can see from the returned ModelVersion object. Notice the current_stage='Production'. Also, if you look closely, you can see that the last element is the version which is equal to 1. Also, notice the source attribute. That's a long file path.

[Video description begins] The input cell 35 is highlighted. Line 1 reads: client.transition_model_version_stage(. Line 2 reads: name = model_name,. Line 3 reads: version = model_version.version,. Line 4 reads: stage = 'Production',. Line 5 reads: ). [Video description ends]

If we look at the middle of the file path, we see the run ID, which is the identifier that starts with 4e9 and ends with ea0f. The model itself is then available in the artifacts folder within the run ID directory, and within the artifacts folder, there is a further subfolder called model. That's the location of the source. We can use this directory location in order to deploy our model and use it for inference, and we'll get to that at a later point in this demo. Now, let's switch over to the MLflow UI. This UI has yet to refresh, but let's click-through into the churn_prediction_model, and there we see that we have Version 1, and the Stage is now Production. It previously was Staging. So, we were successfully able to change the Stage of the model from Staging to Production.

Now, let's click on that Version 1 that's clickable. The Stage reflects as Production here as well, so we verified that that work. Let's switch back to Python. There we see that the contents of code cell 35 were indeed executed successfully. Let's move on to the next cell where we attempt to load model. But we are going to do this in a different fashion than we did previously. There are two differences between the code you now see on screen and the code we executed a moment ago in the previous demo. Here we are loading using the API function mlflow.pyfunc.load_model. You see it here on line 4 of this code cell.

Previously we had made use of mlflow.sklearn.load_model. So, here we are loading in the Python flavor of the model. Previously we had loaded in the sklearn flavor. The second bit that's new here is the URI that we pass in. Previously we had made use of a model_uri which began with runs, so we were accessing the runs directory, and reaching into the models directory within that. Here, on the other hand, the model_uri that we construct on line 5 is much more abstracted. It starts with the word models followed by a colon and a forward slash rather than runs. And then instead of using run ID which is just some internal identifier, we make use of the model_name. You can see that defined on line 1, it's churn_prediction_model, so that's very human-readable.

What follows after that is the stage that is equal to Production. And once we've loaded this model using the pyfunc.load_model method, we invoke the predict method on the loaded_model, that's on line 8, towards the right. We pass an X_test and we compare these predictions to the y_test, and we compute the f1_score.

[Video description begins] The input cell 36 is highlighted. Line 1 reads: model_name = 'churn_prediction_model'. Line 2 reads: stage = 'Production'. Line 4 reads: loaded_model = mlflow.pyfunc.load_model(. Line 5 reads: model_uri = f'models:/{model_name}/{stage}'. Line 6 reads: ). Line 8 reads: print(f'F1 score: {f1_score(y_test, loaded_model.predict(X_test))}'). [Video description ends]

Let's run this code. And there are two bits here worth commenting on. The first is the F1 score itself, which is 0.65822. This matches the value which we had obtained when we ran the run in the first place, and the second is this WARNING. Now, this WARNING requires a little bit of explanation.

The precise wording is mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current Python environment. The mismatch follows mlflow, the current version is 2.3.2, but what's required is just 2.3. The message goes on to tell us that we can fix the mismatches by calling mlflow.pyfunc.get_model_dependencies to fetch the model's environment and that we should then install those dependencies using the resulting environment file.

Now, that advice seems reasonable enough, but the warning itself is pretty peculiar. After all, we just created this model in this same Jupyter Notebook. We haven't changed our version of MLflow. Why then is the warning being thrown? Also note that this warning did not appear when we tried to load in the model using mlflow.sklearn.load_model.

So, this is something that's specific to the mlflow.pyfunc.load_model method. To see what's causing this warning, we'll switch over to the MLflow UI. There's a little quirk there, quite possibly a bug, which explains why this warning is being thrown. Here we are in the MLflow UI. Let's click on the churn_prediction. This is now the model. We click through into Version 1. Let's now click on the Source Run. This is gradient boosting model default parameters. This is where in the Artifacts folder, we can see the real details of the model.

Let's click on the MLmodel file and if we look closely here, we can see that the version of MLflow that's specified is 2.3.2. You can see there right above the model_uuid. Now look at the flavors. In the python_function flavor, in the env section, we see a reference to conda.yaml. Let's click on conda.yaml and if you look closely there, you'll see that the version of mlflow has been recorded slightly differently. There it shows up as 2.3 rather than 2.3.2. This is in the dependencies section of conda.yaml and that in turn serves as the source for requirements.txt. And if we look in requirements.txt, sure enough, we find the same mismatch. The version of mlflow here is again recorded as 2.3 rather than as 2.3.2.

And finally, the reason why this only occurs while loading the Python flavor is also visible. It's because, in the MLmodel file, we only reference the conda.yaml in the Python flavor and not in the sklearn flavor. We've clicked back into the MLmodel file and you can verify this for yourself. So, in summary, this warning that we got in our Jupyter Notebook seems to be a quirk or a bug in the way in which the conda.yaml file records the MLflow version number, and it doesn't seem like we need to worry about it.

That's reassuring to know, because, after all, mismatches in the environment can lead to nasty errors at runtime in a production setting. We can now turn our attention back to the returned F1 score, which was 0.6582. Let's quickly verify that this is correct. We do that by indexing into the df_run_metrics. We had already sorted these by the F1 score, so using loc0 gives us the best F1 score.

[Video description begins] The input cell 37 is highlighted. Line 1 reads: df_run_metrics.loc[0, 'metrics.f1_score_X_test']. [Video description ends]

And that best F1 score is indeed 0.6582, the value that we just saw a moment ago. And in this way, we can satisfy ourselves that the F1 score of the model that we just loaded using mlflow.pyfunc.load_model indeed matched the model that we had created and logged during the run.

10. Video: Registering Another Model and Viewing the Registered Model (it_mlflowdj_03_enus_10)

Learn how to register another model and view the registered model.
register another model and view the registered model
[Video description begins] Topic title: Registering Another Model and Viewing the Registered Model. Your host for this session is Vitthal Srinivasan. [Video description ends]
At this point, we have one model which we have registered into MLflow's Model Registry. We selected that model on the basis of all of the runs in our experiment and we picked that run which had the best F1 score. You see that F1 score on screen now.

[Video description begins] The Jupyter notebook titled ModelsRegistrationAndDeployment appears. It contains an address bar at the top. Under this, there is a toolbar with options like File, Edit, View, Insert, Cell, Kernel, Widgets, and Help. Adjacent to this, there are 2 buttons labeled Trusted, and mlflow_venv. [Video description ends] Let's now publish yet another model. That is, let's have another model registered with the registry. This will be the one with the best recall score rather than the best F1 score. You see the code for this on screen now. We've invoked mlflow.search_runs. The first input argument is the experiment_id and the second is the order_by clause. Now the order_by clause reads metrics.recall_score_X_test, again with the qualifier DESC for descending.

As a result, we are going to get back all of the runs in this particular experiment, with the best recall score appearing first. From the results, we can see that this is the random forest.

[Video description begins] The input cell 38 is highlighted. Line 2 reads: [experiment.experiment_id], order_by = ['metrics.recall_score_X_test DESC']). [Video description ends]

The gradient boosted model which we had used so far, comes in second when sorted by the recall. Also, note how the output is in the form of a pandas DataFrame. Let's print out some details for this new best run based on the recall. We do this by reaching into the same DataFrame and looking for the columns called tags.Classifier, tags.mlflow.runName, and metrics.recall_score_X_test. These input arguments give us a good sense of how the object model is organized within MLflow. The outputs appear down below.

[Video description begins] The input cell 39 is highlighted. Line 1 reads: print('Classifier_name :', best_run_recall_metrics.loc[0, 'tags.Classifier']). Line 3 reads: print('Run_name :', best_run_recall_metrics.loc[0, 'tags.mlflow.runName']). Line 5 reads: print('Best recall score:', best_run_recall_metrics.loc[0, 'metrics.recall_score_X_test']). [Video description ends]

We have the Classifier_name, the Run_name, and the Best recall score, all printed out to screen.

Once we have the run_id for this best run, we can then invoke mlflow.register_model as before. Notice that once again, on line 2, we construct a model_uri. This is done using the run_id and the directory structure within that where we have runs:/ followed by the run_id, followed by the model directory name. What's more interesting here is the second input argument, the one on line 3. That's the model_name. We've already registered a model with this model_name. Let's see what happens when we hit Shift+Enter. The result is success, Registered model 'churn_prediction_model'.

[Video description begins] The input cell 40 is highlighted. Line 1 reads: best_recall_model = mlflow.register_model(. Line 2 reads: f"runs:/{best_run_recall_metrics.loc[0, 'run_id']}/model",. Line 3 reads: model_name. [Video description ends]

And if we look closely, we can see in the last line of the output that this has been registered as version 2. Let's confirm this by switching over to the MLflow UI.

[Video description begins] A page labeled gradient boosting model default parameters appears within mlflow. It displays various details at the top in fields like Run ID, Date, Source, Git Commit, User, Duration, Status, and Lifecycle Stage. Below, 5 collapsible sections appear: Description, Parameters, Metrics, Tags, and Artifacts. [Video description ends]

We navigate into the Models section and there we see that we have just the one model churn_prediction_model.

The Latest Version is Version 2. If we look under Staging, we have nothing in there. If we look under Production, we have Version 1. So, Version 2 is the latest version, but it doesn't yet have a stage assigned to it. We can confirm that by clicking on Version 2. That takes us into the details of this particular version and you can see that we have the Source Run which is random forest model default parameters and the Stage in the top right which is set to None. This version doesn't have a Description. We could Edit it from here, but instead, let's switch back to Python and do it programmatically. You can see that we now invoke the client.update_model_version method. We specify the name and the version. So, version is 2, name is churn_prediction_model, and we add in a description, This model had the best recall score.

We run this code cell.

[Video description begins] The input cell 41 is highlighted. Line 2 reads: name = 'churn_prediction_model',. Line 3 reads: version = 2,. Line 4 reads: description = 'This model had the best recall score'. [Video description ends]

It seems to go through successfully. The model version that's returned does indeed have the description. This model had the best recall score. We can further confirm that this change is reflected in the MLflow UI. But before that, let's also add in a description to version 1. Here it is. We invoke client.update_model_version yet again. The model name that we pass in is the same churn_prediction_model, but this time the version is equal to 1 rather than 2. The description we pass in is, 'This model had the best accuracy score'. That's true as well because this particular model, the gradient boosted version, had both the best accuracy score and the best F1 score.

[Video description begins] The input cell 42 is highlighted. Line 2 reads: name = 'churn_prediction_model',. Line 3 reads: version = 1,. Line 4 reads: description = 'This model had the best accuracy score'. [Video description ends]

In any case, this runs through successfully.

The model version that's returned has the description, we just specified, and you can also see that the version which is the last attribute is equal to 1. Next, let's switch over to the MLflow UI. Then we click on Registered Models. You can see here that we have just the one model, churn_prediction_model. The Latest Version is 2. We click through into it and we see that there are two versions, Version 2, the Description reads, This model had the best recall score, and the Stage is set to None. And then there's Version 1 where the Stage is set to Production, and the Description references the best accuracy score instead. Let's click through into Version 1 and verify that the description is updated there as well, and indeed it is.

Let's go back out into the model, the churn_prediction_model, and double-check the description of Version 2 as well. This is also correct. So, we've correctly and successfully now registered two versions of the churn_prediction_model and also programmatically edited their description. Next, let's perform some stage transition. So, we click on the model, and then we click through into Version 1. This is currently Stage: Production, but we can transition that, and we do, we Transition it to the Archived state. When we set this transition, we are simply prompted for Cancel or OK. We go with OK and this Version 1 is now Archived. The obvious next step is to promote Version 2 into Production. Let's do that programmatically. So, we switch back to Python and we run this method, client.transition_model_version_stage.

The name of the model is model_name, version is 2, and the stage is equal to Production. We hit Shift+Enter and the transition goes through successfully.

[Video description begins] The input cell 43 is highlighted. Line 1 reads: client.transition_model_version_stage(. Line 2 reads: name = model_name,. Line 3 reads: version = 2,. Line 4 reads: stage = 'Production'. [Video description ends]

We can see that from the ModelVersion that's returned where the last attribute version is equal to 2. Also, the current stage is equal to Production. We see that in the first line of the output. This change is immediately picked up by the MLflow UI. We switch back over, we click on churn_prediction_model and there we see that we still have the two versions. However, now Version 2 is in Production, and Version 1 has been Archived. So, all of the changes that we've made, either through the UI or programmatically, are correctly reflected in MLflow UI at this point.

11. Video: Serving Models to a Local REST Endpoint (it_mlflowdj_03_enus_11)

During this video, discover how to serve models to a REST endpoint and access that endpoint.
serve models to a REST endpoint and access that endpoint
[Video description begins] Topic title: Serving Models to a Local REST Endpoint. Your host for this session is Vitthal Srinivasan. [Video description ends]
We ended the previous demo with version 2 of our churn_prediction_model promoted to Production. Now let's actually use this model to make some inferences. For now, we will do this from our local machine using curl. We switch over back to Jupyter and the first thing we do there is to get the run_id for this particular run. We are going to deploy our model to an endpoint and hit that endpoint using curl. To get started, let's find the run_id where this model was created. Here it is on screen now. We've obtained this by indexing into our best_run_recall_metrics Pandas DataFrame.

[Video description begins] The Jupyter notebook titled 'ModelsRegistrationAndDeployment' is open. Code cell 44 reads: best_run_recall_metrics.loc[0, 'run_id']. [Video description ends]

Note that this identifier starts with 807 and ends with 879.

With this information, let's switch over to a terminal window. To begin with, you can see from the prompt that we are in our up-level MLflow directory and we also have the prompt with the virtual environment, mlflow_venv. We run an ls -l here and we see that we have a directory there called mlruns. Within that mlruns directory, we have individual directories corresponding to each run_id and we can use the models which are present inside the artifact store of each of those run directories.

[Video description begins] A Terminal window appears. It displays the following command line: (mlflow_venv) ~/projects/mlflow. The command inserted reads: ls -l. The next command reads: mlflow models serve -m runs:/<run_id>/model --env-manager local --host 127.0.0.1:1234. [Video description ends]

We are going to do that and we are also going to introduce for the first time the MLflow model's serve command. This command will serve a model which was saved by MLflow and it will do so by launching a web server on the specified host and port. Here you can see that the host is localhost 127.0.0.1 and the port we specify is 1234.

In addition, we also have to specify an environment manager. We've specified --env-manager local that will just use the local Python environment, which is fine because we already are within our virtual environment. We could also specify --env-manager to be either virtual env or conda. And then we have the -m flag. -m indicates that what follows is the model_uri. This model_uri has a specific format. Here we have runs:/ and then the run_id followed by the model directory. Here the model directory is simply called model. However, the run_id placeholder needs to be replaced with the actual run_id which we will get from Jupyter. So, let's very quickly switch over to Jupyter, copy the run_id that we had printed out to screen there, and splice it into this URI right here. At this point, if we hit Enter, our model will become available on a web server, and we can hit it at localhost: 1234.

Let's leave this server running. If we look at the INFO messages, we can see that the backend flavor selected is python_function. We also have the same warning down below. mlflow.pyfunc: Detected one or more mismatches between the model's dependencies and the current environment. The server is ready. Let's open up another terminal window with which to hit this server with curl. So, we've now opened a New Tab, and in that tab, we paste in a curl command. You can see that in this curl command, we specify the X values that we wish to pass into the model in the form of JSON. The exact command is curl -X POST -H "Content-Type:application/json" and then the data.

That data is a JSON fragment.

[Video description begins] The following command is inserted: curl -X POST -H "Content-Type: application/json" --data '{"dataframe_split": {"columns":["Age","FrequentFlyer","AnnualIncomeClass","ServicesOpted","AccountSyncedToSocialMedia","BookedHotelOrNot"],"data":[[35, "Yes", 2, 1, "No", "No"]]}}' http://127.0.0.1:1234/invocations. [Video description ends]

We can see within that the specific format which we've gotten from the MLflow docs, dataframe_split, and then come the columns. The columns consist of a list of the column names, Age, FrequentFlyer, AnnualIncomeClass, ServicesOpted, AccountSyncedToSocialMedia, and BookedHotelOrNot. Then come the data items. That is the actual values of those X variables. So, the Age is 35, FrequentFlyer is Yes, AnnualIncomeClass is 2, ServicesOpted is 1, AccountSyncedToSocialMedia is No, as is BookedHotelOrNot. And the last input argument is the exact path that we've got to hit. That's http followed by the localhost and the port 1234, and that's followed by invocations. That's just the standard boilerplate that we've got to use if we'd like to hit this MLflow server.

We hit Enter and we get a response. This response is also a JSON fragment with just the one prediction, and that prediction takes the form of a single-element list. That one element in the list is simply the number 1. A predicted value of 1 indicates that this customer is expected to churn. 1 stands for churn, 0 stands for no churn. Let's try with another choice of inputs and see if we get a different response. This is yet another curl command. All that differs this time is the X values that we pass into the data element in the JSON.

You can now see that the Age is 23, for instance, rather than 35.

[Video description begins] The highlighted fragment of the code reads: "data":[[23, "No", 1, 0, "No", "No". [Video description ends]

We send these values of the X variables to our server and we get back the prediction. This time that prediction is 0. So, the second prediction is that the customer will not churn. Again, remember that 0 indicates no churn, 1 indicates customer churn. And that gets us to the end of this demo in which we were successfully able to serve up our MLflow model and then we were able to hit that model using curl and get differing prediction values. In the next demo, we are going to take things one level up and deploy our model to Microsoft Azure.

12. Video: Creating an Azure Machine Learning (Azure ML) Account (it_mlflowdj_03_enus_12)

In this video, you will learn how to create an Azure ML workspace and register a model to Azure.
create an Azure ML workspace and register a model to Azure
[Video description begins] Topic title: Creating an Azure Machine Learning (Azure ML) Account. Your host for this session is Vitthal Srinivasan. [Video description ends]
In this demo, we are going to switch to using Azure deployment. We are going to register our MLflow model with Azure and then we are going to deploy it via an Azure endpoint. Most of this process is going to be conducted from within our Jupyter Notebook and we would also like to create our Azure workspace programmatically from within Jupyter. However, there is one pre-step which requires us to start out in the Azure portal. You can see that we've hit the URL portal.azure.com in our browser and we quickly go through the sign-in process.

[Video description begins] A page titled Sign in to Microsoft Azure appears. Below, the header reads: Approve sign-in request. It displays number 16 for signing in. [Video description ends]

Soon enough, we find ourselves on the main page where we have all of the recently created resources.

[Video description begins] The Microsoft Azure home page appears. It contains a search bar at the top along with some icons like the notification icon, the settings icon, and so on. Adjacent to this, there is an icon displaying the contact information. Below, various sections appear such as Azure services, Resources, Navigate, and so on. The Azure services section contains several icons like Create a resource, Resource groups, Subscriptions, and so on. The Resources section contains 2 tabs: Recent, and Favorite. The Recent tab is active. It contains a table with 3 column headers: Name, Type, and Last Viewed. [Video description ends]

In case you are new to Azure, here's a very quick word on the organization. We sign in to Azure with an account.

Currently, the account is visible over on the top right. That's contact@loonycorn.com. Within an account, we can have multiple Subscriptions. Each subscription is associated with billing and payment info. Then within subscriptions, we have groups of billable resources, and those groups are called Resource groups. You can see over up top we have two icons, one for Resource groups, and just to the right of that, we have an icon for Subscriptions. Let's click on the Resource groups first. At this point, we see that there are three resource groups associated with this subscription.

[Video description begins] A page titled Resource groups appears. It contains various buttons at the top like Create, Manage view, Refresh, and so on. Below, there is a search bar for different filters. There are 2 drop-downs displayed on the right labeled No grouping and List view. Below, there is a table with 3 column headers: Name, Subscription, and Location. [Video description ends]

The Subscription name is also visible, it's loony-azure-paid-subscription-01. The first thing we are going to do is create a new resource group for the purposes of this demo. We click on the + Create button and go through the dialog that follows.

[Video description begins] A page titled Create a resource group appears. It displays a command bar at the top with 3 tabs: Basics, Tags, and Review + create. The Basics tab is active. It contains 2 sections labeled Project details, and Resource details. The Project details section contains 2 fields: Subscription and Resource group. The Resource details section contains a field labeled Region. [Video description ends]

We give this a descriptive name. We've got to choose a Region for the compute resources. That's (US) East US. And then we quickly go through all of the defaults.

[Video description begins] The Tags and Review + create tabs are selected one after the other. [Video description ends]

We accept all of them and by the end, our Resource group has been created. We are now inside that Resource group. You see the name over on the top left loony-mlflow-rg and this is a container within which we can add additional resources. So, let's click on the Create resources button in the center of the screen and proceed to construct an ML workspace.

[Video description begins] A page titled loony-mlflow-rg appears. The navigation pane on the left contains various sections. The first section consists of options like Overview, Activity log, Tags, Events, and so on. The second section labeled Settings contains options like Deployments, Security, Policies, and so on. The Overview option is highlighted. The main pane displays various buttons at the top, such as Create, Manage view, Refresh, and so on. Below, there are 2 tabs: Resources, and Recommendations. The Resources tab is active. A search bar appears next with different filters followed by an empty table with 3 column headers: Name, Type, and Location. Below, there are 2 buttons labeled Create resources, and Clear filters. [Video description ends] 

[Video description begins] A page titled Marketplace appears. The navigation pane on the left contains various sections like Management, My Marketplace, Categories, and so on. The first section contains options like Get Started, and Service Providers. The Get Started option is highlighted. The main pane contains a search bar for different filters. Below, there is a section labeled Recommended for you. It contains various tabs. [Video description ends]

The service that we are looking for is called Azure Machine Learning.

[Video description begins] A page titled Azure Machine Learning appears. It displays a drop-down field labeled Plan along with a button labeled Create. Below, there are various tabs, such as Overview, Plans, Usage Information + Support, and Ratings + Reviews. The Overview tab is active. [Video description ends]

That's the flagship machine learning service offered on Azure. Let's go ahead and hit Create. We are now taken through the dialog to create a Machine Learning workspace.

[Video description begins] The Azure Machine Learning page now displays various tabs below, such as Basics, Networking, Advanced, Tags, and Review + create. The Basics tab is active. It contains 2 sections labeled Resource details, and Workspace details. The Resource details section contains 2 fields: Subscription, and Resource group. The Workspace details section contains several fields like Workspace name, Region, Storage account, and so on. [Video description ends]

We give this workspace a name, let's call it loony-test, and then, we assign the Region East US.

Once again, we'll go through all of the steps in the dialog without changing any of the defaults, and soon enough, our Azure Machine Learning workspace has been created.

[Video description begins] The other tabs like Networking, Advanced, Tags, and Review + create are selected one after the other. [Video description ends]

Now, this Azure Machine Learning workspace itself has a lot of resources within it.

[Video description begins] A page titled Microsoft.MachineLearningServices | Overview appears. The navigation pane on the left contains options like Overview, Inputs, Outputs, and Template. The Overview option is highlighted. The main pane contains various buttons at the top, such as Delete, Redeploy, Download, and so on. The following header appears next: Your deployment is complete. Below, there are 2 collapsible sections labeled Deployment details and Next steps. A button labeled Go to resource is available. [Video description ends]

Those resources take some time to be deployed, but finally, we can Go to resource. This takes us to the loony-test Azure Machine Learning workspace. We can see the name over on the top left and the details are visible at the center of the screen. The next step is to launch Azure Machine Learning Studio. We can think of this as the starting point for pretty much anything machine learning-related that we wish to do in Azure. We are going to click on that bright blue Launch studio button, but not quite yet. For now, we are going to scroll back to the top and click on the Home link over on the top left.

[Video description begins] A page titled loony-test appears. The navigation pane on the left contains various sections. The first section consists of options like Overview, Activity log, Tags, Events, and so on. The second section labeled Settings contains options like Networking, Properties, and Locks. The Overview option is highlighted. The main pane displays 2 buttons at the top: Download config.json and Delete. Below, there is a section labeled Essentials. It contains various information in fields like Resource group, Studio web URL, Location, and so on. Under this, there is a section that reads: Work with your models in Azure Machine Learning Studio. It contains a button labeled Launch studio. [Video description ends]

That takes us back out to Azure services. There we click on Resource groups and we observe something interesting. The last time we were on the Resource groups page, we had noted 3 resource groups. After that, we created one resource group. That's the loony-mlflow-rg that you see in the center. But when we now look at the count of resource groups, we see five records. So, there is one additional resource group which has come from somewhere, and that's the first entry here, the DefaultResourceGroup-eastus. It turns out that the first time we create an Azure Machine Learning workspace, that also leads to the creation of another resource group. That's the one that you see right here.

This resource group will then be shared across all Azure Machine Learning workspaces in this account. This is why we had to go through this process using the UI. Had we attempted to programmatically create an Azure Machine Learning workspace from Python the first time around, this internal resource group would not have existed and the process would have failed. In any case, we have now jumped through this hoop. Just out of curiosity, let's click through into this and you will notice that this particular resource group has just one resource and that is the DefaultWorkspace.

[Video description begins] A page titled DefaultResourceGroup-eastus appears. The Overview option is highlighted on the navigation pane. The main pane contains various buttons at the top, such as Create, Manage view, Refresh, and so on. Below, there are 2 tabs: Resources, and Recommendations. The Resources tab is active. It displays a search bar for filters at the top. A table appears next with 3 column headers: Name, Type, and Location. [Video description ends]

You can see it listed towards the center-right of the screen. The name is DefaultWorkspace-eastus and the Type is Log Analytics workspace. So, it's this Log Analytics workspace that must exist in order for any programmatic creations of ML workspaces to be successful. We are now almost ready to create our workspace from Jupyter.

But first, we need our subscription ID. So, let's click on the Home link in the top left. Then let's click on the Subscriptions section and that takes us into the details of the one subscription that we do have. What we are interested in is the Subscription ID, which is visible right here, but not completely.

[Video description begins] A page titled Subscriptions appears. It displays various buttons at the top such as Add, Manage Policies, View Requests, and so on. Under this, there is a search bar for various filters. Below, there is a table with several column headers like Subscription name, Subscription ID, My role, Current cost, and so on. [Video description ends]

So, let's click through. We now have the Subscription ID with a nice Copy to clipboard button next to it.

[Video description begins] A page titled loony-azure-paid-subscription-01 appears. The Overview option is highlighted on the navigation pane. The main pane contains a section labeled Essentials. It displays various information fields like Subscription ID, Subscription name, Directory, and so on. [Video description ends]

Let's copy that and switch over to Jupyter and paste that into our create workspace call.

[Video description begins] The Jupyter notebook titled ModelsRegistrationAndDeploymentInAzureMLStudio appears. The Subscription ID is pasted in line 5 of the input cell 4. [Video description ends]

That is line 5 of the fourth code cell that you currently see on screen. On line 3 of that code cell, you see ws = Workspace.create. Now, let's very quickly run through this code.

We start out by installing some of the required libraries with pip install of Azure ML Code, azure-ai-ml, azure-identity, and azureml-mlflow.

[Video description begins] Input cell 1 reads: pip install azureml-core. Input cell 2 reads: pip install azure-ai-ml azure-identity. Input cell 3 reads: pip install azureml-mlflow. [Video description ends]

Once that's done, we finally get to the point where we can create our machine learning workspace programmatically. On line 1 of this code cell, we import workspace from azureml.core. Then we invoke Workspace.create, passing in the name, subscription_id, resource_group, whether we'd like to create this resource group or not that's set to False. And finally, the location, which is eastus. Do note the workspace name on line 4. That's loony-mlflow-wsp. We'll look out for it in the resource group in the Azure portal.

[Video description begins] The input cell 4 is highlighted. Line 1 reads: from azureml.core import Workspace. Line 4 reads: name = 'loony-mlflow-wsp',. [Video description ends]

This command takes a while to run through because there are a large number of resources that need to be deployed. We have seen these even while creating the workspace from the UI.

Once that's successful, we switch back over to the Azure portal, navigate into our resource group, loony-mlflow-rg. And when we click into it, we can see in the list of resources at the center of the screen loony-mlflow-wsp, Type Azure Machine Learning workspace. This is the resource that we just created programmatically from Jupyter. Also, notice right below it is loony-test. That's the other Azure Machine Learning workspace, the one we created manually at the start of this demo. Remember again, that had we not created loony-test, the programmatic creation of loony-mlflow-wsp would have failed. In any case, let's click on this workspace. We are taken into the details and now when we scroll down, we find the same Launch studio button.

[Video description begins] A page titled 'loony-mlflow-wsp' appears. The main pane displays 2 options at the top: Download config.json and Delete. Below, a section labeled Essentials appears. It offers various information in fields like Resource group, Location, Subscription, Key Vault, MLflow tracking URI, and so on. A tab labeled Launch studio appears below. [Video description ends]

This will launch Azure Machine Learning Studio. We click on it and here we are inside Azure ML Studio. Our machine learning workspace is ready for us to push our model into.

[Video description begins] An Azure AI | Machine Learning Studio page appears. The navigation pane on the left contains various sections. The first section contains options like Home, and Model catalog. The second section labeled Authoring contains options like Notebooks, Automated ML, and Designer. The third section labeled Assets contains options like Data, Jobs, Pipelines, and so on. The main pane contains the header: loony-mlflow-wsp. Below, there are various sections like Generative AI models, Notebook samples, Shortcuts, and so on. [Video description ends]

And to perform that push, we've got to switch over to MLflow. Let's pick the model that we wish to push.

[Video description begins] The page titled mlflow 2.3.2 is displayed on the browser. The title bar at the top offers 2 tabs: Experiments and Models. The Experiments tab is active now. The navigation pane on the left displays 2 items: Default and customer_churn_prediction. The customer_churn_prediction item is open on the main pane. It shows the header called Description. Below, a search bar appears along with several filter options such as Table view, Chart view, Sort, Columns, and so on. A table appears below with 5 column headers: Run Name, Created, Duration, Source, and Models. [Video description ends]

We go with the random forest model with default parameters. We click on the corresponding run in the list of runs in our experiment and then from the artifacts, we have the Full Path corresponding to the model directory. You see it at the very center of the screen. Let's click on that little Copy button. This will copy the full path of the model directory. Now we switch over to Jupyter and we paste this path into the invocation of the Model.register method.

This Model.register method, which is imported from azureml.core.model is a part of the integration between MLflow and Azure. We can see that we specify the workspace which is ws, the model_path which is what we just copied in, and again this is the directory of the model from the artifact server. And lastly, the model_name which is churn-prediction-model.

[Video description begins] Line 1 of input cell 5 reads: from azureml.core.model import Model. Line 4 reads: workspace = ws,. Line 5 displays the model path. Line 6 reads: model_name = 'churn-prediction-model'. [Video description ends]

This is a really important function invocation because when we complete this, we are effectively going to have pushed our model from our local file system which is managed by MLflow into Microsoft Azure. Let's clear up a little screen real estate before we run that command. Now we hit Shift+Enter and the model registration goes through successfully. By the end of this process, our model has been registered.

To confirm that's gone through successfully, let's switch back over to the Azure portal. We are still in the Azure Machine Learning Studio and we are still in our loony-mlflow-wsp workspace. Let's click on Models. That's in the list of assets over on the left. And here we can confirm that our churn-prediction-model has indeed been pushed out onto our workspace. Here it is, Version 1, Type CUSTOM. We click through into it and we see all of the Attributes, such as when it was created, the Asset ID.

That's an Azure URI. Then let's click on the Versions tab up above. There's just the one version in existence at this point, so this isn't very interesting. However, under Artifacts we find something familiar. We see the model directory and when we click on that, we see all of the files that we had previously seen on the MLflow UI, conda.yaml, the MLmodel file, and so on. This confirms that we have indeed registered our MLflow model successfully onto our Azure Machine Learning workspace. We click onto Endpoints and we see that there are no endpoints here as yet. This tells us that our model has been registered but not yet deployed. In order to deploy it, we'll have to add an endpoint at which a client can hit it, and we'll get to that in the next demo.

13. Video: Registering a Model on Azure (it_mlflowdj_03_enus_13)

Discover how to deploy a model to Azure and view this model.
deploy a model to Azure and view this model
[Video description begins] Topic title: Registering a Model on Azure. Your host for this session is Vitthal Srinivasan. [Video description ends]
Let's pick up right from where we left off at the end of the last demo. There the last Python command that we had run was model.register and that had the effect of successfully registering our model on the Azure Machine Learning workspace. This implies that there is an MLflow server running within that workspace. We can see on screen now the details of this particular model.

[Video description begins] A page titled 'churn-prediction-model:1' appears within Azure AI | Machine Learning Studio. The navigation pane on the left contains various sections like Authoring, Assets, Manage, and so on. The Assets section contains several options like Data, Jobs, Components, Models, Endpoints, and so on. The main pane displays a command bar at the top with several options like Details, Versions, Artifacts, Endpoints, Jobs, Data, and so on. The Artifacts option is active now. It displays a toolbar with the following options: Refresh and Download all. Below, a left pane displays a directory called model. It contains the following files: conda.yaml, MLmodel, model.pkl, python_env.yaml, and requirements.txt. [Video description ends]

We see the conda.yaml file, the MLmodel file, and the model.pkl file, all of which are pretty familiar. We've seen them in MLflow's UI on our local machine. However, there's a big difference. This time around, the files that we are looking at do not reside on the local artifact server running from our local machine, but rather from the MLflow Tracking server running in this Azure Machine Learning workspace.

We also noted that even though the model had been registered, it hadn't yet been deployed. What do we mean by that? Well, if we click on Endpoints, we see that there are no endpoints. So, there's no way for an external client to hit this registered model, which currently resides on MLflow on Azure. That's what we are going to change in this demo. Let's switch back to Jupyter and begin the deployment process. On screen now, you can see that we start by importing json and then from mlflow.deployments, we import the function get_deploy_client. However, before we can actually deploy our model, we'll have to create something known as the deployment configuration. We do that on line 7.

[Video description begins] The Jupyter notebook titled 'ModelsRegistrationAndDeploymentinAzureMLStudio' is open. Code cell 6 is displayed. Line 1 reads: import json. Line 2 reads: from mlflow.deployments import get_deploy_client. Line 7 reads: deploy_config = {"computeType": "aci"}. [Video description ends]

This is a really simple dictionary with just one key that's computeType and the corresponding value here is aci. Now aci is short for Azure Container Instances.

And this is a relatively new compute offering on Azure. It's serverless compute which allows you to run containers in a serverless fashion. Alternative choices include AKS, that's the Azure Kubernetes Service, web app or the Azure Web Service, batch AI, or VM for virtual machine. ACI is the most lightweight and let's just go with that. Once we create this dictionary deploy_config, we've got to write this out to a JSON file. That JSON file will exist on our local file system and we will pass that in while creating our deployment.

[Video description begins] Line 10 reads: deployment_config_path = "deployment_config.json". Line 12 reads: with open(deployment_config_path, "w") as outfile:. Line 13 reads: outfile.write(json.dumps(deploy_config)). [Video description ends]

So, on lines 10 through 13 of this code cell, we write out this dictionary to the JSON file with the name deployment_config.json. You see that defined on line 10 and then we write it out by invoking json.dumps on line 13.

Let's run this code through and this runs successfully. Next, let's turn our attention to the next input cell where we invoke get_deploy_client. The code that you see there has a placeholder for the tracking_uri. The idea here is that we've got to redirect our client so that when it communicates with the MLflow Tracking server, it's talking to the MLflow Tracking server which is running on Azure and not the local MLflow Tracking server. That's a very important point about this entire demo.

[Video description begins] Line 2 of code cell 7 reads: client = get_deploy_client("<tracking_url>"). [Video description ends]

That tracking URI will be available in Azure. So, let's switch over to the Azure portal. We are back inside our loony-mlflow-wsp and there, towards the center-right, you can see the MLflow Tracking URI. It starts with azureml:// and there's a nice Copy to clipboard button over on the extreme right. So, let's Copy that to clipboard, switch back to Jupyter, and paste that into the invocation of get_deploy_client.

[Video description begins] A page titled 'loony-mlflow-wsp' appears. The main pane displays 2 options at the top: Download config.json and Delete. Below, a section labeled Essentials appears. It offers various information in fields like Resource group, Location, Subscription, Key Vault, MLflow tracking URI, and so on. A tab labeled Launch studio appears below. [Video description ends] 

Amid all of the boilerplate, it's important for us to understand what's just happened. In this line of code, we are effectively redirecting our client so that now when it communicates with the server, that server is going to be the Azure MLflow Tracking server rather than the local MLflow tracking server. Next, we've got to configure some of the deployment information.

[Video description begins] Line 5 reads: config = {"deploy-config-file": deployment_config_path}. Line 6 reads: model_name = 'churn-prediction-model'. Line 7 reads: model_version = 1. [Video description ends]

So, on line 5, we have a config dictionary. This has just the one key/value pair. The key is the deploy-config-file. The value is the path on the local file system to that file. And that's the path which we created on the previous code cell on line 10, deployment_config.json. Then we also define the model_name which is churn-prediction-model and the model_version which is 1.

Let's hit Shift+Enter and see whether this code runs, and it does. On the next line, we again redirect MLflow. This time it's via the mlflow.set_tracking_uri function. Once again, we've got to specify the URI of the MLflow Tracking server that's running on Azure. 

[Video description begins] Line 3 of code cell 8 reads: mlflow.set_tracking_uri('<tracking_url>'). [Video description ends]

That's what we copied into the clipboard a moment ago, so we paste that in and run this line of code as well. Now that we've configured the client to communicate with the right MLflow Tracking server, we can call client.create_deployment. This is a simple enough function call but it takes a good half an hour or so to run.

[Video description begins] Code cell 9 is highlighted. Line 1 reads: client.create_deployment(. Line 2 reads: model_uri = f"models:/{model_name}/{model_version}",. Line 3 reads: config = config,. Line 4 reads: name = "churn-pred-model-aci-deployment",. Line 5 displays a ). [Video description ends]

Notice the input arguments, the model_uri. This is models:/ and then the model_name and the model_version, the config object which we just created, and the name of the deployment. This is churn-pred-model-aci-deployment.

We hit Shift+Enter and then a lot starts to happen. First up, we are asked to authenticate. We are signed in so we simply click on the account and then we are told that we can close this window. We head back to Jupyter where we see that we get intermittent status updates. Let's switch over to the Azure portal. There we see that we have a notification. This just tells us that our deployment is in progress. We click through into the Deploy details link and you can see that here in the Deployment state, it currently states Unhealthy.

[Video description begins] A page titled 'churn-pred-model-aci-deployment' appears within Azure AI | Machine Learning Studio. The main pane displays a command bar at the top with several options like Details, Test, Consume, and Deployment logs. The Details option is active now. It displays 3 sections: Endpoint attributes, Tags, and Properties. The Endpoint attributes section provides various information in fields like Service ID, Deployment state, Compute Type, Operation state, Model ID, Image ID, REST endpoint, and so on. [Video description ends]

That will change as we move along in the process. We also see the Compute type is Container instance and the Service ID and the Model ID are visible as well.

During this deployment process, we can monitor the state of the endpoints as well as the models by finding them in the list of assets over on the left.

[Video description begins] A page titled 'Endpoints' appears. The main pane displays 2 tabs at the top: Real-time endpoints and Batch endpoints. The Real-time endpoints tab is active now. It displays a toolbar at the top with the following options: Create, Refresh, Delete, and View options. A table appears below with several column headers like Name, Description, Created on, Created by, Updated on, Compute Type, Compute Target, and Tags. [Video description ends]

Here we are back in the Endpoints. If we now click through into the details of this particular endpoint, we can get an update on the status. You see that the Deployment state has now changed from Unhealthy to Loading, so that's a good sign. There's also an Operation state which is set to Running. All of this is in the Details tab. At this point, if you click on the other tabs such as Test, Consume, or Deployment logs, we find that all of these are blank. Right now, it just says Endpoint unhealthy in each one of these. That will change when the endpoint becomes healthy. At length, the process completes. Let's switch back to Jupyter.

You can see that there are many more updates here. Finally, the process has gone through. You can see from the timestamps that the time lag was a good 25 minutes and we have a lot of JSON output. At this point, we switch back over to the Azure portal, and in the endpoint section, we click on our endpoint. We find that the Deployment state is now Healthy, the Operation state reads Succeeded, and if we scroll down, the REST endpoint is now available there. At this point, we have successfully deployed our model. We see the REST endpoint that we can access here on screen. All that's left is for us to actually hit it with some X data and get back some predictions. We'll consume this endpoint in the next demo.

14. Video: Accessing Models through Azure REST Endpoints (it_mlflowdj_03_enus_14)

Learn how to predict using a model's Representational State Transfer (REST) endpoint .

predict using a model's Representational State Transfer (REST) endpoint
[Video description begins] Topic title: Accessing Models through Azure REST Endpoints. Your host for this session is Vitthal Srinivasan. [Video description ends]
We ended the previous demo with the successful deployment of our endpoint. Remember that this is an endpoint on Azure and this endpoint allows clients to hit the model which is also hosted on Azure's Machine Learning workspace. So, the MLflow Tracking server that we are now talking to and communicating with is the one on Azure, not the one on our local machine. We pick up the action from the successful deployment. This is the endpoint visible in Azure. You see the Deployment state is Healthy, the Operation state is Succeeded, Compute type is Container instance.

[Video description begins] A page titled 'churn-pred-model-aci-deployment' appears within Azure AI | Machine Learning Studio. The main pane displays a command bar at the top with several options like Details, Test, Consume, and Deployment logs. The Details option is active now. It displays 3 sections: Endpoint attributes, Tags, and Properties. The Endpoint attributes section provides various information in fields like Service ID, Deployment state, Operation state, Compute type, Image ID, REST endpoint, and so on. [Video description ends]

If we scroll down just a little bit right here we see the REST endpoint. We'll copy that link in just a moment, but first, let's look at some sample code which will tell us how we can actually hit this endpoint. We click on the Test tab up top.

This shows us how to set up the input data in order to test the endpoint. This has the form of a dictionary. We see the element called columns. That's a list in which each element is one of the X variables which our model takes in. So, we've got to specify values of the Age, FrequentFlyer, AnnualIncomeClass, and so on. Then we have the index that's going to contain an integer with the number of X variables that we are going to pass in. The last element in the input data is the data itself and that is going to be a list of all of the X variable values. We'll see how to populate this more completely in just a moment.

[Video description begins] The Test option is active now. It displays a few lines of code. The first line displays an {. The second line reads: "input_data": {. The third line reads: "columns": [. The fourth line reads: "Age",. The fifth line reads: "FrequentFlyer",. The sixth line reads: "AnnualIncomeClass",. The seventh line reads: "ServicesOpted",. The eighth line reads: "AccountSyncedToSocialMedia",. The ninth line reads: "BookedHotelOrNot". The tenth line reads:],. The eleventh line reads: "index": [],. The twelfth line reads: "data": []. The following two lines display a }. [Video description ends]

For now, let's click on the Consume tab up top and here we see the boilerplate code which is quite formidable, that's required in order to actually consume this REST endpoint.

At the very top, there is the Basic consumption info which includes the REST endpoint URL. Then comes sample code in Python C# and R. For now, we will just take all of this Python code and copy paste it into our Jupyter Notebook. We've also made some modifications and filled in some of the missing bits. So, let's examine this data in detail. Right up top, you can see that we have some import statements, to consume this endpoint directly using say a curl command.

[Video description begins] The Jupyter notebook titled 'ModelsRegistrationAndDeploymentinAzureMLStudio' is open. Line 1 of the highlighted code cell reads: import urllib.request. Line 2 reads: import json. Line 3 reads: import os. Line 4 reads: import ssl. [Video description ends] urllib.request, json, os, and ssl. Then starting line 6, we have a function allowSelfSignedHttps. We invoke this on line 14. Please note that this code is quite involved and it's not going to be easy [Video description begins] Line 6 reads: def allowSelfSignedHttps(allowed):. Line 8 reads: if allowed and \. Line 9 reads: not os.environ.get('PYTHONHTTPSVERIFY', '') and \. Line 10 reads: getattr(ssl, '_create_unverified_context', None):. Line 11 reads: ssl._create_default_https_context = ssl._create_unverified_context. Line 14 reads: allowSelfSignedHttps(True). [Video description ends] 

That's because we can see here that we perform various bits of pre and post processing and those would require additional work in curl. As the comment there on line 13 tells us, we've got to invoke this function allowSelfSignedHttps with the input argument True if we use self-signed certificates in our service. On line 7, you see what this function does. It turns off or bypasses verification of the server certificate performed on the client. Of course, the server here is our own REST endpoint MLflow server. Scrolling down just a little bit, starting line 17, we've set up the data for our input request.

[Video description begins] Line 7 reads: # bypass the server certificate verification on client side. Line 13 reads: # this line is needed if you use self-signed certificate in your scoring service. [Video description ends] 

[Video description begins] Line 17 reads: data {. Line 18 reads: "input_data": {. Line 19 reads: "columns": [. Line 20 reads: "Age",. Line 21 reads: "FrequentFlyer",. Line 22 reads: "AnnualIncomeClass",. Line 23 reads: "ServicesOpted",. Line 24 reads: "AccountSyncedToSocialMedia",. Line 25 reads: "BookedHotelOrNot". Line 26 reads:],. Line 27 reads: "index": [6],. Line 28 reads: "data": [[28, 'Yes', 2, 6, 'No', 'Yes']]. Lines 29 and 30 display a }. [Video description ends]

Let's start with the actual X variable values. Those are on line 28. We can see that that is a two-dimensional list in which the first element is 28 and the name of that list is data.

On line 27, we have the index which is 6, and on line 19, we have the columns. These are the column names. Note that columns, index, and data are all keys in the dictionary input data which is defined starting line 18. So, this data starting line 17 is one problem instance. The commented-out code that you see right below that is a different problem instance which we will also call on our function in just a bit. Let's scroll further down and pick up from line 48. That's where we start with the actual invocation of the server method. On line 48, we construct the body of our HTTP request by invoking json.dumps on our data dictionary. On line 50, we have the url and this is the REST URL which we copied from the Azure consume information. We have the headers on line 52. Here we just have 'Content-Type' : 'application/json'. We construct the HTTP request object on line 54.

We pass in the URL, the body, and the headers.

[Video description begins] Line 48 reads: body = str.encode(json.dumps(data)). Line 50 reads: url = 'http://4cd03d72-bcee-4b6a-9519-a5caec75e8a7.eastus.azurecontainer.io/score'. Line 52 reads: headers = {'Content-Type': 'application/json'}. Line 54 reads: req = urllib.request.Request(url, body, headers). [Video description ends]

Then we actually make the request on line 57 and that's why that's within a try except block, and then we hopefully get a response, which we print out on line 60. We also cache an HTTPError on line 61, and in the error handling portion, we print out the failure status code that's on line 62. We also print out the headers. These include the request ID and the timestamp, and as the comments tell us that could be helpful in debugging. That does it for the code. 

[Video description begins] Line 56 reads: try:. Line 57 reads: response = urllib.request.urlopen(req). Line 59 reads: result = response.read(). Line 60 reads: print(result). [Video description ends]

[Video description begins] Line 61 reads: except urllib.error.HTTPError as error:. Line 62 reads: print("The request failed with status code: "+str(error.code)). Line 64 reads: # Print the headers - they include the request ID and the timestamp,. Line 65 reads: # which are useful for debugging the failure. Line 66 reads: print(error.info()). Line 67 reads: print(error.read().decode("utf8", 'ignore')). [Video description ends]

Let's hit Shift+Enter and we see right away that we did get a prediction back. That prediction was the number 1. Or rather, it's a list with just one element containing the number 1, telling us that for the problem instance that we passed in, our model predicts churn.

Remember that a prediction of 1 indicates churn, 0 indicates no churn. Next, let's try with a different problem instance. We are going to uncomment out lines 32 through 45 and instead comment out lines 17 through 30.

[Video description begins] Lines 32 to 41 contain codes identical to lines 17 to 26. The codes in lines 42 and 43 are different. Line 42 reads: "index": [8],. Line 43 reads: "data": [[23, 'No', 0, 1, 'No', 'No']]. [Video description ends]

We've made the change now, and when we rerun our code, you see that the prediction that we get back from the server changes. Now, it's a 0 instead of a 1. And in this fashion, we've successfully been able to consume our endpoint not once, but twice. Now all that's left for us to do is to clean up all of these resources. Let's start in MLflow. We click on Models, click through into the first model there, and then select each of the versions one-by-one. We click on the three vertical dots over on the top right and choose the Delete option.

[Video description begins] A page titled 'Version 1' is displayed. There are various details at the top in fields like Registered At, Source Run, Stage, and Last Modified. The field labeled Stage contains a drop-down list with 3 options: Transition to Staging, Transition to Production, and Transition to Archived. A vertical ellipsis icon is displayed at the top right corner. Below, 3 collapsible sections appear: Description, Tags, and Schema. [Video description ends] 

We are warned while doing this that the deletion cannot be undone. We delete Version 1 and Version 2 both in this fashion. But before we delete Version 2, we also change the stage to archived just for good measure. Then we click on Registered Models and we see there that we have the churn_prediction_model in there. We click on that model and then click again on the Delete button over on the top right. Once again, this cannot be undone either. We are done with the models. Let's switch over to the Experiments. We have the one experiment visible over on the left. We click on the little trash can icon. This will prompt us for confirmation and we go ahead and hit Delete. At this point, we've cleaned up everything in the MLflow UI.

Let's now switch over to the Azure portal. Here we are in the loony-mlflow-wsp. This is the Azure Machine Learning workspace. There is a Delete icon right at the center of the screen.

[Video description begins] A page titled 'loony-mlflow-wsp' appears. The main pane displays 2 options at the top: Download config.json and Delete. Below, a section labeled Essentials appears. [Video description ends]

Let's go ahead and click on it. As this message tells us, there is a soft delete state from which we can recover the workspace for up to 14 days. If we do want to permanently delete the workspace, we are prompted to enter the workspace name explicitly. We do so, and the workspace is deleted. At this point, the UI updates and we find ourselves back in the resource group.

[Video description begins] A pane titled 'Delete workspace' appears on the right. It displays a message. At the bottom of the pane, a checkbox labeled Delete the workspace permanently appears. Below, 2 tabs labeled Delete and Cancel appear. [Video description ends] 

Now we select all of the remaining resources here, none of which are of interest to us, and then we click on Delete over on the top right corner.

[Video description begins] A page titled 'loony-mlflow-rg' appears. The main pane displays several options at the top like Create, Manage view, Delete resource group, Refresh, and so on. Additionally, there is an ellipsis icon available. It contains a drop-down list with 3 options: Delete, Export template, and Open in mobile. Below, 2 tabs appear: Resources and Recommendations. The Resources tab displays a table with 3 column headers: Name, Type, and Location. [Video description ends]

This opens up a dialog confirming that we would like to delete 7 resources. We've got to type out the word delete, and at this point, the deletion goes through successfully and our resource group is now completely empty. And with that, we've come to the end of this demo in which we successfully deployed our models which we had built locally using MLflow to Azure's Machine Learning service.

15. Video: Course Summary (it_mlflowdj_03_enus_15)

In this video, we will summarize the key concepts covered in this course.
summarize the key concepts covered in this course
[Video description begins] Topic title: Course Summary [Video description ends]
You have now reached the end of this course, registering and deploying ML models. The MLflow Model Registry enables you to register and deploy machine learning models for future use, either locally or in the cloud. With MLflow Models, data scientist can easily register models, allowing for easy retrieval and versioning. We explored how this feature streamlines model management and facilitates collaboration among team members during the model development and deployment process.

Whether it's for sharing models across teams, deploying models for production use, or reproducing experiments in different environments, MLflow Models provide a powerful solution for model version control and accessibility. First, we created classification models using the regular ML workflow. We started by visualizing and cleaning our data, gaining insights through various techniques. Then by creating an experiment from the MLflow UI, we ran a classification model and viewed its metrics to understand its performance.

To gain deeper insights into the model behavior, we analyzed model insights using SHAP, providing valuable interpretability to our models. We continued by running multiple classification models, allowing us to explore different approaches and through programmatic comparison, we made informed decisions regarding which model is the best performing. Next, we explored the powerful MLflow models feature, which allows us to register trained models and manage their versions efficiently. By registering models, we enabled easy retrieval in versioning, making it convenient to track model changes and facilitate collaboration among team members.

We also learnt how to modify registered model versions and stages and read them into Python, empowering us to work seamlessly with different versions of the same model. Additionally, we explored serving models to a REST endpoint, providing an avenue to deploy our models for production use and integration with other applications. Finally, we delved into integrating MLflow with Azure Machine Learning.

We created an Azure Machine Learning workspace and learned how to register models to Azure rather than local making our models accessible through Azure REST endpoints. This integration with Azure Machine Learning expanded the deployment possibilities for our models and allowed us to leverage the power of the cloud with MLflow. In conclusion, this comprehensive course has provided us with a strong foundation in MLflow Model Registry, and these skills set us up nicely to move on to hyperparameter tuning, ML models in MLflow coming up ahead.

Course File-based Resources
•	Registering & Deploying ML Models
Topic Asset
© 2023 Skillsoft Ireland Limited - All rights reserved.