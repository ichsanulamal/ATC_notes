# DataOps

Examine the code that we’ve written to stream a dataframe which represents text data that is received from a server listening on localhost:7777.


01. import org.apache.spark.api.java.function.FlatMapFunction;
02. import org.apache.spark.sql.*;
03. import org.apache.spark.sql.streaming.StreamingQuery;
04. import java.util.Arrays;
05. import java.util.Iterator;
06. SparkSession spark = SparkSession
07. .builder()
08. .appName("streamingDataWordCount")
09. .getOrCreate();
10. //insert code segment here

Which code segment would you insert in line 10 to read and transform the dataframe in order to calculate the word count?


T Option 1

Dataset<Row> lines = spark
.readStream()
.format("socket")
.option("host", "localhost")
.option("port", 7777)
.load();
Dataset<String> words = lines
.as(Encoders.STRING())
.flatMap((FlatMapFunction<String, String>) x -> Arrays.asList(x.split(" ")).iterator(), Encoders.STRING());
Dataset<Row> wordCounts = words.groupBy("value").count();

Option 2

Dataset<Row> lines = spark
.read()
.format("socket")
.option("host", "localhost")
.option("port", 7777)
.load();
Dataset<String> words = lines
.as(Encoders.STRING())
.flatMap((FlatMapFunction<String, String>) x -> Arrays.asList(x.split(" ")).iterator(), Encoders.STRING());
Dataset<Row> wordCounts = words.groupBy("value").count();

Option 3

Dataset<Row> lines = spark
.readStream()
.format("socket")
.option("host", "localhost")
.option("port", 7777)
.load();
Dataset<String> words = lines
.as(Encoders.DOUBLE())
.flatMap((FlatMapFunction<String, String>) x -> Arrays.asList(x.split(" ")).iterator(), Encoders.STRING());
Dataset<Row> wordCounts = words.groupBy("value").count();

Option 4

Dataset<Row> lines = spark
.readStream()
.format("socket")
.option("host", "localhost")
.option("port", 7777)
.load();
Dataset<String> words = lines
.as(Encoders.DOUBLE())
.flatMap((FlatMapFunction<String, double) x -> Arrays.asList(x.split(" ")).iterator(), Encoders.STRING());
Dataset<Row> wordCounts = words.groupBy("value").count();
Instruction: Choose the option that best answers the question. 

---
Examine the code that we have written for Streaming data, which is being read continuously from S3, transformed and written back to s3://out.


// Read JSON continuously from S3
logsDF = spark.readStream.json("s3://logs")
// Transform with DataFrame API and save
logsDF.select("user", "url", "date")
.writeStream.parquet("s3://out")
.start()

You want to convert the code with a batch version of the same. Which code segment would you use to facilitate the conversion?


Option 1

// Read JSON once from S3
logsDF = spark.readBatch.json("s3://logs")

// Transform with DataFrame API and save
logsDF.select("user", "url", "date")
.writeBatch.parquet("s3://out")

Option 2

// Read JSON once from S3
logsDF = spark.read.json("s3://logs")

// Transform with DataFrame API and save
logsDF.select("user", "url", "date")
.write.parquet("s3://out")

Option 3

// Read JSON once from S3
logsDF = spark.readBatch.json("s3://logs")

// Transform with DataFrame API and save
logsDF.select("user", "url", "date")
.write.parquet("s3://out")

Option 4

// Read JSON once from S3
logsDF = spark.read.json("s3://logs")

// Transform with DataFrame API and save
logsDF.select("user", "url", "date")
.writeBatch.parquet("s3://out")
Instruction: Choose the option that best answers the question. 

---

Examine scenario where a company wants to migrate its monolithic system to distributed systems while also ensuring they’re able to identify the security concerns associated with the migration. You’ve been tasked to list all the security concerns of distributed systems. What are the major security concerns that you need to identify and manage on the distributed system environment?

Instruction: Choose the option that best answers the question. 

"Authentication, Access control, Non-repudiation, Process security "

"Process security, Data security, Confidentiality, Data integrity "

"Authentication, Authorization, Process security, Data security, Integrity "

T "Confidentiality, Data Integrity, Authentication, Access control, Non-repudiation "

---
Examine a scenario where you want to use AWS IoT to extend AWS Edge devices so they can act locally on the data generated by them. You have already installed AWS Greengrass group and you want to configure a custom group. Which code segment would you use?


T Option 1

config_file = "skillsoft.yaml"
group_name = "skillsoft"
region = "US-East-1"

gc = GroupCommands(group_types={
CustomGroupType.CUSTOM_TYPE: CustomGroupType
})
gc.create(
config_file, group_type=CustomGroupType.CUSTOM_TYPE,
group_name=group_name, region=region
)

Option 2

config_file = "skillsoft.yaml"
group_name = "skillsoft"
region = "US-East-1"

gc = GroupCommands(group_types={
CustomGroupType.BUILD_TYPE: CustomGroupType
})
gc.create(
config_file, group_type=CustomGroupType.CUSTOM_TYPE,
group_name=group_name, region=region
)

Option 3

config_file = "skillsoft.yaml"
group_name = "skillsoft"
region = "US"

gc = GroupCommands(group_types={
CustomGroupType.CUSTOM_TYPE: CustomGroupType
})
gc.create(
config_file, group_type=CustomGroupType.CUSTOM_TYPE,
group_name=group_name, region=region
)

Option 4

config_file = "skillsoft.yaml"
group_name = "skillsoft"
region = "US"

gc = GroupCommands(group_types={
CustomGroupType.CUSTOM_TYPE: CustomGroupType
})
gc.create(
config_file, group_type=CustomGroupType.CUSTOM_GRP,
group_name=group_name, region=region
)
Instruction: Choose the option that best answers the question. 

---
Examine the codes where in the first segment we have defined a Java Bean class to convert RDD to a dataframe, and in the second segment we’re reading the data that is being streamed from a server, running at localhost with the port 7777, to create a SparkSession.


public class WordRow implements java.io.Serializable {
private String word;
public String getWord() {
return word;
}
public void setWord(String word) {
this.word = word;
}
}

import org.apache.spark.*;
import org.apache.spark.api.java.function.*;
import org.apache.spark.streaming.*;
import org.apache.spark.streaming.api.java.*;
import scala.Tuple2;
SparkConf conf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount");
JavaStreamingContext jssc = new JavaStreamingContext(conf, Durations.seconds(1));
SparkConf conf = new SparkConf().setMaster("local[2]").setAppName("NetworkWordCount");
JavaStreamingContext jssc = new JavaStreamingContext(conf, Durations.seconds(1));
JavaReceiverInputDStream<String> lines = jssc.socketTextStream("localhost", 7777);
JavaDStream<String> words = lines.flatMap(x -> Arrays.asList(x.split(" ")).iterator());
words.foreachRDD((rdd, time) -> {

// Get the singleton instance of SparkSession
SparkSession spark = SparkSession.builder().config(rdd.sparkContext().getConf()).getOrCreate();

// Convert RDD[String] to RDD[case class] to DataFrame
JavaRDD<WordRow> rowRDD = rdd.map(word -> {
WordRow record = new WordRow();
record.setWord(word);
return record;
});

You want to convert the RDD into a dataframe, register it as a temporary table and get the word count using SQL. Which code segment would you use in this given scenario?


Option 1

DataFrame wordsDataFrame = SparkSession.createDataFrame(rowRDD, WordRow.class);

// Creates a temporary view using the DataFrame
wordsDataFrame.createTempView("words");

// Do word count on table using SQL and print it
DataFrame wordCountsDataFrame =
SparkSession.sql("select word, count(*) as total from words group by word");
wordCountsDataFrame.show();
});

T Option 2

DataFrame wordsDataFrame = spark.createDataFrame(rowRDD, WordRow.class);

// Creates a temporary view using the DataFrame
wordsDataFrame.createOrReplaceTempView("words");

// Do word count on table using SQL and print it
DataFrame wordCountsDataFrame =
spark.sql("select word, count(*) as total from words group by word");
wordCountsDataFrame.show();
});

Option 3

DataFrame wordsDataFrame = SparkSession.createDataFrame(rowRDD, WordRow.class);

// Creates a temporary view using the DataFrame
wordsDataFrame.createOrReplaceTempView("words");

// Do word count on table using SQL and print it
DataFrame wordCountsDataFrame =
spark.sql("select word, count(*) as total from words group by word");
wordCountsDataFrame.show();
});

Option 4

DataFrame wordsDataFrame = SparkSession.createDataFrame(rowRDD, WordRow.class);

// Creates a temporary view using the DataFrame
wordsDataFrame.createTempView("words");

// Do word count on table using SQL and print it
DataFrame wordCountsDataFrame =
spark.sql("select word, count(*) as total from words group by word");
wordCountsDataFrame.show();
});
Instruction: Choose the option that best answers the question. 

---
Examine a scenario where you’ve been tasked to build chat using Netcat. Assume that the server IP is 192.168.0.1. Which command would you use to run the server and the client required for implementing the chat?


Option 1

Server:
nc 192.168.0.1 3333

client:
nc -l 3333

Option 2

Server:
nc server 192.168.0.1 3333

Client:
nc -1 client 3333

Option 3

Server:
nc runServer 192.168.0.1

client:
nc runClient 3333

T Option 4

Server:
nc -l 3333

client:
nc 192.168.0.1 3333
Instruction: Choose the option that best answers the question. 

---
Examine the query.


select * from sales
union
select * from products
intersect
select * from customer
order by productId;

While executing the query you observed the sequence of execution begins with the intersection of product and customer followed by the union of result with sales. You want to change the sequence of execution to begin with the union of sales and products followed by the intersection with customer. Which query statement would you use?


Option 1

select * from sales
union
select * from customer
intersect
select * from product
order by productId;

Option 2

select * from sales
union
(select * from product)
intersect
select * from customer
order by productId;

Option 3

(select * from product)
union
(select * from sales)
intersect
(select * from customer)
order by productId; 

Option 4

(select * from sales
union
select * from product)
intersect
(select * from customer)
order by productId;
Instruction: Choose the option that best answers the question. 

---

Examine the Python code.


01. from numpy.random import seed
02. from numpy.random import randn
03. from numpy import mean
04. from numpy import std
05. seed(1)
06. data = 5 * randn(50000) + 30
07. data_mean, data_std = mean(data), std(data)
08. cut_off = data_std * 3
09. lower, upper = data_mean - cut_off, data_mean + cut_off
10. //insert code segment here

You want to detect outliers and remove them using statistical method. Which code segment would you use in line 10 to ensure you get the values that can be plotted to depict the outliers?


Option 1

## identify outliers
outliers = [x for x in data if x < lower or x > upper]
print('Identified outliers: %d' % len(outliers))
## remove outliers
outliers_removed = [x for x in data if x >= lower and x <= upper]
print('Non-outlier observations: %d' % len(outliers_removed))

Option 2

## remove outliers
outliers_removed = [x for x in data if x >= lower and x <= upper]
print('Non-outlier observations: %d' % len(outliers_removed))

Option 3

## identify outliers
outliers = [x for x in data if x > lower or x < upper]
print('Identified outliers: %d' % len(outliers))
## remove outliers
outliers_removed = [x for x in data if x >= lower and x <= upper]
print('Non-outlier observations: %d' % len(outliers_removed))

Option 4

## identify outliers
outliers = [x for x in data if x < lower or x > upper]
print('Identified outliers: %d' % len(outliers))
## remove outliers
outliers_removed = [x for x in data if x <= lower and x >= upper]
print('Non-outlier observations: %d' % len(outliers_removed))
Instruction: Choose the option that best answers the question. 

---

Examine a scenario where you have to register a user and get the URL for the embedded dashboard. Which command would you use to complete the given task?


Option 1

aws quicksight register-user --aws-account-id sams@skillsoft.com --namespace default --identity-type IAM --iam-arn "arn:aws:iam::111122223333:role/embedding_quicksight_dashboard_role" --user-role READER --session-name "embeddingsession" --email sam@skillsoft.com --region us-east-1

aws quicksight get-dashboard-embed-url --aws-account-id 111122223333 --dashboard-id 1a1ac2b2-3fc3-4b44-5e5d-c6db6778df89 --identity-type IAM

Option 2

aws quicksight register-user --aws-account-id 111122223333 --namespace default --identity-type KMS --iam-arn "arn:aws:iam::111122223333:role/embedding_quicksight_dashboard_role" --user-role READER --session-name "embeddingsession" --email sam@skillsoft.com --region us-east-1

aws quicksight get-dashboard-embed-url --aws-account-id 111122223333 --dashboard-id 1a1ac2b2-3fc3-4b44-5e5d-c6db6778df89 --identity-type IAM

T Option 3

aws quicksight register-user --aws-account-id 111122223333 --namespace default --identity-type IAM --iam-arn "arn:aws:iam::111122223333:role/embedding_quicksight_dashboard_role" --user-role READER --session-name "embeddingsession" --email sam@skillsoft.com --region us-east-1

aws quicksight get-dashboard-embed-url --aws-account-id 111122223333 --dashboard-id 1a1ac2b2-3fc3-4b44-5e5d-c6db6778df89 --identity-type IAM

Option 4

aws register-user --aws-account-id 111122223333 --namespace default --identity-type IAM --iam-arn "arn:aws:iam::111122223333:role/embedding_quicksight_dashboard_role" --user-role READER --session-name "embeddingsession" --email sam@skillsoft.com --region us-east-1

aws get-dashboard-embed-url --aws-account-id 111122223333 --dashboard-id 1a1ac2b2-3fc3-4b44-5e5d-c6db6778df89 --identity-type
Instruction: Choose the option that best answers the question. 

---
Examine the code that we’ve written to identify outliers using the local outlier factor algorithm.


01. import numpy as np
02. from sklearn.neighbors import LocalOutlierFactor
03. np.random.seed(42)
04. X_inliers = 0.3 * np.random.randn(100, 2)
05. X_inliers = np.r_[X_inliers + 2, X_inliers - 2]
06. X_outliers = np.random.uniform(low=-4, high=4, size=(20, 2))
07. X = np.r_[X_inliers, X_outliers]
08. n_outliers = len(X_outliers)
09. ground_value = np.ones(len(X), dtype=int)
10. ground_value[-n_outliers:] = -1
11. clf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)
12. //insert code segment here

You want to compute the predicted labels of the training samples. Which code segment would you use in line 12?


Option 1

y_pred = clf.fit_predict(ground_value)
n_errors = (y_pred != ground_value).sum()
X_scores = clf.negative_outlier_factor_

Option 2

y_pred = clf.fit_predict(X)
n_errors = (y_pred != X).sum()
X_scores = clf.negative_outlier_factor_

T Option 3

y_pred = clf.fit_predict(X)
n_errors = (y_pred != ground_value).sum()
X_scores = clf.negative_outlier_factor_

Option 4

y_pred = clf.fit_predict(X_outliers)
n_errors = (y_pred != ground_value).sum()
X_scores = clf.negative_outlier_factor_
Instruction: Choose the option that best answers the question. 

---
Examine a scenario where you have to reduce the processing time and improve the overall system performance of an Amazon Redshift database containing various tables. You want to use the ANALYZE operation effectively to improve performance. Which ANALYZE command would you use to change the analyze_threshold_percent to 20 percent and display the results of the ANALYZE operations?


Option 1

set analyze_threshold_percent to 20;

select distinct a.xid, trim(t.name) as name, a.status, a.rows, a.modified_rows, a.starttime, a.endtime
from stl_analyze a
join stv_tbl_perm t on t.id=a.table_id
where name = 'users'
order by starttime;

Option 2

set analyze_threshold_percent to 0.20;

select a.xid, trim(t.name) as name, a.status, a.rows, a.modified_rows, a.starttime, a.endtime
from stl_analyze a
join stv_tbl_perm t on t.id=a.table_id
where name = 'users'
order by starttime;

Option 3

set analyze_threshold_percent to 20%;

select distinct xid, trim(t.name) as name, status, rows, modified_rows, starttime, endtime
from stl_analyze a
join stv_tbl_perm t on id=table_id
where name = 'users'
order by starttime;

Option 4

set analyze_threshold_percent to 20;
select distinct xid, trim(t.name) as name, status, rows, modified_rows, starttime, endtime
from stl_analyze a
join stv_tbl_perm t on id=table_id
where name = 'users'
order by starttime;
Instruction: Choose the option that best answers the question. 

---

Examine a scenario where you have to restore a SQL server database titled SkillSoft to a certain point-in-time. Your current implementation restores the database but leaves the database non-operational, does not rollback the uncommitted transactions and you have to wait for the recovery to complete before you can start using it again. You want to make certain changes to ensure the database remains in a read-only mode and not non-operational while the database is still being restored.


Which T-SQL statement would use to enforce the suggested changes?


Option 1

RESTORE DATABASE SKillSoft
FROM SKillSoftBackups
WITH FILE=3, ACTIVE;

Option 2

RESTORE DATABASE SKillSoft
FROM SKillSoftBackups
WITH FILE=3, STANDBY;

Option 3

RESTORE DATABASE SKillSoft
FROM SKillSoftBackups
WITH FILE=3, NORECOVERY;

Option 4

RESTORE DATABASE SKillSoft
FROM SKillSoftBackups
WITH FILE=3, RECOVERY;
Instruction: Choose the option that best answers the question. 

---

Examine the code where we have defined a static dataframe along with a table name using PySpark.


from pyspark.sql.types import *

inputPath = "/skillsoft-datasets/structured-streaming/events/"
jsonSchema = StructType([StructField("time", TimestampType(), True), 
                         StructField("action", StringType(), True)])

## Static DataFrame representing data in the JSON files
staticInputDF = (
    spark
      .read
      .schema(jsonSchema)
      .json(inputPath)
)
//insert code segment here

You want to compute the number of open and closed actions real-time with a one-hour window. Which code segment would you need to include?


Option 1


from pyspark.sql.functions import * ## for window() function

staticCountsDF = (
    staticInputDF
      .groupBy(staticInputDF.action,
               window(staticInputDF.time, "1"))
      .count()
)
staticCountsDF.cache()

## Register the DataFrame as table 'static_counts'
staticCountsDF.createOrReplaceTempView("static_counts")

Option 2


from pyspark.sql.functions import * ## for window() function

staticCountsDF = (
    staticInputDF
      .groupBy(staticInputDF.action,
               window(staticInputDF.datatime, "1"))
      .count()
)
staticCountsDF.cache()

## Register the DataFrame as table 'static_counts'
staticCountsDF.createOrReplaceTempView("static_counts")

Option 3


from pyspark.sql.functions import * ## for window() function

staticCountsDF = (
    staticInputDF
      .groupBy(staticInputDF.action,
               window(staticInputDF.time, "1 hr"))
      .count()
)
staticCountsDF.cache()

## Register the DataFrame as table 'static_counts'
staticCountsDF.createOrReplaceTempView("static_counts")

T Option 4


from pyspark.sql.functions import * ## for window() function

staticCountsDF = (
    staticInputDF
      .groupBy(staticInputDF.action,
               window(staticInputDF.time, "1 hour"))
      .count()
)
staticCountsDF.cache()

## Register the DataFrame as table 'static_counts'
staticCountsDF.createOrReplaceTempView("static_counts")
Instruction: Choose the option that best answers the question. 

---

Examine scenario where you want to connect to an IoT device which is accessible at the url:


wss://<endpoint>.iot.<region>.amazonaws.com/mqtt

You want to create a Paho MQTT client in JavaScript and connect to AWS IoT. Which code segment would you use?


Option 1

var client = new Paho.Websocket.MQTT.Client(requestUrl, clientId);
var connectOptions = {
onSuccess: function(){
// connect succeeded
},
useSSL: false,
timeout: 3,
mqttVersion: 4,
onFailure: function() {
// connect failed
}
};
client.connect(connectOptions);

Option 2

var client = new Paho.MQTT.Client(requestUrl);
var connectOptions = {
onSuccess: function(){
// connect succeeded
},
useSSL: false,
timeout: 3,
mqttVersion: 4,
onFailure: function() {
// connect failed
}
};
client.connect(connectOptions);

Option 3

var client = new Paho.MQTT.Client(requestUrl, clientId);
var connectOptions = {
onSuccess: function(){
// connect succeeded
},
useSSL: true,
timeout: 3,
mqttVersion: 4,
onFailure: function() {
// connect failed
}
};
client.connect(connectOptions);

Option 4

var client = new Paho.MQTT.Client(requestUrl, clientId);
var connectOptions = {
onSuccess: function(){
// connect succeeded
},
useSSL: false,
timeout: 3,
mqttVersion: 4,
onFailure: function() {
// connect failed
}
};
client.connect(connectOptions);
Instruction: Choose the option that best answers the question. 

---

Examine a scenario where your organization wants Amazon Redshift to load data from Amazon S3 buckets. You’ve created a role titled S3RedshiftRole and you’ve also set the permission to AmazonS3ReadOnlyAccess.


You want to load data from S3 to a table in Redshift titled cities, where the ARN is arn:aws:iam::123456789012:role/S3RedshiftRole and the region is us-west-2. Which command would you use to load the data to the table?


Option 1

insertData cities from 's3://awssampledbuswest2/tickit/cities.txt'
credentials 'aws_iam_role=arn:aws:iam::123456789012:role/S3RedshiftRole'
delimiter '|' region 'us-west-2';

Option 2

copy cities from 's3://awssampledbuswest2/tickit/cities.txt'
delimiter '|' region 'us-west-2';

Option 3

copy cities from 's3://awssampledbuswest2/tickit/cities.txt'
credentials 'aws_iam_role=arn:aws:iam::123456789012:role/S3RedshiftRole'
delimiter '|' region 'us-west-2';

Option 4

load cities from 's3://awssampledbuswest2/tickit/cities.txt'
credentials 'aws_iam_role=arn:aws:iam::123456789012:role/S3RedshiftRole'
delimiter '|' region 'us-west-2';
Instruction: Choose the option that best answers the question. 

---

Examine code that defines the Spark entry point.


01. val spark = SparkSession
02. .builder()
03. .appName("Spark Structured Streaming")
04. .master("local[*]")
05. .getOrCreate()
06. //insert code segment here

You want to define the data input for Streaming application using Kafka which is running at localhost:9092, for a topic titled skillsoft-course. Which code segment would you use in line 6?


Option 1

val data_stream = spark
.readStream // constantly expanding dataframe
.format("kafka")
.option("kafka.bootstrap.servers", "localhost:9092")
.option("subscribe", " skillsoft-courses ")
.option("startingOffsets","latest") //or earliest

Option 2

val data_stream = spark
.readStream // constantly expanding dataframe
.format("kafka")
.option("kafka.bootstrap.server", "localhost:9092")
.option("subscribe", " skillsoft-courses ")
.option("startingOffsets","latest") //or earliest
.load()

T Option 3

val data_stream = spark
.readStream // constantly expanding dataframe
.format("kafka")
.option("kafka.bootstrap.servers", "localhost:9092")
.option("subscribe", " skillsoft-courses ")
.option("startingOffsets","latest") //or earliest
.load()

Option 4

val data_stream = spark
.readStream // constantly expanding dataframe
.format("kafka")
.option("kafka.bootstrap.servers", "localhost:9092")
.option("subscriber", " skillsoft-courses ")
.option("startingOffsets","latest") //or earliest
.load()
Instruction: Choose the option that best answers the question. 

---

Examine the given R code.


01. library(tidyverse)
02. library(reshape2)
03. library(lubridate)
04. library(purrrlyr)
05. library(ggrepel)
06. ## devtools::install_github("twitter/AnomalyDetection")
07. library(AnomalyDetection)
08. library(IsolationForest)
09. ## loading raw data##
10. dataframe <- read_csv('data.csv')
11. ## summarizing metrics by dates
12. dataframe_tot <- dataframe %>%
13. group_by(date) %>%
14. summarise(sessions = sum(sessions),
15. goals = sum(goals)) %>%
16. ungroup() %>%
17. mutate(channel = 'total')
18. ## bindind all together
19. dataframe_all <- rbind(dataframe, dataframe_tot) %>%
20. mutate(goals_per_session = ifelse(goals > 0, round(goals / sessions, 2), 0))
21. //insert code segment here

You want to generate a timeseries, identify outliers and plot the data. Which code segment would you use in line 21?


Option 1

dataframe_ts <- dataframe_all %>%
## the package works with POSIX date format
mutate(date = as.POSIXct(date, origin = "1970-01-01", tz = "UTC"))
 
 
AnomalyDetectionTs(dataframe_ts_ses[, c(1, 3)], max_anoms = 0.05, direction = 'both', e_value = TRUE, plot = TRUE) ## 5% of anomalies
AnomalyDetectionTs(dataframe_ts_ses[, c(1, 3)], max_anoms = 0.1, direction = 'both', e_value = TRUE, plot = TRUE) ## 10% of anomalies

Option 2

dataframe_ts_ses <- dataframe_ts %>%
dcast(., date ~ channel, value.var = 'sessions')
dataframe_ts_ses[is.na(dataframe_ts_ses)] <- 0
 
AnomalyDetectionTs(dataframe_ts_ses[, c(1, 3)], max_anoms = 0.05, direction = 'both', e_value = TRUE, plot = TRUE) ## 5% of anomalies
AnomalyDetectionTs(dataframe_ts_ses[, c(1, 3)], max_anoms = 0.1, direction = 'both', e_value = TRUE, plot = TRUE) ## 10% of anomalies

Option 3

dataframe_ts <- dataframe_all %>%
## the package works with POSIX date format
mutate(date = as.POSIXct(date, origin = "1970-01-01", tz = "UTC"))
 
dataframe_ts_ses <- dataframe_ts %>%
dcast(., date ~ channel, value.var =null)
dataframe_ts_ses[is.na(dataframe_ts_ses)] <- 0
 
AnomalyDetectionTs(dataframe_ts_ses[, c(1, 3)], max_anoms = 0.05, direction = 'both', e_value = TRUE, plot = TRUE) ## 5% of anomalies
AnomalyDetectionTs(dataframe_ts_ses[, c(1, 3)], max_anoms = 0.1, direction = 'both', e_value = TRUE, plot = TRUE) ## 10% of anomalies

T Option 4

dataframe_ts <- dataframe_all %>%
## the package works with POSIX date format
mutate(date = as.POSIXct(date, origin = "1970-01-01", tz = "UTC"))
 
dataframe_ts_ses <- dataframe_ts %>%
dcast(., date ~ channel, value.var = 'sessions')
dataframe_ts_ses[is.na(dataframe_ts_ses)] <- 0
 
AnomalyDetectionTs(dataframe_ts_ses[, c(1, 3)], max_anoms = 0.05, direction = 'both', e_value = TRUE, plot = TRUE) ## 5% of anomalies
AnomalyDetectionTs(dataframe_ts_ses[, c(1, 3)], max_anoms = 0.1, direction = 'both', e_value = TRUE, plot = TRUE) ## 10% of anomalies
Instruction: Choose the option that best answers the question. 

---

Examine the code which is written to create an external table titled LineItem located in the database skillsoft, to store Parquet format data in S3 bucket.


CREATE EXTERNAL table "skillsoft"."LineItem" (
L_ORDERKEY BIGINT,
L_PARTKEY BIGINT,
L_SUPPKEY BIGINT,
L_LINENUMBER INT,
L_QUANTITY DECIMAL(12,2),
L_EXTENDEDPRICE DECIMAL(12,2),
L_DISCOUNT DECIMAL(12,2),
L_TAX DECIMAL(12,2),
L_RETURNFLAG VARCHAR(128),
L_LINESTATUS VARCHAR(128),
L_COMMITDATE VARCHAR(128),
L_RECEIPTDATE VARCHAR(128),
L_SHIPINSTRUCT VARCHAR(128),
L_SHIPMODE VARCHAR(128),
L_COMMENT VARCHAR(128))
PARTITIONED BY (L_SHIPDATE VARCHAR(128))
STORED as PARQUET
LOCATION 's3://skillsoft/data/lineitem_partition/';

You want to add partitions in the table to ensure the data is distributed based on the column saledate for the month of February and March. Which ALTER TABLE statement would you use to create the required partition, so the data is distributed in two different files titled l_shipdate=1992-01-02 and l_shipdate=1992-01-03?


Option 1

ALTER TABLE "skillsoft"."lineItem"
ADD PARTITION(saledate='2017-01-02')
LOCATION 's3://skillsoft/lineitem_partition/l_shipdate=1992-01-02/';

ALTER TABLE "skillsoft"."lineitem"
ADD PARTITION(saledate='2017-01-03')
LOCATION 's3://skillsoft/lineitem_partition/l_shipdate=1992-01-03/';

T Option 2

ALTER TABLE "skillsoft"."lineItem"
ADD PARTITION(saledate='2017-01-02')
LOCATION 's3://skillsoft/lineitem_partition/l_shipdate=1992-01-02/';

ALTER TABLE "skillsodt"."lineitem"
ADD PARTITION(saledate='2017-01-03')
LOCATION 's3://skillsoft/lineitem_partition/l_shipdate=1992-01-03/';

Option 3

ALTER TABLE "skillsoft"."lineItem"
ALTER PARTITION(saledate='2017-01-02')
LOCATION 's3://skillsoft/lineitem_partition/l_shipdate=1992-01-02/';

ALTER TABLE "skillsoft"."lineitem"
LOCATION 's3://skillsoft/lineitem_partition/l_shipdate=1992-01-03/';

Option 4

ALTER TABLE "skillsoft"."lineItem"
LOCATION 's3://skillsoft/lineitem_partition/l_shipdate=1992-01-02/';

ALTER TABLE "skillsoft"."lineitem"
LOCATION 's3://skillsoft/lineitem_partition/l_shipdate=1992-01-03/';
Instruction: Choose the option that best answers the question. 

# Data Scientist

Question: Identify the essential properties that are required when we build stacked bar plots using the method barplot.

xlab

col

---

Question: Which of the following methods of R can we use to build area charts?
Result: Partially correct. The correct answers are indicated. 

geom_area()

theme_bw()

---
Question: What are some of the possible values of the Scale property that we can use to normalize heat maps?
Result: Partially correct. The correct answers are indicated. 

Row

Column

---

Select some of the essential components of ggplot2.

Stats
Geom

---

Question: Which of the following graph properties can we use with the method qplot() provided by R in order to customize graphs?

Geoms
Facets

Question: Choose the essential visualization components provided by R.

Univariate visualization
Visualization packages

---

Specify some of the prominent 3D charts that we can create using Plotly in Jupiter Notebook.

3D scatter plot

Ribbon plots
---

FFMpegWriter

PillowWriter



