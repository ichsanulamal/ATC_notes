An Introduction to Generative AI Concepts
This comprehensive course delves deep into the fascinating world of Generative AI. Through a combination of engaging lectures and hands-on practice, participants will gain an in-depth understanding of what generative models are, how they differ from other AI techniques, and the theories and principles underlying them. You will discover various types of generative models, such as generative adversarial networks (GANs) and variational autoencoders (VAEs), and explore the process involved in training these models. Then you will examine the strengths, limitations, and practical applications of generative models across various domains, such as image generation, text generation, and data augmentation. Next, you will learn how to evaluate the performance of generative models and focus on ethical considerations in generative AI and the potential societal impact of these technologies. Finally, you will have the opportunity to generate synthetic data using generative models for training and testing purposes and investigate the notion of responsible AI in the generative era. Upon course completion, you will be prepared not just to use these powerful tools, but to use them wisely and ethically.
Table of Contents
    1. Video: Course Overview (it_gcdgaidj_01_enus_01)

    2. Video: What Is Generative Artificial Intelligence (AI)? (it_gcdgaidj_01_enus_02)

    3. Video: Generative AI vs. Other AI Approaches (it_gcdgaidj_01_enus_03)

    4. Video: Fundamental Concepts in Generative AI (it_gcdgaidj_01_enus_04)

    5. Video: The Generative AI Ecosystem (it_gcdgaidj_01_enus_05)

    6. Video: Generative Models: GANs and VAEs (it_gcdgaidj_01_enus_06)

    7. Video: Workflow of Training Generative Models (it_gcdgaidj_01_enus_07)

    8. Video: Strengths and Limitations of Generative Models (it_gcdgaidj_01_enus_08)

    9. Video: Generative AI: Prospects and Challenges (it_gcdgaidj_01_enus_09)

    10. Video: Synthetic Data with Generative AI (it_gcdgaidj_01_enus_10)

    11. Video: Generating Synthetic Data with Generative AI (it_gcdgaidj_01_enus_11)

    12. Video: Ethical Considerations in Generative AI (it_gcdgaidj_01_enus_12)

    13. Video: Generative Model Evaluation (it_gcdgaidj_01_enus_13)

    14. Video: Real-world Applications of Generative AI (it_gcdgaidj_01_enus_14)

    15. Video: The Impact of Generative AI on Society and Creativity (it_gcdgaidj_01_enus_15)

    16. Video: Responsible AI in the Generative Era (it_gcdgaidj_01_enus_16)

    17. Video: Course Summary (it_gcdgaidj_01_enus_17)

1. Video: Course Overview (it_gcdgaidj_01_enus_01)

In this video, we will discover the key concepts covered in this course.
discover the key concepts covered in this course
[Video description begins] Topic title: Course Overview. Presented by: Elias Zoghaib. [Video description ends]
This comprehensive course aims to delve deep into the fascinating world of generative AI. Through a combination of engaging lectures and hands-on practice, I will give an in-depth understanding of what generative models are, how they differ from other AI techniques, and the theories and principles underlying them. I'll also cover the various topics of generative models, such as generative adversarial networks and variational autoencoders, and explore the process involved in training these models.

I'll further examine the strengths, limitations, and practical applications of generative models across various domains, such as image generation, text generation, and data augmentation. Key topics of discussion will include evaluating the performance of these generative models, ethical considerations in Generative AI, and the potential societal impact of these technologies. Participants will also have the opportunity to generate synthetic data using generative models for training and testing purposes. I'll cap this course off with an exploration of the notion of responsible AI in the generative era, preparing learners not just to use these powerful tools, but to use them wisely and ethically.

2. Video: What Is Generative Artificial Intelligence (AI)? (it_gcdgaidj_01_enus_02)

After completing this video, you will be able to define generative AI and its key components.
define generative AI and its key components
[Video description begins] Topic title: What Is Generative Artificial Intelligence (AI)?. Presented by: Elias Zoghaib. [Video description ends]
In this video, I'm going to be discussing the generative AI and its key components. So, first, with the brief definition of what is generative AI. Generative AI, or generative artificial intelligence refers to the deep learning models that can take things like raw data and generate probable outputs. Now, what's happening here is that we are going to be based on the raw data that we fed this deep learning model, which we'll talk a little bit about what that is, a little bit later.

We are going to ask it to do an action based on the data that it knows, and what'll happen is it'll spit out some information. Could be an image, an audio file, a number, whatever it is, and it will encode a simplified representation of the output so that a human can understand it. So what it does is it will take all sorts of data, it could generate and input images, or even you'll see it create some speech audio files that will replicate a human's voice and it will be completely indistinguishable from a human. You would never be able to tell that this was actually generated from an AI, and it even works with all sorts of complicated data, even structured data or unstructured data.

And this makes it very powerful for things like pattern recognition. In fact, if we look at traditional AI and generative AI, they're both very good at pattern recognition. The issue with the pattern recognition in traditional AI is that it's going to be making predictions based on that pattern, whereas with the generative AI, it'll create new patterns based on the data that you fed it. This is where the generation of our AI comes in.

The term generate a pattern or generate data because we're going to be making predictions with traditional AI. However, with generative AI, we're going to be creating something new or generating new data or generating new images based on the training set that we fed. And so this is pretty profound because what it revolves on is basically with traditional AI we're teaching it rules and it's very rule-based in that we come up with those rules as a human being. But with a generative AI, it's all data-driven. Based on the information I'm giving it, it's going to create these patterns, come up with its own rules to figure out what it needs to do to get the right answer.

Such as when I ask it for pictures of a cat it wants, we train it so that when it spits out a picture of a cat, it says that it hasn't made an error. And so then it has to generate its own set of rules from its data that we're training it on and gives us the output. Now the interesting thing is, just as I said before, I'll say it again, is that the rules are generated by humans for traditional AI. But what's happening with the generative AI and its pattern creation process is it's actually learning from very large datasets, and I mean very large.

There could be petabytes of information that we're using to generate this information. So, this leads to another concept that I want to discuss and that's a type of generative AI that's called variational autoencoders. Now, this is a specific deep learning model. They are particularly generating images and speech. And the biggest significant advantage is that their potential for simplified scaling.

Now what this means is we're really referring to the idea of creating new models or advanced models based on the variations within our dataset. So if I'm just training my images, I'm training the autoencoder on pictures of cats, what I expect would happen because my training data doesn't really vary that much. It's all cats. There won't be as many parameters for us to actually deal with, but if we had pictures of cats and cars and all sorts of things, well, now there's a large variance in my training set.

Well, we're now need to scale that model so it can count for more parameters so that it can fit those images to this new, more varied data set that I'm trying to do. And that's what I mean by simplified scaling of models here. Now, the interesting piece about this is that we are talking about encoding and decoding processes. Now when I refer to an encoder, what this means is take some input that's compressed and that's referring to the encoding process. Now what I mean by compressed is that it takes an image for example, and we see pixels on a screen.

The encoder will first transform this into a matrix where it could be the brightness of pixels or the colors of pixels if it's a color photo. Or if it's just a black and white photo, it will just be a matrix with the zero being completely white and one being black. And so then how it takes that image that a human understands, transforms it into a matrix based on the brightness of each pixel in that photo, and then it does some process with it. Now the decoding is where the original input is reconstructed from this compressed form. So the model is going to do some calculation and come up with a new matrix.

So now we're going to have different brightness levels for each of the pixels. And then what we want to do is take those brightness levels and put them back into a form that a human can understand, which is usually an image. And so this is where we get the idea of decoding and encoding. We take something in a human format, transform it into a machine understandable format, the machine does an operation on this machine understandable format, and then it's able to transform it back to a human readable format, which in this case would be a picture.

Now the subsequent technologies that come out of autoencoders are generative AI, and this is where we come up with the generative adversarial networks and we call these GANs in the literature. And what this means is that we're going to be pitting two networks against each other and they're going to produce high quality results. So, the first network is trying to get the right answer and the second network is trying to detect if the answer is right or wrong. If it's wrong, it's going to do what's called a feedback set for the first network that was making the guess. And says these are the patterns that you were making that were incorrect.

The original network that's generating the images will go back and make some changes to its model and try again. And you do this millions of times on millions of different inputs and eventually, you end up with a very advanced network that can be used. And this refers another process is known as the diffusion model. Now diffusion models are capable of producing even more realistic, still fake images and this is where we've used the variational autoencoders to set the stage for these two types of models. So, the next thing I want to talk about are known as transformers. Now, this is actually introduced by Google in 2017.

There's an actual paper that you can look this up, it's called Attention is All You Need. And what this does is it combines the encoder/decoder architecture with the text processing mechanism and it's going to show you how language models were particular trained. So they found an application for this encoder, decoder, and text processing and not just image processing, but actual languages. And what happens is, is that with this input on these types of encoders, we found that it can actually learn how different languages can work.

And this is basically how you can think about Google Translate is going to work soon. And so this is where we end up with things like parallel text processing. And what's groundbreaking is this ability to process text in a parallel fashion. And it's very flexible. It doesn't actually need a predefined task. What's crazy is that you can just give it a set of languages, articles in a particular language, and it will decipher how that language works. It will know the pattern of how to basically speak English or whatever language you fed it.

Now transformer world, we've got essentially three different types. First, we've got the encoder-only models and this is just really good at processing input data. And there are models like Bert, which power search engines and customer service chatbots. If you ever use something like the IBM Watson Assistant, it's powered by this encoder-only transformer. And then we have something known as the decoder-only models, and these are very popular very recently. If you've ever used the GPT family of models like ChatGPT for example, or GPT 3, what's happening is, is that they're trying to predict the next word without an encoded representation.

So, it doesn't need to take the English language and transform it into a matrix of numbers so that the machine can understand it. It actually takes the raw language and predicts what's the next word. And because of that, it needs a 175 billion parameters so that it can guess statistically what you're trying to say. So, and this has been released by OpenAI in 2020. And so there's even more advanced models that are coming with even more parameters.

Think about the number of parameters in the models as the level of flexibility the model will contain. So, the more parameters you have, the more flexible it is. And so the ChatGPT 3 models have 175 billion parameters, but there are even more ones called the Google POM for just as an example is 540 billion parameters. So that's even going to be more flexible than GPT family when that released. And so then we have this thing known as the encoder-decoder model. If we combine these two things together, what you'll see is that we can utilize both of these encoder processes and our decoder processes to allow them to get the best results for enhanced results from both of these things.

So if you've looked at the Google text to transfer transformer or what's known as the T5, they'll combine features of the GPT style models and the Bert style models. And what they can do is many of the generator's tasks that the decoder-only models can, but their compact size makes them faster and cheaper to tune and serve on different models. If you think about these decoder-only models like the GPT family, they are very large and very difficult to put on a server and build infrastructure around.

So, coming up with an encoder-decoder model like this is going to make things more portable. Now moving on, I want to talk a little bit about the resurgence of supervised learning. Now when we think about supervised learning and AI autonomy, really they're just buzzwords. But the supervised learning is coming back and you'll see it happening in a few places. We have techniques like instruction-tuning, and instruction-tuning is you're going into the model and you're transforming some of the pieces that are part of the model, that makes what steps that it has to change.

So, based on the inputs that you're providing it, the order of inputs, the variance in the input, that also matters. And what we're doing basically at this step is we're going to be talking about the zero-shot and few-shot systems. Now what this does is we are going to dramatically lower the time it takes to build an AI solution because there will be minimal data gathering that's required to get a result and zero and few-shot learning if you've ever will see it again, it gets very technical how this works underneath the hood.

But really, what's going to happen is that many generative models are sensitive to how their instructions are formatted, and this has inspired a new AI discipline that I want to bring up, and this is known as prompt-tuning. Now what I'm going to do in prompt-tuning or prompt engineering is a good instruction. Prompt will deliver the desired results in one or two tries, but you first have to place the colons and carriage returns to the right place for example. You have to be careful about how you're prompting the system.

And really this is, it seems a little strange at first because we are talking about a computer program that something like Google. You know, if I enter something into Google and you enter something into Google, we're bound to get the same kind of results. But with these kind of models, depending on the order and types of words that you're using, you could get a completely different result than I am. And so you can guide the model to produce results that you're interested in by kind of warming it up in a way.

I know it sounds a little strange, but you'll see a few examples later on, on how this is actually done. And so then there's things like the adapters. These are major alternatives. They allow you to adapt the model without having to adjust its billions or even trillions of parameters. They work by distilling the user's data, and they target task into a small number of parameters that are inserted into what's known as a frozen large model, which is the parameters aren't going to be changing.

There they'll modulate the model's behavior without directly changing it. And this is kind of how you keep your AI on rails. If you're trying to control what it could say depending on particular prompts, you've used this kind of adapter to keep everything in track. And so then the alignment here that I want to talk about is you'll see that human supervision is shaping the generative models by aligning their behaviors with ours. And so when I refer to alignment, what I'm really referring to is the idea that we can shape a generative model's responses so that they better align with what we want to see with human responses.

And we do this using what's known as Reinforcement Learning from Human Feedback. If you read some papers, you'll probably see some acronyms attached to this, which is RLHF. And this is just an alignment method that's popularized by OpenAI, the makers of GPT that gives models like our ChatGPT, their human-like conversational abilities. And what's happening with this process is that a generative model will output a set of candidate responses that humans will rate for correctness.

And based on your rating, it understands that, OK, I generated these five. Number three is the most human-like. I'm going to try and make answers like number 3, and this is known as the reinforcement learning. The model is adjusted to output some more responses, like those that are highly rated by the humans. So, think of every time you use these GPT models, you're teaching it what it means, how it responds.

You're teaching it what's more human and what's less human. And that's the fascinating things about this, is that it's basically trying to fit us. It's trying to learn from its users. So, this is a brief introduction to what generative AI is. But we could talk about this for hours, about what these pieces are and how they work and connect with each other. This is just a very broad view of what generative AI is and we've talked a little bit about some of the transformers and applications for different types of variational autoencoders and generative adversarial networks.

So, where is generative AI headed? Well, until recently we've seen that there's a dominant trend in generative AI and it's always been about scale. And when we're referring to scale, what we're really talking about are the three components of scale, which are the model size, the training data size, and the computational resources required to make these training steps work for these generative AI. And what we're trying to do is we're trying to achieve better and better results.

Now with these kind of scaling laws, it allows AI and large language models will relate to how increasing the model size can lead in its improvement, but only up to a certain point. Afterwards, we get this kind of diminishing return, but it allows us to identify any emergent capabilities that arise when we reach a certain model size. So it's not just the models architecture that causes these skills to emerge, but the actual size of the model, things like the size of our training state, the model size itself and what training steps we're following.

And this could include like glimmers of logical reasoning and the ability to follow instructions. And so what we're trying to do here is the future is going to involve labs continuing to train on very large models and we're trying to chase these emerging capabilities that are coming out. And one of the other things that we're trying to do is this concept known as model distillation. So the question of whether generative models are going to be bigger or smaller is going to be looked at with this area of research.

This comes from a Stanford group, and the idea is that they're trying to use another model such as ChatGPT 3.5 to generate thousands of instructions and responses through what's known as instruction tuning, and they're going to use this AI-generated data to infuse it with another model. So if this other model doesn't have very good conversational skills, but say something like ChatGPT 3.5 has great conversational skills, well, why don't we distill some of the outputs from ChatGPT 3.5 into this newer model that we're trying to build? And that way we can almost look into piggybacking on our already existing model. And that's the concept of model distillation.

And so that's where we're going to see in the next year, next couple of years what the research is going to be like for the future of generative AI. And so this has been just a brief overview of how generative AI works, how they're connected together at some of the pieces and the advantages and disadvantages of using them, as well as the future capabilities of generative AI.

3. Video: Generative AI vs. Other AI Approaches (it_gcdgaidj_01_enus_03)

Upon completion of this video, you will be able to differentiate between generative AI and other AI methods, showcasing their unique features and use cases.
differentiate between generative AI and other AI methods, showcasing their unique features and use cases
[Video description begins] Topic title: Generative AI vs. Other AI Approaches. Presented by: Elias Zoghaib. [Video description ends]

In this video, I'm going to explain the differences between generative AI and some other AI methods, and I'm going to showcase some of their unique features and use cases. Let's first understand what discriminative models are. Discriminative models refers to a class of models that are used in statistical classification. We mainly use these in supervised machine learning.
These types of models are also known as conditional models since they learn the boundaries between the labels in a dataset. Think like pictures of dogs versus pictures of cats. Those are two labels on a dataset. But what's happening? You'll refer to those labels sometimes, depending on the literature that you're reading, as classes. So try not to get confused if you see that terminology. Classes or labels in a dataset are just the answer to the picture that's attached.

So with the discriminative models, they focus on modeling the decision boundary between these classes and the classification problem. The goal is to learn a function that maps inputs to binary outputs, and this prediction of the specific class labels is the whole point of using a discriminative model. We use things like the maximum likelihood estimation to estimate the parameters of that discriminative models. These could be things such as the coefficients of a logistic regression model or the weights of a neural network.

Now the key here is the question that we want to ask is what is the difference between the generative and the discriminative models in deep learning. Discriminative models are focusing on modeling this decision boundary by using the prediction of a specific class label. Given the input data, the goal is to predict that specific class label. With discriminative models, what are we using for in the classification tasks? Well, these are classification-specific tasks.

Remember, discriminative models are used for classification tasks where the goal is to predict that class label just as I said before. These class labels were predicted based on the features of the dataset that it's trying to analyze. Discriminative models separate these classes instead of modeling the conditional probability, and they don't make any assumptions about the data points. Now, the models aren't capable of generating new data points, they're just meant to predict a particular feature about that data point. In this case, it would be the label.

The ultimate objective is to separate these classes from the other data points, and these are actually used everywhere, particularly in financial institutions. If we have somebody's financial records, we can predict based on plenty of people from before what this label is on this user.

For example, is it possible that they could default on a loan I'm about to give them based on their income, based on their postal code, based on all sorts of information that you give it, you could get some type of label on a particular class of customer, they could be to a bank. And so this is really powerful in different industries such as that or even if I give it images of a tumor or a particular skin lesion, is it possible that it can take a look at that image and based on all of the datasets that we've fed it about cancerous tumors or cancerous moles on a person's skin, just from the picture, can it decide if somebody has that kind of cancer?

And it's possible for it to do that. And so there's a few different types of discriminative models that we're using. We've got things like the logistic regression. Logistic regression is a type of classification that is trying to predict between two classes. It's either zero or one, cancer or no cancer, failure to pay, or successful to pay in a particular loan.

There's also neural networks. What we're doing here with neural networks is each neuron in that network is going to have a particular weight associated with it. So we're figuring out what that weight is going to be and as well be able to come up with separate different classes. Not just two classes, but you could have five or six or seven or multiple classes that you want to be able to identify and classify. There's also known as the Support Vector Machine.

This is another type of discriminative model. It's very good at figuring out again what the boundary is between the classes and decide based on how many classes there are in the dataset what that best boundary is going to be and see where the data points will lie in which region and its boundary that it's created. There's also known as the Conditional Random Field and another one, the easiest one to understand is the nearest neighbor.

Similar if I have very similar data to somebody who is very successfully financially, the model is going to decide that I'm a good resource because the other customers have been successfully financially and that's what it's going to do. It's finding my nearest neighbor in the dataset in the region that's it's been trained on. And then there's the decision tree. And this basically is like if you ever played 20 questions, this is a lot like what this is. Now, you're basically asking a few different hierarchy of questions and you're trying to see, OK, the answer based on the answer to those questions, you can classify them in a particular region or particular bin and that's what a decision tree is.

So the generative models are considered a class of statistical models that can generate new data instances, and they primarily work with unsupervised learning, using the unsupervised machine learning as it means to perform tasks that could be used in probability and likelihood estimation. I can model the data points, I can even describe particular trends or phenomenon within the data, and I can distinguish between the classes based on the probabilities that I'm able to generate. And there's a few different types of common generative models.

But before I move on to talking about the types, remember core idea between a discriminative and a generative model. If you think of a discriminative model, we're trying to draw the boundaries in data space. And I mean by that is, if I had the two boundaries on a graph, I can draw a boundary between the two data points. And generative models, will try to model how data is placed throughout that space.

Now, a generative model will explain how the data was generated. While a discriminative model will focus on predicting the labels of that data. That's the key piece here. Let's move on to talk about some common generative models. Few of them that I want to talk about is the Bayesian networks. And so remember, this is all very math heavy, so I don't want to get too far into it.

But when we're training generative classifiers, what we're trying to do is we're trying to estimate a function or a probability. What is the probability of why given these features on that data point? So, that's kind of what we're trying to do. And the Bayesian network and Naive Bayes, and the Markov random fields and the Hidden Markov Model or the Generative Adversarial Network that we're talking about today. Even the autoregression model or the Latent Dirichlet Allocation or the LDA models, they've all been used as generative models.

They're meant to look at probabilities of the system and the one that we're really concerned with is the scan or the generative adversarial networks. Now with generative models, it gets a little complex very quickly because the math becomes very complicated. A human is not able to understand the steps that the model is going to be making.

So when we're doing the generative approach, remember we're focusing on the distribution of the individual classes in our dataset. We're trying to model the joint distribution of inputs and the outputs, and these learning algorithms will tend to model this underlying pattern. Now, as it does that, it's going to be looking at things like Gaussians or some type of distribution of the data points. It's going to identify what that distribution is, and they'll use the concept of this joint probability to create instances or a given feature of that input and the desired output of that label that you're trying to predict will exist simultaneously in that distribution.

Now these models are using these probability estimates and this concept of likelihood and statistics to model that data point and differentiate between these two different class labels. Now the output data should be in the same distribution as our training data. That's really important. So as in, I basically need to have my output data in the same type of distribution as my input data as the training data that I've provided.

That means if my training data is following this kind of distribution, such as a Gaussian, my output should also be in that same distribution as that training data. Now, there's some drawbacks here because if there's some outliers in the dataset, then it'll affect the types of models to a significant extent. You need to remove any outliers in your training set to get something that's useful that can be generated from this generative model. So, this is the key differences between the generative models and the discriminative models.

4. Video: Fundamental Concepts in Generative AI (it_gcdgaidj_01_enus_04)

In this video, we will identify the fundamental principles and theories that drive generative AI.
identify the fundamental principles and theories that drive generative AI
[Video description begins] Topic title: Fundamental Concepts in Generative AI. Presented by: Elias Zoghaib. [Video description ends]
In this video, I'm going to be discussing the fundamental concepts in generative AI and I'm going to identify and explore the fundamental principles that drive generative artificial intelligence. First start with generative AI concepts. Generative AI models will combine various AI algorithms to represent and process the content.

For example, to generate text, there could be some various natural language processing techniques that'll transform the characters such as letters, punctuation, words into sentences, and parts of speech and actions. Now what we're doing here is we're taking something that's known as an autoencoder, and these autoencoders are a type of neural network that's used in these generative models. They learn to compress this input data could be used like in letters and punctuation of words into sentences into a lower dimensional or what's known as a latent space.

And a latent space is basically a format of the data that you've provided to this model that only a machine can understand. You can understand it by looking at it, but it would just look like numbers. It would just be a very complicated matrix of numbers that a human being won't be able to grasp or see. And what we're going to do is then do some type of process on these numbers and then reconstruct the original data from this compressed representation later on after we've done some changes to it.

Now the key generative AI concepts that it's built on is known as Bayes' theorem. Now this is a fundamental principle in probability theory and statistics. Again, a lot of the concepts that power generative AI are mathematical and there's really no separation between the two because on the inside of these systems are very complicated statistical systems.

And this is a fundamental principle of probability theory and statistics that will quantify the relationship between conditional probabilities of statistical quantities. And the context of generative models such as Bayes' theorem is used in algorithms to update the probability estimates in the light of new data such as Naive Bayes' learning. And so, this is deep learning concepts in neural network that are fundamental to many generative models.

Because we're able to use these types of classifiers and Bayes' theorem and Naive Bayes' classifiers to make some type of prediction. The models will learn to represent the data by training on large amounts of information that we've given it previously. The generative adversarial network, or GAN will operate based on this GAN theoretic framework where you're going to have two different models. They're typically neural networks as well, and what they're going to do is they're going to compete with each other.

So, one network will generate new data instances, that's known as the generator and the other value that will evaluate them as discriminator. So, the aim is to train the generator network to produce data that the discriminator cannot distinguish from real data. That's the key. That's what makes these two systems really kind of go yin and yang with each other. One of them is trying to sneak one past the goalie in a way.

They're trying to generate data so that the discriminator can't tell them apart from real data that we fed the discriminator to. And the generative versus the discriminative models in this context and machine learning distinction between these generative and discriminative models. Remember, the discriminative models will learn the boundary between the classes and generative models will learn the distribution of the classes. So, the discriminator is really good at understanding fake data and real data, and the generator is going to try and create data that the discriminator can't tell the difference between, so it can't tell if it's real or fake.

So that's the idea here. The discriminator isn't actually generating data, it's just analyzing it and classifying its label. And the generator model is what's actually creating pictures and the discriminator is trying to place it in these regions. And when the discriminator can't tell the difference between what region to put something in, it can't tell if it's actually real data or generated data by a machine. That's where that sweet spot is.

That's where the generative network is really advanced now. Now the key generative AI concepts is actually interesting because now we've got the variational inference and variational autoencoders. Now the variational inference is a method of approximating this complex distribution. And what we're doing here is we're using this in variational autoencoders, the type of generative model that adds probabilistic tools to the concept of these autoencoders.

Now, when we use some of the probability theory and the statistical inference, remember these generative models will leverage the probability theory to model and understand the distribution of that data. That's where the probability piece comes in. The statistical inference is used to make predictions about unseen or future data based on the observed data that we've given it in the training step.

So when I'm training the data, I'm giving it pictures of cats, pictures of dogs, pictures of cars, and it's going to understand what these things are and it's going to come up with a probability based on the things I give it. What can it do so that, OK, you've seen all these pictures of dogs generate me brand new pictures of dogs that I haven't seen before given to you. And this is where the statistical inference step comes from. Basically, the inference step is what happens when you take your completed trained model and try to deploy it so that you can actually use it in your business, for example.

Now the few last concepts I want to talk about, in generative AI concepts, where we're going to talk about the statistical pieces to generative AI and this is known as the maximum likelihood estimation or MLE. And this is a method that's used in estimating the parameters of a statistical model given observations. So in the context of generative AI, this maximum likelihood estimation is used to estimate the model parameters.

Remember when I was saying that the GPT family of video of suites for example, has billions of parameters. So what we have to do is we need to fit these parameters using this type of statistical system that we're doing, and that's the maximum likelihood estimation. And then we do this with reinforcement learning. And in some generative models like certain types of generative adversarial networks, the principles of reinforcement learning is really important.

So remember, the generator is going to generate new data. The discriminator is going to see if it can tell the difference between fake data and real data, and if it's, the really powerful thing is if the discriminator can say you're very close. These pieces that you generated I could tell were fake, and then tell that to the generator, and the generator is going to make corrections to itself based on the input that we're getting from the discriminator.

That's the key. Now another thing here is the transfer learning is a machine learning method where a model is developed for a task and is then reused as a starting point for a model on a second task. This is a really common approach in generative AI when fine tuning pre-trained models to generate new data in a specific domain. So these are the very important concepts in generative AI.

And if we're going to be talking about them for the rest of this course is understanding what these things are, the reinforcement learning, the maximum likelihood estimation and the fact that at the very core of a GAN of a generative adversarial network are two neural networks, a generator and a discriminator. So, this is the foundation of all generative AI concepts.

5. Video: The Generative AI Ecosystem (it_gcdgaidj_01_enus_05)

After completing this video, you will be able to outline the various tools, technologies, platforms, and communities that comprise the generative AI ecosystem.
outline the various tools, technologies, platforms, and communities that comprise the generative AI ecosystem
[Video description begins] Topic title: The Generative AI Ecosystem. Presented by: Elias Zoghaib. [Video description ends]
In this video, we will discuss the various tools, technologies, platforms, and communities that compromise the generative AI ecosystem. So let's get started with understanding typical use cases for generative AI. Generative AI can be applied in various use cases to generate virtually any kind of content.

The technology is becoming more accessible and it's used for all sorts of cutting-edge use cases. In fact, things like GPT can be tuned for different applications, so some of the use cases for generative AIs that I've seen so far is chatbots. Obviously, if we implement chatbots for things like customer service and technical support, using something like a ChatGPT model for communicating and providing customer service or answers to frequently asked questions is a great tool for generative AI.

Deepfakes is interesting as well. With deepfakes, what you're trying to see is we can deploy deepfakes for mimicking people or even specific individuals. That one can be a little bit dangerous if we're not careful, but it is a possibility to use generative AI to generate images on top of videos. Another thing that is very useful is scientific testing. In fact, I can give it two sets of data, ask it to interpret it in some form, and then it will be able to spit out some analysis based on that kind of sets of data that I'm providing.

It's great for scientific testing as well. And things like product design as well is for designing physical products and buildings or suggesting new compounds to test in scientific testing. These are all part of generating new pieces of information that don't already exist. But you know, we're using them in different use cases in the example of scientific testing. And not only can it interpret data efficiently, but it can also try and come up with new ideas that maybe you haven't seen before.

So, a great example in scientific testing as well kind of touches on product design is that we can design new drug compounds based on the previous compounds that it knows about. It might be able to point us in a direction that we don't yet see. There are some more use cases for generative AI and so I'd like to go over them as well. And that's one of them, is that we can improve dubbing for movies and educational content, especially in different languages.

So you can have different voiceovers and artwork being generated. It can create a photo realistic artwork in a particular style. You can create documents, things like e-mail responses, dating profiles, resumes, and even term papers which makes it quite challenging in different fields especially in education for trying to identify student plagiarism. One of the fascinating things that are actually going on right now and different services on the cloud are providing is music composition.

You can write music in a specific style or tone. One other issue is there, you can have product demonstration videos by it can improve your product demonstration videos by giving it the exact product and showing you what's possible with your product. And another thing is that it can do chip design optimization. This is very interesting. So, there are certain parameters that have to be optimized when designing pieces of hardware for computers. Your generative AI will be able to optimize that kind of design and hardware, especially the chips, so that you can have the most efficient hardware for your applications.

Now there are some popular generative AI interfaces that I like to bring up. One of them, of course, is the ChatGPT. This is a very powerful tool. If you haven't used it, go ahead it's offered by OpenAI, and this incorporates the all sorts of different tools. So, how this works is that they trained it similar to another application that we'll talk about in just a second on the Internet. So, they've taken a lot of web pages, a lot of human text that's been generated and provided it as a training module for this generative adversarial network called ChatGPT.

And it trained and it knows how to generate text. It's basically an advanced chatbot. It started in November 2022. It started with the ChatGPT 3.5 implementation. And then OpenAI has provided a way to interact and fine-tune text responses via this chat interface and interactive feedback. In fact, the way you can talk to it a lot like a person and it responds just like a person would.

So, the earlier versions of ChatGPT were only accessible via an API, but now the latest release I believe is ChatGPT 4, which requires a paid subscription. It was released on March 14th, 2023. It incorporates the history of its conversation with the user into its results, just like a real conversation would happen. After the incredible popularity of this GPT interface, Microsoft announced significant investment and it's actually now partnering with Bing search engine. And so this is going to power the Bing search engine in the future.

And if you'd like to try it out, download the Bing app or check out Internet Explorer, whatever it's called. I believe it's Microsoft Edge and have a Microsoft account and give it a shot. You'll be able to surf the web with ChatGPT. Another tool similar to what ChatGPT is doing with Bing, Bard is another tool that's specifically attached to Google. Google is another leader in pioneering transforming AI techniques for processing language. We've used it for things like proteins and other types of content.

It's open sourced and some of these models for researchers, but one of the things it's never released to is a public interface for these models. So Microsoft's decision to implement GPT into Bing quickly drove Google to rush to market a public-facing chatbot called Google Bard. This is only available in the States. It's not yet available in Canada, although you can access it with a VPN, I'm sure. It's built on a lightweight version of what's known as the LaMDA family of models. And this is just a very large language model.

Google suffered a significant loss in stock price following its Bard's rush debut, and it's been a big problem to actually use this tool. It's not yet available everywhere. It's kind of forcing Google in the market currently because GPT has taken the world by storm. So they're trying to come up with their own tool that's similar to the ChatGPT Bing partnership. Finally, not only can we have conversations with ChatGPT, and Bard, but there are other models, and this is called Dall-E. It's not for conversations. In fact, it's for image generation. It's trained on a large dataset of images and other associated text descriptions.

It's a example of a multimodal AI application that identifies connections across multiple media. It's a way for it to look at things like vision, text, and audio data. In this case, it connects the meanings to visual elements. And so now you can actually ask it to generate images so it knows what a picture of a cat is or a dog is or a car. So, asking it to generate that kind of image or mixing images together is a possibility.

And so, Dall-E again is another tool that's offered by the OpenAI Suite and this is the same company that generated ChatGPT. So, there are some more things that I want to talk about for common generative AI tools. And this is just kind of tool specific like Bard and these ChatGPT models and OpenAI tools, these are just in general. So for example, we can actually have voice synthesis tools. This includes things like podcast.ai which will actually generate and clean up your actual voice data.

And so that's really important for things like radio and television and broadcasting. If you're interested in looking at some tools, try Listnr without the e and Descript with like description but without the ion at the end. And so then there are some other tools called for code creation, and this is taken the world by storm as well. I use this a lot in my daily workflow. Code generation tools include things like codeStarter, Codex, and what's known as the GitHub Copilot.

If you haven't used this, what it does is it's an AI that's trained on open-source products that are hosted on the GitHub repositories, and it learns how the code is. So I can ask it certain questions to write me a Python function that does certain operations and because if it's already written by someone else somewhere on GitHub, it will retrieve that information and help me write that function, very little. I don't have to do very much anymore. It knows what I'm trying to do. The trick is asking it what you want it to do specifically and I touched on this earlier and this is the chip design tools.

So, AI chip design tool companies, you've got things like Cadence, Google, and NVIDIA, they all have their own in-house AI that helps them design these GPUs and CPUs for what they're trying to do. Another thing to bring up is text creation tools. We're using ChatGPT. This is Jasper and all Rytr and Lax, but the most advanced one that I've seen on the market currently as of 2023 is GPT.

So, this is great text creation tools and being able to generate paragraphs, emails, books, whatever you'd like. It's able to understand and create information for you on your command. Another thing as well is image creation tools. Again, I touched on Dall-E 2, but if you're liking to look at some other tools as well besides Dall-E 2 and the OpenAI tools, try things like Midjourney and Stable Diffusion. These are all powerful image creation tools that you can just prompt and bring up any information.

There's also music creation tools and this is really interesting. So it's all AI-generated music in a particular tone. So there's things like Amper, Databots, and MuseNet. In fact, AWS has its own service that will help you generate music as well and we'll touch on that a little bit later. Another thing that I want to bring up here is the use case for generative AI by its industry. So obviously, I'm giving you some great tools and some techniques that are being used. But specifically, what is happening?

What are we using this thing for? Well, these new generative AI technologies, they can be described as general purpose technologies, all sorts of things like electricity and computing, because they can profoundly affect many industries and use case. But if I can hone it down to a single thing, something like gaming companies will use generative AI to design game content levels or items or something like that. If you've got plenty of other games in your library, as a game developer you can actually draw from those and mix these things together because they're all your own original content.

Manufacturing, manufacturers can use generative AI to combine data from things like cameras and X-ray and other metrics to identify defective parts and the root cause more accurately and economically. And then there's architectural firms that can use generative AI to design and adapt prototypes more quickly for particular building. And then what's really interesting is the medical industry can use generative AI to identify promising drug candidates more effectively.

Or that's essentially what's happening is that we can actually use these to generate new drugs that might be better fit for the disease or the treatment that we're trying to target. And finally, I just want to touch on a couple more because it's so widespread now this generative AI, it's so powerful. We've got things like the film and media companies that can use generative AI to produce content more economically and translate it into other languages with the actor's own voices. In fact, one of the applications that they're trying to do, or at least were trying to do in the past, is being able to use extras.

So if we take an image of you as a person, we can use you and your likeness as a background in scenes for movies and TV shows without you having to be there. You're just superimposed onto the image. And then there's the finance. This can watch transactions in the context of an individual's history and we can use that to better build fraud detection services and systems. So, it's watching you and your financial patterns and your financial behavior, and then all of a sudden, once something starts to go wrong, your finance industry will identify, or your financial institution can identify that there's some type of fraud, such as a sale of something or a purchase of something halfway across the world when you were just at home.

And then there's legal. Legal firms can use generative AI to design and interpret contracts, and they can analyze evidence and suggest arguments, again by feeding it all sorts of information on previous cases for the last few 100 years, if you'd like, and have it spit out information that's relevant to your case, just like you were talking in a ChatGPT interface. So, those are some industries that use generative AI, and that's just some of the widespread use cases that we're talking about when it comes to things like text, image, and video generation.

6. Video: Generative Models: GANs and VAEs (it_gcdgaidj_01_enus_06)

Upon completion of this video, you will be able to describe generative models like GANs (generative adversarial networks) and VAEs (variational autoencoders), including their structure and function.
describe generative models like GANs (generative adversarial networks) and VAEs (variational autoencoders), including their structure and function
[Video description begins] Topic title: Generative Models: GANs and VAEs. Presented by: Elias Zoghaib. [Video description ends]
In this video, I'm going to detail some of the exploration of different generative models like generative adversarial networks and variational autoencoders. I'm going to discuss their structure and their function. Let's first with understanding the generative adversarial network or GAN. These GANs or generative adversarial networks were first introduced by Ian Goodfellow in 2014. They are components of two. They have two things inside of them. They play a game of two, trying to learn from the distribution of the input of data to generate new synthetic data. And these two components are based on what's known as the generator and the discriminator and we'll discuss what that is in just a moment here. And what we're trying to do is we're trying to generate new data and decipher if any of these data that's being generated by the generator, is either fake or real.

Real being made by human and fake being generated by a machine. So, we're trying to learn this input distribution to generate new ones. So, based on my distribution of my input, I'm going to use that to make new data come out of it. Now, just as I discussed, the generative adversarial network has two components, the generator and the discriminator. Now, what's happening in this is that the generator will try to generate new data, and these two networks, the generator and the discriminator, are trained simultaneously in kind of a game. The generator will aim to produce data that the discriminator can't differentiate from real data, and the discriminator will try to get better at telling real data from fake data, and that's how that works. And, so, when the discriminator identifies that there's fake data, it tells the generator that I've made a mistake and since that what's called backpropagation to the generator to allow it to fix itself. This is basically what's happening here.

This is a kind of a complex diagram, so just bear with me here to sort of explain what's happening. Now, the generator's purpose is to produce, let's say, synthetic images from original ones with random noise in order to fool the discriminator. That's how it work. You know, I can't just give it an input of an image and then use that same image against the generator, against the discriminator. It won't work. What it has to do is it has to get a little bit of noise on top of the image that it's generating and sending that out to the discriminator so that it can do its purpose, which is to classify the images created by the generator and decide whether they are synthetic, which is created by the generator, or they're an original as in it hasn't seen it before. And, so, that's what's actually happening here is that the generator will take the noise array and then generate some fake images and then feed it to the discriminator.

Now the discriminator has some real images that it knows from. And what it's going to do is it's going to decide if this is a image coming from the generator or if it's a brand new real image that's been created by something else. And once it makes a predicted label, it's going to say that this is real or fake and then what's going to happen is it's going to send a vector, which is known as a loss, back to the generator and this is where this backpropagation exists. This is where the generator either gets a pat on the head saying that it did a good job, doing its job correctly, as in it fooled the discriminator, or it gets corrections that it has to make and it fixes itself. So, that's the idea of what's happening here. And what we do now is with the generator model, we go back and we update our model and then we create a generated example based of a real example and we feed it back to the discriminator model and it just this chain reaction happens over and over and over again until we have something that's at a stable point. And then that's when the model has finished training as in it can't get any smarter. So, that's how generative adversarial networks work. But let's move on now to variational autoencoders.

So, variational autoencoders, these are a type of encoders that learn to encode and decode data in a way, while they're unsupervised in a way, but they're just meant for image generation. So, what I'm seeing here is that this was introduced in 2013 and what we're using these things as is unsupervised models to produce high quality images by analyzing and retrieving the fundamental information of that input image and then recreating it in another image. This is where the variational aspect involves approximating the distribution, which in this case is usually a Gaussian with a simpler one and using a specific loss function that ensures this. Now, I'll talk a little bit about what these loss functions are.

Now, what's going to happen here in the variational autoencoder is we're using what's known as a probabilistic architecture. So, this probabilistic architecture will consist of two major aspects. This is known as the encoder and the decoder network. And what I mean by an encoder is we're taking something, we're taking input data and we're transforming it into something that the machine can understand then, but not a human can understand it. So, think of it like that. When I do encoding and so I don't know if you know this, but let's say if that there's a black-and-white image. Well, a black and white image can be represented as a matrix where each cell in the matrix is a number between zero and one, one being black, zero being white. And it will have all sorts of numbers in between those, zero and one, and that will tell us what the brightness is. If I have multiple cells like a big matrix, I can make an image out of this. I can have black and white, gray. I can have different shades of gray and different shades of black, like that kind of thing until it's able to generate something that the human can understand.

Now we don't understand a matrix of numbers and can see an image, right? Human eye just needs to see a picture and then we understand what it is. But the machine has to take that and transform it into a matrix of numbers and for it to do an operation, and this part is known as encoding. And then when it's doing information on this encoded data, it has to then decode it. This can actually be represented in this image that you see on the left. You see the input data on the far left of the image, and you'll see that this could be an image, say it's a black and white data, which is a black-and-white image, which is just a cell of information. And then, the first thing we're going to do is we're going to transform that input data into something that the machine is going to be able to encode into latent space known as encoded data. Now, this is something that we don't even understand how to read. Only a machine can learn how to do it. So we're basically taking the full dimensions of the data, breaking it down into smaller dimensions, and then passing that encoded data that the machine can only understand, and send that over to latent space.

Now, the latent space does some strange operations that we don't fully grasp, and from there it can then decode the data into a reconstructed data that the human can understand, which in this case could be like a brand new image. So, that's the idea of what the encoder and the decoder will work, OK. So, it's the first component of a variational autoencoder is known as the encoder. The second component of a variational autoencoder is the decoder, and this is formed by what's called a convolutional neural network. And, so what we're doing here is we're taking data coming out of this latent space and we're transferring it back into its original output. So, that's the pieces of what we're seeing.

We're seeing the input goes to the encoder, which does some operation on it, goes through the latent space, which does some actual machine-specific operation that we won't be able to grasp or understand what's going on there. And, then, it decodes it and sends it back into an output that we understand. Now, again, this slide here is a little bit advanced. There's a lot of math here that's going on. But the idea that I want to talk about with this slide is just showing you what the possible steps are to doing this. So, we're taking an input image, we're encoding it, we're then providing the latent distribution, going through the sampling step to add some noise, and then doing some type of latent vector. And, so, this latent vector is then going to be decoded into our reconstructed input. And, so, this is what's known as loss. So, what the loss is, is we take the reconstruction loss and the similarity loss. Basically it's a formula that looks at the reconstructed output and the input and sees if there's how much of a difference there is between the two. Now, the second component that I want to discuss is comparing these GANs and variational autoencoders.

So, that's a lot of math that we saw on the slide before. Again, we could spend a very long time explaining what's actually happening, but those are just some of the levels that we're seeing. Now, what we're doing here when we're comparing GANs and variational autoencoders, well, the key difference here is that there's a difference in training approach. VAEs or variational autoencoders, well, training will follow what's known as an unsupervised approach. But with GANs, they follow what's known as a supervised technique. That's kind of what the discriminator is for, right? It's supervising the generator. So, during their training phases, the variational autoencoder will aim to maximize the probability of the generated output and it'll respect to the input and produce an output from a target distribution. Another key component that's different between these things is what's known as the loss function. Now, loss just means like we're looking at the error, so the loss is derived depending on the variational autoencoder or generative adversarial network.

For GANs, we're referring to losses derived from the game between the generator and the discriminator, and it's what we call like a min-max game. It's trying to minimize the error and maximize its output. The variational autoencoder has two components. It has a reconstruction loss, which refers to how well the decoded data matches the original, and a regularization term that ensures the encoded data will follow the desired distribution, which in this case, the noise that's layered on top of the image. Now, GANs tend to be easier to train specifically because what we're seeing here, you've got the discriminator that's already an unsupervised trained classifier, and it does the easier thing to train because it's now spitting input and sending back feedback loops to the generator. It tells it what's broken and what to fix. And GANs are actually useful for more complicated tasks. Variational autoencoders don't actually help with the things like GPT can work.

You have to use things like a very complicated text input and output, are all done by generative adversarial networks. So, there are just some of the differences between these two. And I just want to give you a bit of a table here to discuss the GANs and the variational autoencoders. Now we know that in GANs, they focus on what's known as image generation by producing images that are identical to the original one with high resolution. And then we've got the variational autoencoder, which is also used for image generation. We've got text generation as well. They can be useful in audio and text generation and create never-before-seen audio and text at all. And the variational autoencoder is actually very good at natural language processing, not so much as text generation. Now, again I've shown you that there's audio generation, but the variational autoencoder will find any detection of anomalies within the data. And as we know with a generative adversarial network, keyword here is generate. This is meant for data augmentation. I can add more data as well. The variational autoencoder can be used to identify anomalies in medical data or potential frauds, all sorts of complex data that you can use in medical datasets by detecting these anomalies. And these anomalies in medical data could include things like tumors and diseases. And, then, finally with the generative adversarial network, we're seeing image editing is a very big one. So, that is some of the differences between variational autoencoders and generative adversarial networks.

7. Video: Workflow of Training Generative Models (it_gcdgaidj_01_enus_07)

After completing this video, you will be able to outline the process involved in training generative models, from data gathering to model deployment.
outline the process involved in training generative models, from data gathering to model deployment
[Video description begins] Topic title: Workflow of Training Generative Models. Presented by: Elias Zoghaib. [Video description ends]
In this video, I'm going to give a walkthrough of an entire process involved in training generative models, from data gathering to model deployment. Let's first understand an objective definition of what the training pieces are. Remember when we're talking about generative AI, we're talking about some models that have taken a lot of data to begin understanding its goals.

First, we have to provide this information to this dataset. So we have to 1st understand exactly what informations are required to fulfill this goal. Am I creating a generative AI to make images or generate text? Am I using it to generate video or particular music? If that's what I'm trying to do, then I need to provide the selected content that are directly related to one of my goals.

So, that means I'm going to have to give images as a training set for generating images. I'm going to have to provide text from the Internet or user generated text to the model to generate that text. Having this goal in mind is essential, and once we have that objective, I can just tailor that process by ensuring that there's a proper content to support this training process. Now, once I have the content that I'm interested in, I first have to figure out, well, how much content am I going to need?

The quantity of it. Remember, these models can learn from large amounts of data, so the quality and quantity of the data is really important. We have to understand that it's a quantity and quality approach. It's not a tradeoff. So if I want to train an image generator, I have to gather a wide range of images in the desired style or genre, and then I have to do some type of preprocessing and cleaning the data. It should also be necessary to remove noise and just ensure consistency with the data.

Now when we have to get this data, we have to select an architecture, and there's all sorts of different architectures to choose from. So, there are various architectures such as the generative adversarial networks, the variational autoencoders, and there are even transformers. Each architecture has its strengths and limitations, so you have to make sure that there's some suitability with the architecture that you're using for our dataset. Now after that, I need to apply to the models.

So I have chosen a particular model architecture. I'd like to now generate some code and writing some code to create this neural network. I have to define the layers in the network itself and then finally I have to establish connections between the layers. Now there are all sorts of frameworks and libraries. One of the ones that I like to use is TensorFlow or PyTorch. They provide prebuilt components and resources, and they're built in Python, a very powerful general purpose programming language, and it's used a lot in AI and data projects such as these.

Now, once I have this model implemented and it's created using the necessary libraries of my programming language, I now have to move on to actually training the model. Now, training a generative AI just involves iteratively presenting the training data to the model and adjusting its parameters to minimize between the generated output and the desired output. That's really all that's happening.

I'm going to say that again, what we're trying to do here is provide one sample of data at a time to the model and have it make a prediction, and then we compare predictions. So I'm going to look at the prediction that the model gave me and what the desired output is, and I'm going to look at the difference between those two variables. And from that difference, I'm going to adjust the parameters in my model accordingly so that it can fit properly. That's all that's going on here. And we do this over and over again.

Provided a new entry of data, it tries to make a prediction. It might get it right, it might get it wrong. If it gets it wrong, we look at the difference between the correct answer and the wrong answer, and we make the necessary adjustments. And this happens millions of times in the training process. It requires a substantial computational resources and it can take a significant amount of time. In fact, depending on the complexity of the model and the size of the dataset, it could take weeks actually just running this to converge depending on what's happening and what your dataset looks like.

It's crucial to monitor the model's progress and adjust the training parameters such as the learning rate and batch size to achieve proper optimal results when training the model. Now, the assessment and optimization of this tool is the final step. So I've now created my model. I'd like to see how does it work on brand new dataset. So, this is where we get into the quality evaluation. We have to evaluate its performance to see how it's working.

So, assessing the quality of the generated content using appropriate metrics and you can compare the desired outputs. Once you've compared it with the expected output and you find that the results are not satisfactory, it might be necessary to optimize the model by adjusting the architecture or the training parameters. Or you might have to add or remove certain entries in your dataset that you're using to train. And then the final thing I want to discuss here is how we tweak and iterate the model.

So, once we're done training, we have to understand that the whole training process is completely iterative. We have to evaluate the initial results, identify the areas for the improvement, and then try and find ways to improve. Now, fine-tuning the model by incorporating things like user feedback or introducing new training data or refining the training process can all lead to better outcomes. So, we want to draw from those multiple sources. This continuous iteration and improvement are key to developing a high-quality generative AI model. So, these are the processes and steps that we have to go at a very 1000-foot view to train our model from step zero to step one or two, to final step of our training process.

8. Video: Strengths and Limitations of Generative Models (it_gcdgaidj_01_enus_08)

Upon completion of this video, you will be able to provide a critical assessment of the strengths and weaknesses of generative models for various applications.
provide a critical assessment of the strengths and weaknesses of generative models for various applications
[Video description begins] Topic title: Strengths and Limitations of Generative Models. Presented by: Elias Zoghaib. [Video description ends]
In this video, I'm going to go over some of the strengths and limitations of generative models. There are some critical assessments of strengths and weaknesses. Generative models for various applications is the main topic for this topic for this video. So what are the benefits of generative AI? So there are certain strengths to these and one of them is that there's an automated content writing.

Obviously, we've seen that this generative AI can be applied extensively across many areas of businesses and science even. It can make it easier to interpret and understand existing content and automatically create new content. And one of the things that it can be used for is to generate text. You can ask it to generate text on anything, an e-mail, short story, a paragraph, whatever you'd like. It's able to create that content. But it's not only just able to make content, it's able to mimic humans.

That's the incredible thing about these generative AI is that I can ask it to write content and you would not be able to tell if it was written by an artificial intelligence or by a person. And you can set up automated e-mail responses. So as people are asking questions through an e-mail, it can respond to that e-mail and give the answer to their query via e-mail. So again, it feels like you're talking to a person when in reality you're talking to an AI. There's also technical query responses that are great for this.

And so, what I'd like to do when I use generative AI tools is that it's very good at providing content based on documentation because it's already consumed all documentation on the tools that we use. So I can ask it particular questions and have it spit out an answer to my question on a tool, one that I'd like to use. If I have a database question or a PostgreSQL question or anything like that, asking it for that kind of information and it'll tell me how to write a query or what's happening underneath the hood of an RDBMS or whatever question you'd like.

There's some technical query responses, and that saves me time because usually I would have to go into the documentation for my tool and do some searching and reading on my own, which takes up some time. Then there's information summarization, so I can give it a large amount of text and ask it to summarize it in just a few short sentences, and it will be able to do that. That's the incredible thing. It seems to be able to pass some type of cognition and able to tell you what the text means and explain it to you.

And if you don't understand the explanation, you can ask it to slow down and it's able to continue on that way. And of course, you can create stylized content. As we've seen before, there are tools that can generate video or images or even music, some type of content that you'd like to use. You can go ahead and create that kind of information. Now just as there are strengths, there are weaknesses to these tools. One of them is that it's very difficult to identify the sources.

It's the idea of what I mean by this is that when I'm training a dataset or I have a dataset that's very, very large, say it's all the web pages and I'm trying to train a large language model or some type of a generative adversarial network, and I want to use those web pages and start giving it to the GANs. Now what'll happen is once I start asking it questions, I'd like to know where it got, it's going to give me a response to my question and that'll have some information in the response.

There's no way for us to tell or it's very difficult for us to go back and say, OK, well, this information that it's giving me today is coming from this web page from our training set. It's hard to know where it's getting that info. So, there are some difficult processes to identify what the source of the response is. Now there's a limited ability to identify inaccuracies as well. So, that's something to keep in mind. It's very difficult to identify any inaccuracies within the ChatGPT or any large language model or generative adversarial network response.

And really because the complexity of the model of how much parameters there are. And again, it's very difficult to tune in very complex state. So really, you can only do the tuning in really simple generative adversarial networks. So, that's something to keep in mind. And the reason why it becomes difficult is the more parameters you have, the less control you have on what's happening within the model. It becomes almost like a black box.

It's very hard to trace and understand. OK, if I tune this particular aspect of the GAN, it'll have this effect on its accuracy. It's almost like at that point you have to run experiments to identify what you should do moving forward to increase a particular parameter, such as accuracy. And then there's poor bias assessment. So that's something that's important if based on the training set, if there's some type of bias within your training set, your generative AI will also see that bias in production, and there's improper representation of prejudice and bigotry.

So, this is unfortunate. It's kind of a byproduct. Let's say that you are dealing with the Internet and you're trying to train web pages, as I from my earlier examples. If you're training on the entirety of the Internet or all web pages that you could find, chances are you're going to find some improper prejudiced websites on particular races and religions and ethnic minorities and things like that. And so, that can go towards you essentially are training it on hatred from the Internet.

I'm sure there's plenty of that. So it's something to think about. Remember garbage in is equal to garbage out with these machines, so understanding that what you feed into it to train is going to be what its output is, almost like a child. So in the generative AI concerns, there are a few things that we should look into when we are identifying things like trustworthiness. So I need to be able to trust the results or if there's a potential misuse and abuse or potential to disrupt existing business models, right?

So I know that it's possible that it can provide inaccurate and misleading information, so it's not going to be bang on every single time, but it is more difficult to trust without knowing the source and where it got that information. Another thing is that it can promote new kinds of plagiarism around the beginning of the school year for us here now. And it's again a topic that is being brought up in all programs, and one of them is that it ignores the rights of content creators and artists of original content.

It's also bringing up some issues with your students could use these tools and there's no way for you to detect them. So it becomes difficult to identify who's using these models in a school setting. And then there's misrepresentation of photographs. It makes it easier to claim that real photographic evidence of a wrongdoing was just a generated fake. So if you have visual evidence, somebody doing something wrong or doing something heinous, the alleged perpetrator can just say that this was an AI-generated model or AI-generated image and there's a defense right there.

And because there's no way to detect if that image or video is fake or real, it becomes very difficult to sort of move forward. Now there are some more concerns that we should have with this. Again, because they are able to generate all sorts of information, it makes it easier to generate what's known as false news. And this means that again, it's generating anything.

And so, what that tells us is if it starts able to make things and people start posting it on the Internet, how would you be able to tell what's real news versus fake news? And that becomes a very difficult thing. Another issue is that it could impersonate people for more effective social engineering cyberattacks. So, it's a cyber warfare tool as well. And these are some of the concerns that they're bringing up as well with the ChatGPT model is that there's no way for you to tell that this was written by a model.

It seems so human. So it's what they're doing is some type of like social engineering attack through that where it's able to generate text and you just assume that you're speaking to a human when in fact you are talking to a machine. Another thing is that it could provide search corruption. And the last part that I want to discuss after search corruption here is that it might disrupt the existing business model built around the search engine optimization.

And this is what I mean by the search corruption. So let's say that Google had a major issue when ChatGPT came out and pre-released Bard for their search engine. And so, that created this big disruption in Google and it was a major problem because now they have to adapt to this new environment. So, this created some issues with business models that relied around search engine optimization and advertising. So, if you're in digital marketing, no one's using Google anymore, it's very hard that your ads are going to be expensive or use money.

So, that's just some information about that. One thing I wanted to bring up here is what does it mean to think. What I mean by that is that they seem to have that ability to think, right? They seem really believable. You know, the latest crop of generative AI apps, they just sound more coherent on the surface. But the key here is that the combination of human-like language and coherence is not synonymous with human intelligence. And this race of sentience may be just real or imagined.

It's kind of odd, but I understand that it would look sentient like you won't be able to tell if this is written by a human or that. And there's currently a great debate about whether generative AI models can be trained to have this kind of reasoning ability. But remember what's happening previous video before, if you'd like to look at that, it was that there was math that was written that explains what these generative adversarial networks are and the levels of what they're actually doing step by step. So, this is a debate that's happening within the field is within the field is like, what does this actually mean?

Becomes more of a philosophical question. And so we've got, you know, people who are Google engineers who've even been fired after publicly declaring that the company's generative AI app, LaMDA, was sentient, which is that requires extraordinary evidence for such an extraordinary claim. But no, they just seem awfully believable. And the reason why is because they're just learning from the text, and so they're just very good at mimicking us, but they're not good at, like, generating brand new things on their own.

So, the last thing I want to bring up here is that it there are some new risks that are brought up into this AI-generated world and this is the ability to detect AI-generated content. So, because this stuff looks so real, it's so hard to detect this content and it makes it more difficult to detect when things are wrong and there's this lack of transparency. These generative AIs are almost like black boxes. We don't really know how they're taking the input and generating the output at that point because it's so complex within the network.

And this is what I mean by it's not transparent, so it's hard to determine if they infringe on copyrights or if there's a problem with the original sources from which they draw results. You don't really know what's going on underneath the hood of this machine, and disproving things will become even more complicated because there's so much fake news or fake information out there already, and that's just generated by humans. Now what we have is we have a machine that can just generate content on a whim about anything in seconds.

So now it just starts generating content and it starts being posted on the Internet. It starts being picked by news media, it starts being spread around everywhere. And now, how would you be able to tell? Now you've got so much fake news or unreal news, you would have no idea how to tell the truth from what's AI-generated and what's not true at all. So this is the idea that the question comes, if you don't know how the AI came to a conclusion, you cannot reason about why it might be wrong and that this is the idea of what this risk on generative AI is.

And so this is just some of the strengths and weaknesses of generative AI and what it actually means. So some of the models there about generative AI and we've discussed some of the abilities to think about them, what it means to actually have some type of sentient when we're also looking at generative AI weaknesses as well.

9. Video: Generative AI: Prospects and Challenges (it_gcdgaidj_01_enus_09)

After completing this video, you will be able to evaluate the potential benefits and hurdles of using generative AI across various domains.
evaluate the potential benefits and hurdles of using generative AI across various domains
[Video description begins] Topic title: Generative AI: Prospects and Challenges. Presented by: Elias Zoghaib. [Video description ends]

In this video, I'm going to go over some of the potential benefits and hurdles of using generative AI across various domains. Let's first begin by understanding that there are some opportunities and challenges just like anything when we adopt generative AI. And I want us to talk about the healthcare and life sciences to begin with.
So if we look at healthcare specific large language models, what we're talking about is opening up the possibility for a lot of new use cases for various businesses to automate and enhance this manual processes. And you can help people be more productive, like your employees become more productive and provide proper customer service across all sorts of wide domains. Now when we're talking about things like healthcare, we're talking about things like being able to apply these things on detecting tumors or detecting some type of from a file of a patient, are we able to detect particular diseases.

Now all of this is able to help this kind of productivity for healthcare workers, but it can also help you with any employee in any field. So if there are models that are trained in the healthcare domain to provide accurate answers to medical questions, well, now you don't actually have to have a person come in and answer those medical questions. You can actually have the machine there to do it. And so there are several companies that are developing these healthcare and medical domain specific models.

And all of this is just going to help productivity for the workers and the employees in this industry and it's going to help with customer service as well. It's going to be able to answer any question on a product. So that's something that is great about these generative AI. Now there are some prospects and challenges as well. For example, there are some societal impact. For example, this AI has permeated our lives incrementally. So it started out with, you know, things like our smartphones to autonomous driving.

And there are tools that are using being able to write text and generate images out of whim. So its progress has been just incredible. But there are some societal impacts that comes with the challenges of generative AI. And one of it is the fact that, well, we are now pushing for information that could be generated by AI and you have no idea if it's real or not. You can just start asking it to do things and posting it online.

And then no one knows if that's generated by an AI or generated by a human being. And so this requires, again, there's a major business impact as well because, well, let's say that you are a tech company and now your developers are using these tools such as GitHub Copilot to help them write code. They're able to make things quickly, efficiently. So that's actually has a incredible business impact for a tech company because now they're able to generate and features much quickly or be able to identify bugs easier than they normally could. And so that's the idea of these generative AIs.

Now what's again interesting is that there's going to be AI impact on business and society, but there's no context to help anyone make sense of what this means. For example, like since the generative AI just came out. The speed at which generative AI technology is developing, it's not making things any easier, right? So if we look at ChatGPT, it was released in November of 2022. Four months later, OpenAI released a new model called ChatGPT-4 and it has improved capabilities and that's just a few months later.

And so it's really just keeping track of the innovations is becoming even difficult to do. You know, what we're also seeing is that there's you can take a 100,000 tokens of text generated in about 75,000 words in a minute, the length of an average novel. You know, it's unbelievable what these things are able to accomplish. And even now Google is getting in the mix because in May of 2023, it announced several new features powered by its generative AI, including what's known as the new large language model called PaLM 2.

And that's being able to interface a generative AI with its search engine, and it's going to power its Bard chatbot. You don't have to Google anything anymore. Technically, you just talk to this chatbot and say I'm looking for pictures of cats or understanding databases or whatever you'd like, and it's able to understand where you're what you're trying to do and provide the necessary information for you and provide sources. So really what I'm thinking about this kind of stuff is keeping track of the innovations is difficult.

And even just understanding of the breakthroughs that have enabled the rise of generative AI is important. I mean, this is decades in the making. We've had the building blocks to these things for a very long time, but the computational power of our machines wasn't enough to sort of implement back when we had these models. Now we're starting to see things start to rise. You know, there are going to be some foundational models in that practical applications.

And so what I want to also bring up is that there's going to be some novel functionality coming out of these tools as well. You know, some of the practical applications could include things like images, video, and audio, and computer code that could be generated and AI trained on these models can perform several functions. And it comes with this novel functionality that I can classify, edit, summarize, answer questions. I can even draft new contents like it becomes very powerful very quickly. And I'm just if we're depending on the large language model, it's very easy to use.

It's no longer an API thing that we'd have to send back and forth. You can just hop on a website and just talk to a chatbot and its able to spit out information. So I want to bring up more about the healthcare industry because of how prevalent it could become and this is just a way that it can make our lives much better. So again within the healthcare specific large language models that I was discussing earlier, you know generative AI opens up the possibility for a lot of new use cases for various businesses to automate, augment, and enhance manual processes and it can help improve customer experiences as well as employee productivity.

But the several companies that we're looking at now, it comes with this great decision-making features and they are targeting specifically at the healthcare and medical domains models. So there are some models called the Med-PaLM which is from Google, there's BioGPT Clinical BERT. So there's all sorts of things. Another one is called GatorTron, that's from the University of Florida NVIDIA, the graphics card company, the GPU company.

And these models are all trained in the healthcare domain to provide accurate questions to medical questions. So what I don't want you to think is that these things aren't necessarily ready to replace human beings yet. That's not really what it's trying to do. It's not trying to replace your doctor. What it's trying to do is its aid in the decision-making for a human being to make on what to do with the healthcare situation. That's the idea.

This is going to increase the patient care because now they are relying on a tool to help them make these decisions that are able to quickly acquire the information they need to make those decisions and that's going to lead to patient care. It's going to have more patients for them to make them more efficient. So now you can actually give a doctor more patients and have them understand what's happening. And it's going to also aid in research, you know, improving the patient care and the experience clinical decision-making. You can also speed up medical research and enhance operational efficiency.

And the final thing that I want to discuss is that there's the healthcare professional workload. I can reduce that healthcare professional workload because now these machines are there to help them get the answers a little more efficiently. There's now could be a front-facing chatbot that could answer some basic medical questions that before that would take a human being to do. Now a machine is able to replace a piece of their work. So one of the immediate near opportunities lies in the potential to automate this documentation and reporting work done by physicians and nurses in both clinical and research scenarios.

So that right there is how we can reduce their workload just by having a tool to help them do those kind of reports. Within the healthcare industry, again, there are some more things that we can have with generative AI, some more positives. This is diagnosis and treatment. So when I use a generative AI, it can assists doctors and medical professionals make accurate and informed diagnosis and suggest treatment options.

And it can do that by analyzing data from a patient's medical records, lab results, previous treatments, even medical imaging since they can understand images. So if you come in with an X-ray, you can feed it the X-ray and it will know what's going on. It can also help in a patient education by addressing queries related to disease and treatment so it's able to understand the patient data. So there's a medical note taking procedure for this. And generative AI can help generate synthetic patient level medical data that is realistic to train machine learning models without the risk of exposing private information about actual patients.

Generative AI can also streamline and automate the medical note taking process and capture key facts from patient conversations and summarizing them as physicians' notes. They can be used to summarize and create clinical notes such as visit summaries, discharge notes, radiology reports or even pathology reports. And then there is the telemedicine and remote patient monitoring.

Now this is incredible, so if you have somebody in remote areas, not a lot of doctors around ChatGPT based virtual assistants can help patients with things like schedule appointments, receive treatment, manage their health information, can also be used to monitor patients remotely by analyzing data from things like wearable sensors and other monitoring devices. So you know, we all have these wearable devices around our wrists for whatever reasons, could be an Apple Watch, could be a Fitbit that's all tracking some type of data.

And so now you can have some type of medical-grade device that sits around your wrist, could be something monitoring your heart rate for example. And it's able to now send that information to a model and provide real-time insights into a patient's health status to healthcare providers. So it's very easy to monitor someone with these things. And since generative AI is very good at consuming data, this would be an application for that. Now as I discussed earlier, scheduling becomes a very easy problem to solve. If we have some type of ChatGPT virtual assistant kind of tool, you can help patients schedule appointments, receive treatment, and manage their health information.

And the other thing that it can do is that it can design clinical trials and author protocol documents. So if you're trying to create a clinical study reports by generating summaries of clinical trials, including the study design, patient characteristics, all sorts of things that have to be there, it can significantly reduce the time and effort required to compile that information manually. So now they can just consume that data and generate it for you. And then there's the pharmaceutical development. This is the most interesting one to me.

So the use of Gen AI can accelerate this drug discovery and development process and it can search through things like the medical and scientific literature on websites and then provide some type of suggestion of what the next drug could be or potential drug candidates can be identified and tested for effectiveness. They do things like that all the time because they have computer simulations to try the drug out in certain situations before proceeding to the clinical trials on animals and humans. So that will do a lot of help there for finding new drugs and because that is what's known as drug discovery in chemistry and pharmacy. And so that is going to reduce the time it takes to generate new treatments and new drugs for particular diseases. So that is how generative AI is working with the healthcare industry and those are just some of the applications that are there within that field.

10. Video: Synthetic Data with Generative AI (it_gcdgaidj_01_enus_10)

Upon completion of this video, you will be able to describe how to use generative models to generate synthetic data for training and testing purposes.
describe how to use generative models to generate synthetic data for training and testing purposes
[Video description begins] Topic title: Synthetic Data with Generative AI. Presented by: Elias Zoghaib. [Video description ends]

In this video, I'm going to demonstrate how to use generative models to generate synthetic data for training and test purposes. Let's first understand what it means for synthetic data generation. What does that actually do? Well, synthetic data generation just means that we're going to be taking some training data, identifying some correlations within that training data and whatever data structures that are inside of our training data.
And then we're going to use that to look at statistical qualities within our data. Once we have that from our training data and we've got a statistical pattern and correlation identified, we're going to generate new data by using this generative adversarial network to provide a data structure of data that is similar to what we see in our training data. And what this just means is that we're going to be creating a new data point within this statistical distribution that we identified within our training dataset.

That's all synthetic data generation is. Training data, it'll identify some patterns with inside of it and then spit out another data point with a similar data structure. Now once trained, these statistical and structurally identical to the original training data will be generated, right. We're going to create new data points that is going to be look a lot like our training data, very similar to it statistically and structurally.

But the data points are synthetic in that they will look real, but they are AI-generated and completely artificial. That's the idea of the synthetic term in synthetic data generation. Now there are some problems that we have to look out for and one of them is the overfitting prevention. So, it's extremely important to prevent an algorithm from overfitting to the original data. What this means is the AI could potentially learn too well from our training data and accidentally generate the exact original data point that was in our training data.

For example, if I have images and I have an image of a cat in my training data, the output of my machine, my generative adversarial network, will show that there is some image that's generated that's exactly like that image of the cat that was in my training data. And this is where we have to have that data check. That's imperative to see OK, what exactly is happening with this output of my synthetic data generator and is it overfitting? Is it underfitting?

How is it passable for something that's within that training data sample that I gave it? And this is why you have to be able to check and test your generative adversarial network models. Now, there are different types of synthetic data, and I'm going to talk about two of them. So, so far I've been talking about AI-generated synthetic data. But there's also mock data, and they are similar but different and how they're generated and what the precursors are like, what the prerequisites are for actually generating them, and I'll talk about that here in just a second.

If we look at AI-generated synthetic data, we know that this is sample-based. In other words, I have to provide a training data sample to my generative adversarial network. I had to identify some statistical pattern and correlations within that dataset for me to actually generate another data point that could be in the sample. But with mock data generators, it doesn't use any sample. I can skip that training data portion completely, now that they don't require any data samples, and the resulting data is then completely fake.

There's no statistical intelligence or inference that's contained within that model. It just generates new data. It's not going to have any type of resemblance to anything real that was generated by a person. Another thing that I want to bring up here is the synthetic data generation. So, another important difference is between structured and unstructured synthetic data. So if we think about unstructured synthetic data, what we're really thinking about are synthetic images and video.

So, structured synthetic data, it could be considered tabular data where data points and the relationships are the most important properties, right? So, if you think about tabular data, that could be things like financial transaction records, CRM databases, all sorts of things that actually have that kind of structure to them where we're interested in the points between them. But unstructured data are things like images and videos, where you could have all sorts of things inside of your image, color, different resolutions, all sorts of stuff.

That's the inherent things that makes them unstructured. Now, most of the types of data that describe human behavior in a chronological way is referred to as time series data. And again, that's considered some type of structured data as well. And those are just some of the main differences between the AI-generated synthetic data and mock data, and they're patterns that we'd have to follow in order for us to generate synthetic data.

11. Video: Generating Synthetic Data with Generative AI (it_gcdgaidj_01_enus_11)

In this video, you will learn how to use generative models to generate synthetic data for training and testing purposes.
use generative models to generate synthetic data for training and testing purposes
[Video description begins] Topic title: Generating Synthetic Data with Generative AI. Presented by: Elias Zoghaib. [Video description ends]
In this video, I'm going to walk you through on how to generate mock data using a package called DataSynthesizer. Now, since DataSynthesizer is not using a system like a generative adversarial network, in the interest of time, I can't generate a generative adversarial network training model. That would take too long to do. And so, what we're going to be relying on is a statistical package that's called DataSynthesizer that will attempt to

[Video description begins] A Jupyter notebook titled 'DataSynthesizer__random_mode' appears. It displays a menu bar at the top with options like File, Edit, View, Insert, Cell, Kernel, Widgets, and so on. The right corner displays the label Python 3 (ipykernel). Below, a toolbar appears followed by several code cells. [Video description ends]

generate image based on dataset that we have collected here. And inside of the dataset is a file that contains Social Security numbers in CSV, or mock Social Security numbers for people in the United States. And what we're going to do is we're just going to walk you through the steps of using this package.

So, the very first step that you want to do is use pip install datasynthesizer with Python to install the necessary packages to start synthesizing data. And then what we're going to do is start importing some of the packages. So, from DataSynthesizer.DataDescriber and a DataGenerator, we're going to import these lines of code. So, it says from DataSynthesizer.ModelInspector import ModelInspector, from DataSynthesizer.lib.utils import read_json_file, and display_bayesian_network import pandas as pd is the other one.

[Video description begins] There is a space before code cell 1. It displays the heading: Step 1 import packages. Line 1 of code cell 1 reads: from DataSynthesizer.DataDescriber import DataDescriber. Line 2 reads: from DataSynthesizer.DataGenerator import DataGenerator. Line 3 reads: from DataSynthesizer.ModelInspector import ModelInspector. Line 4 reads: from DataSynthesizer.lib.utils import read_json_file, display_bayesian_network. Line 6 reads: import pandas as pd. [Video description ends]

So, that's our first step. These are all the packages that we're going to do.

Now, the way DataSynthesizer will work is it'll take a dataset that we currently have, just the SSN, and it's going to then find some statistical patterns for those Social Security numbers and then generate new ones for us based on that pattern that it obtained. So, similar to a generative adversarial network, it's more primitive version. We're using what's known as a Bayesian network rather than a actual generative adversarial network, but the system it still stands as synthetic data. So, I will create the input dataset like so and just import the necessary packages. And if we continue this way, we'll do give it a moment here to actually import everything. You'll see that everything should continue. Now, I'm just creating some variables for the input data.

This is the relative path to where my input file is. I'm going to be using what's known as random_mode, so I'm interested in just randomly generating data that's based on any type based on this information that we have in the CSV file that contains the Social Security numbers. We're also going to need an output for the synthetic data. So, the synthetic data is going to be in CSV, and the description file here, which will contain different records, different information about the JSON file. It's going to be in a JSON format and I'll show you what goes inside the description file.

[Video description begins] The space before code cell 3 displays the heading: Step 2 user-defined parameters. Line 1 of code cell 3 reads: # input dataset. Line 2 reads: input_data = './data/adult_ssn.csv'. Line 3 reads: # location of two output files. Line 4 reads: mode = 'random_mode'. Line 5 reads: description_file = f'./out/{mode}/description.json'. Line 6 reads: synthetic_data = f'./out/{mode}/synthetic_data.csv'. [Video description ends]

It's just certain features such as the number of synthetic_data points that have been generated and all sorts of metadata that are associated with the synthetic_data. Now, this is known as the threshold value.

What I've written here is An attribute is categorical if its domain size is less than this threshold, and so we want to modify this threshold to adapt the domain size of the "education", which is 14 in the input dataset that we have. So, I'm having the threshold_value is set to 20. And then so this allows it to generate continuous data and now this is the Number of tuples generated in nice synthetic dataset. Now I chose this number because it's the same as the input dataset, but you can choose to be another number. So, I'm going to generate 32561 synthetic data points.

[Video description begins] Line 3 of code cell 4 reads: threshold_value = 20. Line 6 reads: num_tuples_to_generate = 32561 # Here 32561 is the same as input dataset, but it can be set to another number. [Video description ends]

And if we continue down here, you'll see that there's the DataDescriber where we should Instantiate a DataDescriber, Compute the statistics of the dataset, Save the dataset description to a file on a local machine. That's the three steps that we're going to do.

[Video description begins] The space above code cell 5 displays the heading: Step 3 DataDescriber. Below, 3 points are enumerated: Instantiate a DataDescriber, Compute the statistics of the dataset, and Save dataset description to a file on local machine. [Video description ends]

So, here you'll see me create the describer.

This is where the category_threshold is equal to 20 like I have set above. I have the describe_dataset_in_random_mode, which is the input_data that I'm interested in analyzing. And then I want the describer to save the data description to a file, which is the description_file, which is the JSON file that I've described earlier. So, now I will go ahead and just run that code.

[Video description begins] Line 1 of code cell 5 reads: describer = DataDescriber(category_threshold=threshold_value). Line 2 reads: describer.describe_dataset_in_random_mode(input_data). Line 3 reads: describer.save_dataset_description_to_file(description_file). [Video description ends]

OK, and the next step is now to compute the statistics of the dataset, which is what's been done in cell 5 and I've saved that. Now I'm going to instantiate a DataGenerator. Now, this is what's going to be using the DataDescriber to generate the synthetic dataset and then I'm going to save it to my local machine.

[Video description begins] The space above code cell 5 displays the heading: Step 4 generate synthetic dataset. Below, 3 points are enumerated: Instantiate a DataGenerator, Generate a synthetic dataset, and Save it to local machine. [Video description ends]

So, those are the three steps that I'm going to be following. So, first is how you Instantiate a DataGenerator.

It's just a DataGenerator in camel case with an open closed parentheses. This creates the DataGenerator class model. And then we're going to generate the dataset in random_mode with the number of tuples to generate with the following description_file. And we're going to save that synthetic data into a synthetic data file, so and that's what we're going to have. So, the synthetic_data that we have stored here is going to be generated. And if we do this, you'll see that that is now stored and now we've got our

[Video description begins] Line 1 of code cell 6 reads: generator = DataGenerator(). Line 2 reads: generator.generate_dataset_in_random_mode(num_tuples_to_generate, description_file). Line 3 reads: generator.save_synthetic_data(synthetic_data). [Video description ends]

final thing to do is compare the statistics of the input and the synthetic data. Now I just wanted to show you this. This really is just more about using pandas in Matplotlib.

[Video description begins] The space above code cell 7 displays a header: Step 5 compare the statistics of input and synthetic data (optional). Below, a sub-header appears: 5.1 Instantiate a ModelInspector. Line 1 of code cell 7 reads: # Read both datasets using Pandas. Line 2 reads: input_df = pd.read_csv(input_data, skipinitialspace=True). Line 3 reads: synthetic_df = pd.read_csv(synthetic_data). Line 4 reads: # Read attribute description from the dataset description file. Line 5 reads: attribute_description = read_json_file(description_file)[' attribute_description']. Line 7 reads: inspector = ModelInspector(input_df, synthetic_df, attribute_description). [Video description ends]

So, I'm not really going to go over the code on how this is done, but I just want to show you the graph on how this will look like if we compare histograms between the input and the synthetic datasets. [Video description begins] The space above code cell 8 displays the sub-header: 5.2 compare histograms between input and synthetic datasets. Code cell 8 displays 2 lines of code. Line 1 reads: for attribute in synthetic_df.columns:. Line 2 reads: inspector.compare_histograms(attribute). The output displays several histograms. [Video description ends]

So, on the left, here you're seeing the input, and on the right, here you're seeing the synthetic. And this is as you can see, we're trying to show you how the data will vary. Now, it's not since it's so random, it's not really bang on. For example, we've got Divorced and different types of classes. As you can see, it's kind of keeping everything as uniform as possible since it is random and that's exactly what we kind of expect with this kind of synthetic data. Here are the different classes. So, in our Social Security data, we've got that there was a lot of, 40% were Husband, 25% were Not in a family, it could be an other-relative, or an own-child, a certain 10% were Unmarried. And that's kind of what's happening here as well.

You've got your Own-child, your Wife, you're Unmarried, they're all equal with each other and we've got the Female, Male. As you can see, there's in my actual dataset, there's more Male than there are Females. And in my synthetic dataset, because we're doing random, it's following some type of distribution, and we're seeing this male versus female. So, this is exactly kind of how synthetic data is going to be generated. You can tell that when we're using random data, it's following some type of distribution. It's trying to keep things uniform as possible, and that's usually how synthetic data is generated. Now, typically the only difference is in my example is I'm using a very primitive statistical engine to make this synthetic data.

When we're using a actual GAN is when we'd use the Keras or TensorFlow library to actually train the dataset on a particular model that we've created. There's some type of generative adversarial network that's actually doing the AI. Here we're using a very primitive machine learning tool. As you can see, the situation is the same. We started with the training set, we trained the model on that dataset, and then we asked it to generate a few different outputs based on that training set that we provided. So, the model complexity will change, but the actual flow of the tool will stay the same and this is how DataSynthesizer is used in random_mode to generate synthetic data.

12. Video: Ethical Considerations in Generative AI (it_gcdgaidj_01_enus_12)

After completing this video, you will be able to outline the potential biases, ethical considerations, and societal impact when designing and using generative AI.
outline the potential biases, ethical considerations, and societal impact when designing and using generative AI
[Video description begins] Topic title: Ethical Considerations in Generative AI. Presented by: Elias Zoghaib. [Video description ends]
In this video, I'm going to discuss the potential biases, ethical considerations, and societal impact when designing and using generative AI. Now, there are some ethical considerations we should focus on when it comes to generative AI, and they are a complex and continuing problem with today's world. There are modern issues that could also exasperate the problems.

There are fantastic pieces of technology, incredibly promising, but they can open a can of worms when we're talking about regarding accuracy, trustworthiness, bias, hallucination, and plagiarism. And these are some of the ethical issues that'll take just a few years just to sort out. And these aren't particularly new to AI. Like I said, these were all issues that we see in modern worlds. I'm sure like there's fake articles or improper journalism and all sorts of things that are on the Internet, and the AI is just going to exasperate that problem.

And just to give you an example of something like this happening is that Microsoft's first intro into chatbots in about 2016 was through a program called Tay, and it had to be turned off after it started spewing inflammatory rhetoric on Twitter, just as a brief example of improper generative AI. Now, generative AI ethics are important for a number of reasons, but you know, just with any emerging technologies it's really easy to unintentionally use this technology in a harmful way.

It's important that we understand what the ethical and legal considerations are. In fact, the legal considerations are just starting to catch up. So we have the user data protection, right? Creating an ethical frameworks with these parameters that I'm about to discuss here how we can use generative AI to help organize and the user data is a very great example. We want to be able to protect customers and their personal data. So if you're using these chatbots and you're using some type of information, we obviously don't want your information to be used in somebody else's query in the generative adversarial network, that kind of thing.

So we also want to be able to organize and keep proprietary and corporate data protected. Obviously, if we're going to be using these types of tools, we don't want it to actually infringe on copyright laws or using someone else's technology that is patented, for example. We also want to be able to protect creators and their ownerships and the rights for their work. This is a big controversy when it comes to art and music is that now we're using these image generators, for example, drawing and creating new images based on images that are already created by a person, by a content creator.

So it's almost as if I were to do that myself, that would be considered plagiarism, and yet it's a machine doing that. So we have to be able to protect that kind of thing from happening. And then there's the environmental protection. These tools are great for this. We obviously want to be able to make sure that there's some type of minimize the environmental impact that these generative AI tools can have. And then there's the preventing the spread of false information. This is a big one.

There's already false information now. Before generative AI was out on the Internet, right? And the concept of fake news is such a big thing. And now that these models are coming out and starting to generate text and images and videos and all sorts of things like that, we have no way for us to tell what's the truth and what's just noise that's generated by these AIs. So, preventing these dangerous biases and falsehoods from being proliferated is a major concern for us.

Now let's say that we want to be able to use these things ethically. Well, what should we discuss? Well, particularly we want to be able to train employees on the appropriate use of our artificial intelligence. So if you've got employees and they're allowed to use generative AI in their daily work, you have to train them on what it does and doesn't count as appropriate use of this technology. And usually this comes with the appropriate use of data, not being able to provide data for someone else.

And this will be especially important if your organization is subject to some type of regional or industry-specific regulations such as HIPAA and all sorts of systems like that to keep data protected. Now, those are just a couple of the ideas to sort of make generative AI ethical. And there's a few of these things that we want to pay attention to the development. When we're talking about making AI safer, we really need to start at the development stage.

What are the steps that can be taken to enable safer AI-enabled products? Those are some questions that we should be able to answer, and one of them is to change the way the development cycle works. So a lot of the times, the technology and the society will both clash and that can be avoided in the design phase. But it's another way to encourage safe AI usage is to change the narrative. We want to be able to promote safety and honor around these data sets that we're using to train our AI models.

Is it ethical for us to use this kind of data for us to use things like Google and some of the tools that they're coming out with Bard and ChatGPT to promote higher education around this tool and make sure that it's safe and that we're not using it for nefarious methods? And there's going to be a few different things when it comes to data security and management. So if your team wants to use generative AI to get more insights from sensitive corporate or consumer data, well, now we have to be very careful because now we're could be giving away sensitive information such as passwords or keys to databases or something like that to these models.

That's a very dangerous slippery slope. So there's going to be some certain data security and data management steps that should be taken to protect any data that's used as inputs in a generative AI. So one of the things is, you know, data encryption, you could have things like data anonymization. So we're removing sensitive information like usernames and emails and things like that and just kind of giving it some dummy data just to see what it'll do. And then there's things like the digital twins.

Now what this means is I am kind of taking data and I'm muddying it up a little bit and then providing it for the model just so that it doesn't have any type of sensitive information. And these are all techniques that can be helpful methods for protecting your data while still getting the most out of the generative AI tools. We're just trying to be a little safer with providing it information. Now, the idea then comes to fact-checking, right? So, now we've got a generative AI response. Say you went to Bard or you went to ChatGPT or some other chatbot.

Well, it seems like they're thinking and they're generating truth-based answers, but what they're trained to do is produce the most logical sequence of content. Like they're just very advanced parrots is a way like they kind of just, they've been trained on the text. They know what order of words humans speak in and they know what to do. And so they have to be very careful. They can give general, accurate, and helpful responses through this training, but they can also produce false information that sounds true. So you first need to educate that your team on the difference.

Make sure that every member of your team is aware of the shortcoming of generative AI, everything that comes out of it, it's not the truth. So, your staff should be not solely rely on the tools for the research needs, it's just kind of a way for us to get started into looking for things. And this is where we have use information-checking tools on your own and there's online industry-specific resources that could use to fact-check all the responses that you receive from a generative AI tool. So, that's useful as well.

And then important is to champion transparency. You know if generative AI is going to be part of your organization's internal workflow and make sure your clients are aware of it. It's best if your customers are aware of this tool, especially when it comes to their personal data and how it's used. Making that aware and providing the upside of them using your data on the generative AI is very important. There's a tradeoff, right? There's a bit of risk for security, but you are getting faster and more detailed results, and so this is going to make that information readily available.

If you explain that on your website or some tool for your customers to directly view how you're using generative AI to make your products and services better and then you just clearly state what steps you're taking to further protect their data in best interest and just let them know, like these are the tools that you're using. This is how you're going to be using them and this is how we're protecting you and just get ahead of the curve kind of things to sort of remove that anxiety for data and AI in this day and age. So, that's just a little bit on the transparency and the fact-checking steps for data security and management when using generative AI.

13. Video: Generative Model Evaluation (it_gcdgaidj_01_enus_13)

Upon completion of this video, you will be able to provide an overview of the appropriate metrics and methods to evaluate the performance and quality of generative models.
provide an overview of the appropriate metrics and methods to evaluate the performance and quality of generative models
[Video description begins] Topic title: Generative Model Evaluation. Presented by: Elias Zoghaib. [Video description ends]
In this video, I'm going to discuss the appropriate metrics and methods to evaluate the performance and the quality of generative models. So, when we're talking about any type of model, specifically any machine learning models, there's a few different things that we want to do. Mainly, we're either trying to classify our data so there's some classification machine learning algorithm. This is more of a supervised approach where we're trying to bin the data based on certain features and figure out what its label would be. There's also the unsupervised approach, which is the clustering of data. And this is where we're trying to figure out what data points are similar to each other and then clustering them on a graph and seeing, OK, there's two clusters here and they relate to these two labels, so that's a way for us to figure that out.

There's also another thing known as regression, and regression uses what's known either as linear or logistic regression. Linear regression just means that we're trying to predict a the value of a dependent variable based on the value of an independent variable. And logistic regression is we're trying to identify a binary class on a data point so it can either be one or zero. For example, what the classes could be, and based on the dependent variable certain features, we want to do some type of statistical calculation and figure out where it's likely to either be a zero or one. And all of these are dealing with probabilities, right? When we're talking about generative models, it's very different from those three models that I was just discussing before, because what we're doing here is we're learning the probability distribution of training data and then we're generating new data based on that probability distribution. And this makes it so that we can generate that similar samples to our training data. But it's not exactly the same as our training data. There's a little bit of noise.

It's figured out that there's some statistical test, there's some probability distribution that I can learn from, and it says, OK, I'm going to generate new data points within that probability distribution, and that's how the generative model works. So then how do we know that it's behaving appropriately? Well, there's visual inspection, and this is probably the easiest way for you to check. You can assess the performance of a model is just to look at the samples it's generated. Does it make sense? If I gave it a whole bunch of pictures of cats, is it generating pictures of cats? And so, this is really easy when we're dealing with images, but it's inappropriate to do this visual inspection for non-image or textual data. And so, what for textual data we want to actually do something a little more mathematical, and that's using the inception score. Now, what this means is I'm going to be looking at the output, measuring a particular difference between this and the statistical distribution, and identifying a probability of that output compared to the statistical distribution and coming up with a score.

How similar is this to my training data sample? If it's too similar, then maybe I've overfit. If it's not similar at all, then maybe there's something else going on. I need more training data for the parameters to fit appropriately. Now remember, inception scores don't apply for anything that isn't an image, because the inception score is just a model trained on images. That's the idea of what's happening here. So remember, these may correlate with human perception of image quality because it's a type of visual inspection, but it's just an automated visual inspection. If we're thinking about that, we want to specifically target generative adversarial network and this is what we're going to first talk about is the reconstruction error. Now, variational autoencoders we know are really good at taking data, taking it apart, putting it into a lower-dimensional space and then reversing the embedding to reconstruct the original data point.

Now, the error of the reconstruction tends to decrease during that training, but you can usually classify or obtain models logo reconstruction error. So, that's the idea. Remember, variational autoencoders take something apart and they put it back together on the other end. What we want to do in the reconstruction error is take a look at that reconstructive data point and compare it to the original data point. What is the difference between those two data points? The more difference there are, the larger the reconstruction error will be. Now what you'll notice is, as you're training the variational autoencoder, the more data points you feed it, the lower that reconstruction error will be. So at first, when it's starting to train and you've given it maybe the first ten data points in your training set, it's going to have a really high reconstruction error. By the time you get to the thousandth training data point, it's going to have a little less, and you keep giving that until the variational autoencoder is intelligent enough to be able to take your data point, put it back together, and have a very low reconstruction error. The next thing I want to give off is the log likelihood.

Now what this is, is we're estimating the log likelihood of the data using what's known as the parsing window estimates or the annealed importance sampling. These are just common approaches for quantifying the importance of what these generative models are, and they're basically statistical techniques, so they're statistical calculations. I'm not going to get into them here, but just know that they do exist. The log likelihood is just an appropriate metric for all models and data types, but it's usually really difficult to estimate. So, you have to calculate them appropriately. Now they can be very difficult to interpret in high dimensions, so think about what your generative model is actually spitting out. How many dimensions are there? Is this something simple or difficult, like what exactly is easy to understand? Now let's go ahead and move on to just in general, we're talking about task-based evaluations. What this means is I'd like to know how good is my generative model or in my model at all or any machine learning model. But specifically for generative models, I want to evaluate it in place as it would be used by the end-user or how it would be used in industry or the actual application for this model.

Does it perform well? Does it make any sense? That's the idea. The next thing is more of a turing-based test. So, this is basically a way for us to distinguish how well the model performs compared to a human. So if you go on ChatGPT for example, it's a popular large language model that's available. And you go ahead and you say, hey, I want to output an e-mail. If you were to send that e-mail to your friend, would they be able to tell that it was written by a machine or a human being? That's the idea that you want to check because a lot of the times people can't tell the difference, and it's becoming a major issue for this one, is because it's so good at replicating a human text that it's affecting educational institutions. So there's no way for them to figure out if a student is actually writing something on their own or using it with the help of a generative adversarial network or ChatGPT. And the next thing is truthfulness.

How true are the model's outputs, right? Does it fabricate or reproduce real-world biases? So this is important, like exactly what content is being generated by this? Does it make sense? Is it saying the wrong things? That's known as the veracity or the truthfulness test. Now, there's other things that we want to do. We want to compare it to a type of gold standard. So if I'm using this model for some industrial application, there's obviously going to be some way for metric that I would trust it, right? If I was using this in medicine, it would need to have a very high gold standard for it to hit. What is the accuracy score that they should have? That's what you want to do. You want to compare it to that gold standard. Then there's also grammatical validity. Now, this is kind of the complement to the truthfulness metric, where we're just looking at the content of the message, exactly what it's saying qualitatively, but the grammatical validity we're using a separate model to check for grammatical errors.

Is it using the English language properly, or any language for that matter? Does it make sense what it's saying? This is actually how a human would write. And then finally, once you have all of these in place, you want to use what's called a mark scheme. Just as your human examiner would mark an exam for you in school, right? You would use a some type of scheme or grading to keep track of your settings for when you were training your model and give it a grade. And then maybe change the settings and retrain your model and give it another grade. And so, what you'll end up seeing as you're changing parameters, you're experimenting with the different settings and the training for example, you might see that there is an optimal grade that you can provide with those settings that you've given it. And so, that's why you want to keep track of your grades for your generative models. So, that's just a little bit on how we can evaluate the performance of these generative adversarial networks.

14. Video: Real-world Applications of Generative AI (it_gcdgaidj_01_enus_14)

After completing this video, you will be able to outline the use cases and practical applications of generative AI across various industries.
outline the use cases and practical applications of generative AI across various industries
[Video description begins] Topic title: Real-world Applications of Generative AI. Presented by: Elias Zoghaib. [Video description ends]
In this video, we're going to explore the use cases and practical applications of generative AI across various industries. Let's first begin with understanding that generative AI come in a few different classes, right? So, there could be visual-based, which is the image generation or video generation in generative AI. There's text-based, which are the tools that are like your chatbots that are able to answer questions about particular topics. And there's code-based. Now, this is something like your GitHub Copilot is a great example of this. What this does is it's able to generate code, and this comes in a few different things like code completion. It's able to guess based on other people's code in GitHub, learning at different code bases.

It will help you write functions based on the information that it's trained with. And so, then there's the real-world applications as well that we could also discuss which is the audio-based because it can also generate voice, which is really interesting. You get things like the text-to-speech generator. In fact, GANs will allow the production of realistic speech audios. I'm sure if you can sit around and Google some machine learning algorithm or some generative AI applications that are able to take famous people's voices and start making them sound like they're saying things on the Internet, and so that is a application to that. There's also things like music generation as well. So, you can have generative AI that is great for music production. Music generation tools can be used to generate things like musical materials for advertisements or really other creative purposes.

There's also what's known as the speech-to-speech conversion and this is where an audio-related application to generative AI involves using voice generation and they use the existing voice sources. And then there's the STS conversion, voiceover can be easily and quickly created. And so, what we're doing here with speech-to-speech conversion speaking into a microphone and it's able to identify the information that you're seeing. And so, there's all sorts of many others as well that I can't get into here. But there's a lot of different applications to these generative AI, but these are the most common ones that I've seen. And so, what we want to do then is use these types of generative AI and apply them to different industries, right?

So, there's things like manufacturing where we can do things like predictive maintenance or quality control or we can have some type of production planning and inventory management, which are all excellent applications, right? Imagine now you've got some tool that can predict equipment failures and maintain their equipments proactively for aircrafts or trains or any other industrial equipment for your organization. There's also quality control. It can help improve quality and control processes in manufacturing. You can learn from images of products in the past and identifying those defective. And there's other things such as, again, production planning, and inventory management and these can simulate various production scenarios, predict demand, and optimize your inventory levels. And then there's education. So, with this, you can have personalized lesson. You can leverage generative AI.

Personalized lessons can provide students with the most effective and tailored education possible. And these plants are crafted by analyzing the student data such as their past performance, their skill set, and you can provide feedback they may given the curriculum content. So, what you're doing here, you're pretty much tailoring on a per student level, what their abilities are, and what they're proficient at and need help with. And so, this can actually help with receiving an individualized experience to maximize success for students. It's also used by educators as well for course design. There's also things for content creation for courses that you can start generating lesson plans or you can generate questions or practice questions to give to a student. Those are all some of the abilities that it can do for our education. Another thing that is popular is, of course, the healthcare.

Now, what this does, the most powerful thing that it's doing currently is that we're now streamlining what's known as drug discovery and development. So, what we're doing is we're experimenting with different types of drugs and different chemicals and we're trying to see how they perform under simulations that these drugs that we're testing under a simulation are generated from this generative AI application. And so, then we absolutely create this new drug it came up with. It wants to try based on the previous drugs that were working and other drugs that failed, maybe this is the right way for it to work. And so, you put it under a simulation, it seems to perform well and then you can go on to animal testing or clinical tests on humans. So, you could tell that something might be working as well, we can move on to personalized medicine. Now, this is where it gets interesting because generative AI algorithms can offer potential in the healthcare industry by crafting individualized treatment plan tailored specifically for your personal medical history.

No longer are we kind of doing the guess. We are looking at your symptoms individually, your medical history individually, and we are tailoring the correct drug or the correct treatment for you. And another thing that's really powerful is improved medical imaging. You can combine the power of machine learning with medical imaging technologies such as CT and MRI scans and it can then read those CT and MRI scans and identify any anomalies that are there. A few more things that we want to discuss obviously with banking, guess where that's going to come in play with your fraud detection. That's right, generative AI will provide banks with a powerful tool to detect suspicious or fraudulent transactions, and they can enhance the ability to combat financial crime. There's also using for risk management with insurance and with their ability to see if they can pay back the loan or not.

You can leverage these kind of AI to look at the value at risk estimations for the individual lender. Another thing that's really interesting to use is gaming. There are certain gaming applications. So, generative AI can generate game content such as levels, maps, and quests. Any type of information, any type of image that you need for a game, you can go ahead and generate that with these tools. Not only that, but if you have say like an online game or a game as a service, you can look at the analyzing the player behavior data and you can see what are some of the game play patterns and preferences so that you can tailor create sections of the game for those users. You can also use these for non-player character behavior.

Guess what if ChatGPT or some of these large language models are so good at generating text that we can't decipher whether they're human or they're generated by a machine, well, we can actually have a large language model inside of our game so that when we communicate with characters inside of the game, they can communicate back similar to the way a human would. And that's really interesting because now the game is actually reacting to your decisions and how you would interact with it. Another thing that is useful is travel. You can do things like identity verification with these, the utilization of generative AI and face identification, and verification systems at airports can aid in passenger identification and authentication and can overwhelm strength and security while traveling abroad. There are personalized travel and destination recommendations. In fact, there are services that are available now that are incorporating with ChatGPT where you can build an itinerary for your travel and just ask it questions of what you'd like to see.

One great exercise that you could do is just go to ChatGPT and say I'm planning a trip to Rome and have it build an itinerary for you and see what it comes up with. It's quite fascinating what it's capable of doing. Another thing that is really useful is in retail. So, when we're dealing with retail applications, there's all sorts of things that we need to do, such as product and display design. It can create new product designs based on the analysis of current market trends or consumer preferences and even historic sales data. You can have automated retail content generation, and this is for promotional content for social media blog posts. And you can use it to improve your SEO inside of your search engine optimization and it can drive customer engagement forward for you. Not only this, but it can give proper product recommendations based on your buying history.

Say if Amazon decides to incorporate with some of these generative AIs, which I'm sure they're pumping a lot of research into now, it wants to be able to based on your previous buying cycle, what you purchased before, what are the next things you can learn and this thing can do that, it's very powerful. What can it purchase? And what is your next purchase? And kind of give it to you at your front of the stores, so that you're more likely to purchase that. And then finally, there are some legal applications, in fact, lawyers are doing some type of generative AI for contract generation and they can use these to generate contracts based on predefined templates and criteria.

This is quite fascinating because contracts have been written and it can learn from those contracts and write them in a particular way so that it can save you time and effort for procurement departments and help ensure consistency and accuracy across any contract language. That's really incredible. And so, then there's things like contract compliance and the companies have thousands of contracts with various negotiated terms. So, if your generative AI applications or your large language models with language capabilities can understand and categorize your contracts or identify common terms or highlight unique or rare terms even. There's also insurance applications. Remember, when we're talking about policy documentation, these generative AI tools can help generate policy documents based on user-specific details. Another thing that I mentioned, just as similar with banks where we're trying to identify the risk of a lender or a loaner will be able to pay back that loan, the insurance will have to look at your risk assessment and your premium calculation.

So, it can use to simulate different risk scenarios based on your historical data such as any previous accidents, for example, if it's for car insurance and it can calculate the premium accordingly, and that's how we can use those things for insurance, legal, and retail. But it even goes further as well. It can even help your sales team with your sales applications. Generative AI can be used to provide personalized sales coaching to individual sales reps based on their performance data and learning style. There's also things like your marketing and marketing is such a powerful tool where we can have content creation for marketing such as online app. Because these content creation or text generation specifically, by using models like ChatGPT, you can actually generate these AI texts for YouTube and your Google ad if that's what you'd like to do and see how they perform.

So, content creation for content marketing in the forms of email, social media posts, or blog articles can all be generated with a generative AI approach. There's also another thing that we can do and that's what's search engine optimization. Now, this is where it gets really fascinating because we know what's search engine optimization, there are people who sit there and do some operations to make sure that when somebody googles specific terms, your search engine is going to pop up first and under the Google headlines. Well, now what we're going to do here is we can generate topic ideas for content writing and for SEO content writing by utilizing a particular language and that's one way it'll help.

You can produce relevant keywords and phrases or analyze competitors' contents to identify gaps in coverage that could help with your search engine optimization. You can even do a conducting keyword search and the process of including related keywords to a content is crucial for a successful SEO strategy. And it helps determine the terms and phrases the potential customers use when they're looking for products like yours or a company or an institution that service that you're offering. So, that's something that you can do and it does that by generating your keywords and identifying keyword trend. For example, if you're looking for ideas for business-to-business marketing content, you should look at some of these models that are available and have it try and generate some content for you and your use case. And that's just some of the settings here for search engine optimization that could help you with generative AI.

Then we get into human resources, in particular HR applications. You can use generative AI to generate onboarding materials. AI can be used to generate onboarding materials for new employees like training videos or handbooks or other documentation. You can have it write job descriptions generations. Generative AI can be used for creating job descriptions that accurately reflect the required skills and qualifications for that particular position. And there's things like customer service. And so, now it has a multilingual customer support and multilingual support offered by generative AI tools like ChatGPT for customer service involves using the large language model capabilities of the system provides support to customers who speak different languages.

So, these large language models are capable of speaking whatever language you'd like. And remember especially ChatGPT, they're trained with the entire content on the Internet. So, if the Internet is English, French, Chinese, whatever, it's able to understand your language and it has personalized customer responses. Conversational generative AI tools can not only answer your question, but they can answer your question with the context of things like your past purchases or your chat history or a previous customer feedback that you've given. There are some more issues that we can tackle with generative AI, including the supply chain management. We can do things like demand forecasting and supply chain management. It can help business predict demand for specific products and services to optimize the supply chain.

There's also things like auditing. We can have a manual process such as reporting, and it could be time-consuming and error-prone. But these generative models like ChatGPT, for example, or Bard, or whatever you'd like to use, they can help auditors automate repetitive tasks such as the paperwork and the reports, and it can eliminate human error. So, specifically, it can produce standardized reports and those will offer consistency and how findings are presented across all of the reports that are great. And so, those are just some of the industries that generative AI are going to be used for in the future and they're being used for currently now. And it's just an idea for you to see what can these tools do for my organization and my business. And those, I've just given you just a handful of examples and you should go ahead and give it a shot to see what it can do for you.

15. Video: The Impact of Generative AI on Society and Creativity (it_gcdgaidj_01_enus_15)

Upon completion of this video, you will be able to describe how generative AI has the potential to redefine creativity and the possible societal consequences.
describe how generative AI has the potential to redefine creativity and the possible societal consequences
[Video description begins] Topic title: The Impact of Generative AI on Society and Creativity. Presented by: Elias Zoghaib. [Video description ends]
In this video, I'm going to discuss how we can use generative AI to redefine recreativity and its possible societal consequences. So, generative AI and its impact on creativity. Well, as we know that it comes with art, it's revolutionizing creativity across industries, including things like art or data augmentation where we can just add new pieces to existing art or music. We can also help with things like manufacturing and society and we can also do things with medicine. And so, these are all just some of the possibilities to use generative AI for art and as well in different areas of industry. One other thing that I want to discuss is generative AI specifically on movies and video.

People are now generating data or images similar to CGI within some of these movies and home videos that they it be created. You can also use them to generate text or written words, things like novels or short stories that can all be generated with things like ChatGPT and other large language models. Music is another tool that's going to be revolutionized by this generative AI. So, music is an absolute application that can have statistical samples. There's a statistical distribution within music. If you actually give it a sample of music, will it be able to generate new music based on that sample? And it can. And also another thing that's going to be used for is fashion. They're going to be using generative AI in things like designing new clothes, which is going to be pretty incredible considering that if it can relate to some data points from other outfits that people have put together or some other pieces of clothing that people have designed, will it be able to generate new designs for fashion?

And it can do that. Now, there are some responsible uses, as you can tell, like this is fantastic that we can now have this machine that can generate all sorts of images and music and all sorts of stuff. We need to make sure that we're using this responsibly. And this just happens when we want to harness this potential responsibly, we have to address some of the ethical considerations and inclusivity and try to remove bias inside of the data as much as possible. And so, we want to embrace generative AI's transformative power, and we can drive things like innovation or shape industries and unlock the full extent of human creativity as well.

There are some other roles that we can use with generative AI, and one of them is that, well, some administrative roles that will diminish, but creative roles will thrive. So, some of the administrative things that we're going to see is that a lot of the mundane tasks such as even coding or some type of statistics or mathematics jobs will start to disappear and we're going to see in place of them are things like creative roles and but AI's impact jobs will be uneven. You're going to see creative problem-solving roles in advertising and marketing will thrive and with the increased adoption of AI technologies. But at the same time process-oriented roles will be automated over time including things like clerical, operation, back office, even some finance, and certain other roles that are highly repetitive. Those are going to be replaced by generative AI but it's the creative problem-solving jobs that are going to stay.

So, doing a job that requires a little thinking and creativity gives you, the human worker, a short runway before AI potentially eliminates that role, which is a possibility as well. So, then there are some effects on generative AI. What tasks are done by AI? Well, some of the tasks that automation and AI is best for is doing tedious, repetitive tasks that require precision but a low level of thinking. You're going to see this influence your jobs in a majority of ways. So, many factors will slow down the adoption of generative AI. These inhibiting factors will include the inaccuracy of AI, the bias, and the contained it within the training sets and data. And there's yet to be determined legal questions about copyright. Since we would be able to generate images based off a sample of images or video, what does that mean?

Are we violating copyright technically? Maybe, if that is a violation of the copyright role. And so, that has to be a legal question that has to be answered before this becomes widely available for everyone to use. And there's going to be an evolving regulatory environment around AI, especially within the US and Europe. So, that's going to affect our jobs and how we're going to accept AI into the workforce. Now, there are some bottom line issues that I want to discuss, right? AI probably won't replace our creative jobs, at least not for the foreseeable future. But what they will do is they could act as an assistant who can do some tasks quite well, but they would still need human supervision, right? You still have to be smart enough to know what to ask the AI to do.

So, it's great at idea generation and also for doing things like outlines for your courses or whatever you like, but it's going to help break people out of a writer's block and get started at being creative. But you would typically have to rewrite much of the generated text because it comes often with some grammatical errors and there could be some factual discrepancies in it. Remember, this isn't perfect yet. So, what we're going to start seeing then is that your job roles may change. Similar to when computers first came around.

It changed how people worked, but it didn't eliminate people's jobs per say. It just kind of shifted what they do at some of the jobs. This is a lot like what's going to happen with the AI role. It's not going to replace your creative job, but someone with more knowledge and experience using AI as a tool just might be able to replace your job completely. So it's important to accept these tools within your workplace and your industry and try and use them and incorporate in your workflow if you are allowed to. So, that's a little bit about the societal impacts on generative AI and how they can be used for creativity.

16. Video: Responsible AI in the Generative Era (it_gcdgaidj_01_enus_16)

After completing this video, you will be able to outline the importance of responsible AI practices and guidelines in the era of generative AI.
outline the importance of responsible AI practices and guidelines in the era of generative AI
[Video description begins] Topic title: Responsible AI in the Generative Era. Presented by: Elias Zoghaib. [Video description ends]
In this video, we're going to understand the importance of responsible AI practices and guidelines in this era of generative AI. Now, generative AI are going to raise new challenges in defining, measuring, and mitigating concerns about fairness, toxicity, intellectual property. There's so many things that it's touching right now and we don't really know how to answer some of these questions, but work has started on the solutions. So, what we're going to start seeing here is that there's going to be usage surface has been barely scratched. We're just scratching the surface of what these things are going to be used for and we're going to find new applications as we go through it.

So, you know, as we talk about this, well, there could be writing aids, creative content production and refinement, there's personal assistance, copywriting, code generation, and so much more. So there's a considerable excitement about these transformations and there's so many opportunities to use what generative AI can provide in many different industries. And now there are some concerns and that's understandable. I've talked a little bit about those a minute ago with some of them could include things like our copyright and fairness and toxicity and the intellectual property. So, those are all some issues that we have to sort of begin to address. What this means is let's try and look at some of the risks associated with it. So, there are some certain societal social risks, as well as some policy risks as well, and mechanisms, but even some legal risks in terms of like copywriting, like as I discussed. For example, if we want to think about a societal impact, remember these machines are very good at generating fake text or fake news or fake stories, and there's no way for you to tell if they are human or a machine.

So, what does that mean? How do we know this stuff is going to change the way we look at news? There would be no way for us to tell what's true and what's fake. There could be some policy mechanisms that will also have important roles to play. Even at places like Amazon, what they're trying to do is they're trying to use like a significantly balanced approach to try and reduce the risk in within their generative AI models for when they're trying to recommend purchases for you on the Amazon store. So, exactly what is the problem? Well, remember the generative and generative AI refers to the fact that the technology can generate open-ended content that varies with repeated retries.

So, there's varied content that can be created. So, as we're creating that content, we need to make sure that there's no bias in the content that fairness is conserved. That's really important when we're trying to create these models. We have to make sure that these fairness for LLM is really created and established, but it's murkier because what's going to happen is that there's an open-ended content they generate, and we don't really know what exactly they're going to generate based on the input. So, if we're looking at pronoun choices, like, how would they know if an LLM, when generating content about women, uses an ever so slightly more negative tone in choice of words because it's trained on misogynistic websites, for example, then it's going to generate content that is considered misogynistic because we trained it with misogynistic data.

So, how do we ensure that that fairness is sound? That's not happening. And that's just an example there. Also, you could replace that with any race or religious belief that somebody could actually generate some type of racist or disparaging remarks about that. So, we need to make sure that that is dealt with at some point. Now, there are some more privacy concerns right from not just fairness that we have to worry about, but we have to consider these privacy. It's important that a consumer lending the model not leak information about the financial or other data of the individual applicants and that training data. So, that's really important.

If I'm training a model on this kind of training data, well, what's going to happen is we don't want that data specifically to be leaked out as the output for when somebody asks it a question. So, in this kind of traditional more narrow machine learning, there are techniques for mitigating such leaks by making sure the model outputs are not overly dependent on a particular piece of training data and that's what we really have to do. Now, the issue is, is when we're dealing with these open-ended nature of this generative AI, like we're asking it open-ended questions, and it's now broadening this set of concerns from verbatim leaks of training data to more subtle copying phenomenon from within the training data. Let's say if a programmer has written some code and it uses a certain variable names, and then we ask that large language model for help in writing a subroutine or a function, the L, the large language model may generate code from its training data but with the original variable names replaced with those chosen by the programmer.

So, that way the generated code is not exactly like the training data. There's some variation between the two, but it's different only in a cosmetic way. The underlying structure is still there. There are defenses against these kind of challenges and some more challenges that I want to discuss, there is again with intellectual property. Now, a problem with these early LLMs was their tendency to occasionally produce text or code passages that exactly like parts of the training data. And so, this is resulting in privacy and other concerns that as well there could be some legal concerns with us. But there have been some improvements in this regard, but they haven't been fully prevented.

Reproductions of training content that are more ambiguous and nuanced. So, there's still some areas there, especially with things like plagiarism and cheating discussed a couple of other slides. There are some creative capabilities of generative AI, that gives worries to things that you can use it to write college essays or university essays, you can write samples for job applications, or there could be other forms of cheating and illicit copying. So, there are debates on this topic that how is this going to change education? Because now if I have an assignment from my university and they say OK, write me an essay on something on some topic, I can just go and use the generative AI tools because they're so human-like in text, it would not be able to detect it in any way.

So, this underlying challenge of verifying that a given piece of content was authored by a person is likely to present concerns not just in education but in many other contexts as well. And another thing that's interesting is known as hallucinations. So, let's say we're trying to consider the next word distribution sampling that are used in LLMs. You're going to notice that in more objective or factual use cases, they're more susceptible to what's called hallucinations. And what I mean by hallucination is that there's assertions or claims that are sound plausible but are verifiably incorrect. So, just because it sounds true, you can go and verify it, and realize that it's not true at all. And that's called hallucination. So, let me give you a bit of an example. A common phenomenon with these LLMs is creating non-existent scientific citations.

So, if one of these LLMs is prompted with the request of something like tell me about some papers that are about black holes, it is not actually searching for legitimate citations. But what it's doing, because it is a generative AI, is it's generating ones from distribution of words associated with that topic or that particular author that I could have asked. And the result will be realistic titles and topics in the area of things like machine learning. But they're not real articles, and they may include even plausible co-authors, but not actual ones that exist. So, it's really interesting to see that kind of hallucination. So remember, these things can be wrong. There are some bugs in them that we're trying to fix, and we're going to fix those eventually with the later models that we're seeing.

Again, we're just scratching the surface and capabilities of what these things can do. So, the next thing, I want to discuss is the toxicity. This is a primary concern with a generative AI, and it's the possibility of generating content that could be considered racist or offensive or disturbing, or otherwise just completely inappropriate. So, that's one of the issues that we have to do and worry about. But we have to define and scope that problem. This is all very subjective. The subjectivity involved in determining what constitutes toxic content is a huge challenge. What does that mean? What is the boundary between restricting toxic content and censorship? Like, how do we draw that line?

So again, these are questions almost as if they're philosophical questions at that point before there're even technical questions. And the next thing we want to discuss is the disruption of the nature of work. So, because these things are so proficient, the proficiency with generative AI is to able to create compelling text and images and they perform well on standardized tests. They can write entire articles on a particular topic, you can successfully summarize or improve the grammar of provided articles even. And this has created some anxiety that you could be replaced or seriously disrupted by this technology. Some of your industries or your jobs right now could be completely disrupted because of this tool, and this may be premature. Does seem though, that this generative AI at the moment will be able to at least help you in doing your work.

But eventually, what is the next version? What is the next breakthrough going to happen with these tools? Will that mean that your job is considered replaced by a machine? It's a little premature to say whether or not that will happen, but it is going to definitely shift your job roles in a little different way that you may not have realized. So, your career may have to change depending on this disruption. Now, there are some things that we can do, and one of them is that we can have training data curation. So, what if I train the data and I make sure that my data is not biased, it's not misogynistic, it doesn't have any disturbing content, that kind of thing? And so, I'm making sure that I'm curating my data so that it has very little toxicity and it's very fair and what is its content?

And that can provide some improvements. But remember, if the data doesn't contain any offensive or biased words or phrases, large language model simply won't be able to generate them. It's literally monkey see, monkey do. So, if you don't want it to generate any of this disturbing content, you start with the training data and curate that training data so that it doesn't have it. Another thing that you can do is do what's known as user training. What this is that, with user training, you can actually have a supervised approach, and this is important for when you're trying to provide it some type of guardrail that can detect and filter out unwanted content in the training data and input prompts and in generated prompts. And so, these models will require human-annotated training data in which varying types of degrees of toxicity or biases are identified.

And you're basically teaching it what is toxic and what is not toxic to prevent these prompts from showing up in the model. And then the final thing here is the sharding approaches. Now what this means is that we're going to divide the training data into smaller portions and which separate submodels are trained. Since we're sharding, we're splitting up the model into submodels. What we're going to do at the very end is we're going to take the submodels and then combine them to form the overall model. So I've basically broken down the bigger problem into smaller chunks, and then each one of them is at the end. After I'm done solving each individual chunk, I combine them into one and this is so that we can undo the effects of any particular item of data on the overall model and it allows me then to only remove it from its shared and retain the submodel. It's really interesting what we can do.

These are all some training methodologies, but we can also consider even filtering or blocking some approaches where some of the presentation to the user-generated content is explicitly compared to protected content in the training data. And then we can figure out if it's too similar to something like if I use keywords that are considered offensive. I have a list of keywords that are considered offensive, and so if a prompt contains that, or the response contains any of those words, then I can filter those out from being used. Even some interesting approaches to discourage cheating using generative AI, but they're still under development.

So, one is just simply to train a model to detect whether a given text was produced by a human or generative model. Those are still in its infancy. It's not really doing the job correctly, if you actually play around with those models and try and generate text from an AI and write text yourself, you're going to see the accuracy and the precision of those models aren't bang on or a 100%. So, that's just a little bit of some of the guidelines for prevention and for some of these challenges that you're going to see with the generative AI and what we should focus on when we're dealing with things like fairness and societal impact.

17. Video: Course Summary (it_gcdgaidj_01_enus_17)

In this video, we will summarize the key concepts covered in this course.
summarize the key concepts covered in this course
[Video description begins] Topic title: Course Summary. Presented by: Elias Zoghaib. [Video description ends]
So, in this course, we've examined generative AI concepts and approaches. We did this by exploring generative AI vs. other AI approaches, the fundamental concepts, and the generative AI ecosystem. Generative adversarial networks and variational autoencoder models, training generative models, and model strengths and limitations, prospects, and challenges. Looked at generating synthetic data with generative AI. We've looked at ethical considerations in generative AI and other ethical considerations when we're evaluating our models. In our next course, we'll move on to explore generative models and types.

© 2024 Skillsoft Ireland Limited - All rights reserved.