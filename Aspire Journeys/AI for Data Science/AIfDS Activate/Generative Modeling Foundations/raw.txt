Generative Modeling Foundations
This course dives deep into the world of generative models, providing learners with a comprehensive understanding of various generative techniques and their applications. This course is carefully designed to bridge theoretical concepts with practical applications, demystifying the methods used in popular generative models like generative adversarial networks (GANs), variational autoencoders (VAEs), and more. Through a combination of rich imagery, illustrative examples, and detailed explanations, participants will explore the differences between generative and discriminative modeling, the foundational framework of generative artificial intelligence (AI), and the various evaluation metrics that gauge the success of these models. Whether you're a budding data scientist, an AI enthusiast, or a seasoned researcher, this course offers a deep dive into the cutting-edge techniques that are shaping the future of artificial intelligence.
Table of Contents
    1. Video: Course Overview (it_gcdgaidj_02_enus_01)

    2. Video: Introduction to Training Generative Models (it_gcdgaidj_02_enus_02)

    3. Video: Bayesian Statistics and Examples (it_gcdgaidj_02_enus_03)

    4. Video: Generative vs. Discriminative Models (it_gcdgaidj_02_enus_04)

    5. Video: Framework of Generative Modeling (it_gcdgaidj_02_enus_05)

    6. Video: Generative Model Types (it_gcdgaidj_02_enus_06)

    7. Video: Deep Dive into Variational Autoencoders (VAEs) (it_gcdgaidj_02_enus_07)

    8. Video: Generative Adversarial Networks (GANs) Unveiled (it_gcdgaidj_02_enus_08)

    9. Video: Autoregressive Models Explored (it_gcdgaidj_02_enus_09)

    10. Video: Introduction to Normalizing Flow Models (it_gcdgaidj_02_enus_10)

    11. Video: Energy-based Models: A Primer (it_gcdgaidj_02_enus_11)

    12. Video: Diffusion Models (it_gcdgaidj_02_enus_12)

    13. Video: Generative Model Evaluation (it_gcdgaidj_02_enus_13)

    14. Video: Course Summary (it_gcdgaidj_02_enus_14)

1. Video: Course Overview (it_gcdgaidj_02_enus_01)

In this video, we will discover the key concepts covered in this course.
discover the key concepts covered in this course
[Video description begins] Topic title: Course Overview. Presented by: Elias Zoghaib. [Video description ends]
This course dives deep into the world of generative models, providing learners with a comprehensive understanding of various generative techniques and their applications. In this course, I'll bridge theoretical concepts with practical applications, demystifying the methods used in popular generative models like GANs and VAEs, and more. Through a combination of rich imagery, illustrative examples, and detailed explanations, participants will explore the differences between generative and discriminative modeling, the foundational framework of generative AI, and the various evaluation metrics that gauge the success of these models.

Whether you're a budding data scientist, an AI enthusiast, or a seasoned researcher, this course offers a deep dive into the cutting-edge techniques that are shaping the future of artificial intelligence.

2. Video: Introduction to Training Generative Models (it_gcdgaidj_02_enus_02)

After completing this video, you will be able to outline how generative models are trained using images and practical examples.
outline how generative models are trained using images and practical examples
[Video description begins] Topic title: Introduction to Training Generative Models. Presented by: Elias Zoghaib. [Video description ends]
In this video, I'm going to discuss how to use generative models and how they're trained using images and practical examples. Let's first understand what this actually is. A generative model takes training data that contains many different observations. OK, here each observation is a picture of a dog. When we take each of these images of dogs as one observation and we pass them to a generative model, this procedure is known as training.

What the generative model is doing is it's actually learning what a dog looks like based on the images of dogs that you've provided. Now it finds this because it's going to generate new data points that aren't exactly the same as the training data that we've seen. So this is where we take the generative model and we generate new data, hence the generative approach. And they're going to be added on with some random noise. So the images that it generates aren't exactly like the training data that we've presented. It's going to be a mixture of all of the data points together so that you won't be able to tell the difference between is this a generated image of a dog or is this a dog that actually exists and somebody took a photo of it.

You won't be able to do that. So what these models are usually used in unsupervised machine learning problems because that's how they work. This is an unsupervised approach. Its job is to find the patterns within the data. And this actually leads me to my next point. Generative models will go in-depth to model the actual data distribution within the training set. And what they're doing is they're learning the different data points rather than model just the decision boundaries between the classes. And what this does is it allows the model to create new data points out of data distribution that it has, not just the boundaries. Now, these models are prone to outliers.

This is their only drawback compared to discriminative models. Now since they're prone to outliers, their only drawback with discriminative models. But the good thing about them is that the mathematics behind the generative models can be pretty intuitive. It's an intuitive model. It's not very difficult to understand. If you've got some basic statistics and linear algebra, you'll be able to understand what these things are doing. Now here's some of the stats that I'd like to go over here. Now, this might seem a little daunting at first. I understand not everyone will have the necessary statistics approach, so I'm going to try and break this down as easily as I can. What you're seeing here is what's known as a Bayesian inference.

What we're trying to do with a Bayesian inference is it's just a method of statistical inference in which Bayes theorem is used to update the probability estimate for a hypothesis. What this means is all the Bayesian inference is just combining prior beliefs, which is what you're seeing here as prior, with the current observed data point called the likelihood. And what we're trying to do is we're trying to form a revised belief about some event or hypothesis which we call the posterior or the posterior probability. Now what I just referring to these variables that you're seeing here, the posterior is just the probability of the hypothesis after observing the data. The likelihood is the probability of observing the data given the hypothesis.

And the prior is the probability of the hypothesis before observing the data. Now, what's interesting here is that all we're trying to do is we're trying to learn as we go. It's based on the principle of updating our beliefs when given new evidence. We've all had some ideas, and that's how we learn. But when presented with a new evidence, we should really go back and change our hypothesis. This is the essence of what's happening under the hood inside of these generative models. Now, I'm not going to go too much deep into the mathematics here, but you go ahead and look at Bayesian inferences if you'd like to get a more accurate explanation of what these formulas are doing.

So let's move on here and understand some of the basic examples of generative models. Now, Bayesian Network, this is just a model that's using a directed acyclic graph or what's known as a DAG. And what these do is they draw Bayesian inferences just like I was showing you on the previous slide over a set of random variables just to calculate the probabilities. It has many applications like prediction or anomaly detection or time series prediction. Because now what's happening is as we're using Bayesian inferences, we're learning as we go, we're updating our probabilities as we present new evidence. So we're learning patterns the more we feed patterns into this training program. Now another one that I want to discuss is known as the autoregressive model.

Now what this is, this is mainly used for time series modeling and it finds a correlation between past behaviors to predict future behaviors. I'm looking at the past to try and figure out what might happen tomorrow or the next day. This is what we call autoregressive. And finally, we have the generative adversarial network. This is the GAN. You'll see it referred to in the literature. It's just a deep learning technology that uses 2 sub models. The generator model is the first part of the model and it generates new data points and the discriminative model is the model that is classifying these generated data points that are made by the generator model into real or fake.

And eventually, what'll happen is the discriminative model will give feedback to the generator model saying I could tell your data point or your picture of a dog that you generated is fake because of these points and how you drew it. And the discriminative model send information back to the generator model so that it can correct itself just like a student would. If you're trying to teach them, you grade them wrong, you show them where they went wrong and they go back and fix it.

This is exactly what's happening. There's two sub models, two AIs working with each other to try and learn how to generate data that you won't be able to tell was generated by an AI. Here's the interesting thing. There are other models here. We're just scratching the surface and I can't go into all of them here in this video. So I'm just going to give you a few of them and you can have a look for yourself online.

There's Naive Bayes, there's Hidden Markov Model or HMM, there's the Markov random field model, and there's even the Latent Dirichlet Allocation model or the LDA. Go ahead and have a look at this. You'll see that there are just underneath the hood are the same kind of Bayesian inferences that we're seeing. So that's a little bit on how generative models work and some of the probabilities and underlying mathematics behind all of these models.

3. Video: Bayesian Statistics and Examples (it_gcdgaidj_02_enus_03)

After completing this video, you will be able to outline Bayes theorem examples of prior probability, posterior probability, likelihood, and evidence.

outline Bayes theorem examples of prior probability, posterior probability, likelihood, and evidence
[Video description begins] Topic title: Bayesian Statistics and Examples. Presented by: Elias Zoghaib. [Video description ends]
In this video, I'm going to show what is statistics exactly. I'm going to provide a brief overview of what stats is, and I want to talk about some of the applications of statistics. You know, statistics is just a branch of mathematics, and we've seen it used in physics, medicine, chemistry. It's used in a lot of different organizations. Even financial mathematics uses the statistics approach. Machine learning uses statistics at its core. You'll find that there's different types of statistics that we're using, and the main one here is Bayesian statistics that are used in these AI applications.

So I want to provide just a mathematical foundation of what Bayes theorem is and what statistics is as a whole. So statistics is just a branch of mathematics and it deals with things like data collection, data organization, data analysis, data interpretation, and even data presentation. Now, there's some branches of statistics that are foundational. There's two major ones. One of them is known as frequent statistics. Now what this is frequent statistics just focuses on the likelihood of the outcomes based on the long-term frequency of occurrence. So I'm looking at what happened yesterday or last week to tell me what's going to happen in the future. Now the Bayesian stats is a little bit different. What you'll notice here is that Bayesian statistics will update the probability of beliefs based on the new evidence.

So it's not just looking at the past, but as new information gets shown to us, our probability is going to change. And there are some definitions when it comes to Bayesian statistics that I want to talk about. One of them is known as the Prior Probability. Now the Prior Probability this is where the initial belief before new evidence is even considered. So let's say the probability it will rain tomorrow is based on historical data based on the information that we had yesterday, and the likelihood is the probability of observing new evidence assuming the initial belief is true. So given that it rained, the likelihood could be the probability of observing certain humidity levels, so that might be a likelihood interpretation of it. Now the Posterior Probability is the updated probability or the belief after considering the new evidence.

So an example of this is given the humidity levels observed, the posterior would be the updated probability of rain the next day. So let me give you like a real-life example of what this is. Right. Let's say that prior suggests here that there is a likelihood that team A has a 70% chance of winning based on its past performance. So considering a sports scenario like this, I can say, OK, we're going to have some type of team that's playing and it has a 70% chance of winning based on their past performance on how they were doing in the previous number of games. So then the likelihood is going to be a given team A is winning at half-time, there's now an 80% chance of winning. And so, this is great for us. And that tells us that our likelihood changed because based on how they were doing in the game that they're playing, now our probability has gone up that we're going to win and that's considered the likelihood.

Now the posterior would be the updated chance of winning based on this half-time performance. Then we get to this idea of what conditional probability is, right. So, we're getting into Bayes theorem, and Bayes theorem is represented by this mathematical expression here. It tells us that there's a few different pieces to it and it looks a little strange. But let me explain how to read these probabilities that you're seeing. So, the P and the A with the bar and the B next to it is basically known as the posterior probability. Now in mathematics, we read this as the probability of event A happening given that event B has already happened. So, that is known as the posterior probability. Now the likelihood is given by the P then B happening given that A has already happened which is the first term in the numerator there.

And then P(A) is the prior probability and P(B) would be the total or marginal probability. Now let me just break this down the pieces of Bayes theorem 1 by 1 like I said. So P(A) is going to be the prior probability and this is the probability an event before new data is collected. And we have also the likelihood, this is the probability of the new data under the given hypothesis. And probability our Total or Marginal Probability is known as the P(B) there and this is the total probability of observing the evidence that we have and the Posterior Probability, this is the updated probability of an event given the new data calculated using this Bayes theorem. Let me give you a brief example on how to calculate this information.

So, when we're trying to understand the example problem here, I'm just going to give you a very basic picture. So, you have two bags of balls. Bag number 1 has two red balls and three blue balls. Bag number 2 has four red balls and one blue ball. Now if you draw a red ball, what is the probability that it came from bag number 1? So really, what we're trying to solve for is what's known as the posterior probability. See, we knew that we got a red ball, so that's given to us. What we're trying to solve for is the probability that we got it from the first bag, bag number 1. So, that's something that we can solve using Bayes theorem. And so you start with the solution of this model. So let's say that we've got our prior assuming that we can go to bag 1.

So we have two bags and we could have either gone to bag 1 or bag 2. That's fine. Let's assume that there's a 50-50 distribution that we could have gone to either bag. We're not biased in any way. So let's just say that the probability of going to bag 1 is 1/2. And we can also say by conservation of you know, the other probability of going to bag two is 1/2 as well, so that their probabilities would add up to 1. So if we've got a 50-50 chance of going to either bag, then we need to understand that there's a likelihood of going to the red bag at bag 1. So, what I think of doing is let's say then that the likelihood of getting a red ball from bag 1.

If you recall, the first bag has two red balls and three blue balls. So the probability of getting a red ball is 2 over the total number of balls in the bag, which is five. So, that is the probability of getting a red ball in bag 1. And if we continue on this concept, we can get the total probability of drawing a red ball. We can say that drawing a red ball is straightforward because now we can say, OK, what's the Bayesian probability of getting a red ball from bag 1 and bag 2? So, there's two situations. The first situation is I go to bag 1 and I grab a red ball out of bag 1 and then I can calculate that probability. The other scenario is I go to bag 2 and what is the probability of getting a red ball out of bag 2?

So I can calculate this and then I can use it to find the posterior, which is what we're trying to do. It says what is the probability that we got the red ball out of bag 1 but not out of bag 2? So we know we got a red ball. What's the probability that we got it out of bag 1? So if this is our posterior, then we can use Bayes theorem to solve for this. So let me just plug in our variables that we have. So, we know that we've got the probability of A bar B. So, bag 1 is A, red is B. Fine. Then we can reverse that. That's going to be equal to the probability of getting a red ball out of bag 1 multiplied by the probability of getting it to bag 1, which is the prior probability, and divide it by the total probability of getting a red ball out of both bags.

So let's go ahead and calculate the probability of getting a red ball. So, we know to get a red ball out of bag 1, the probability is 2/5. And we also know that getting a ball out of bag one is 1/2. So, we multiply those two probabilities together and so that will give us the first term in our total or marginal probability. The second term is going to bag 2. Now, if you recall, bag 2 has four red balls and one blue ball. So, that means that there's a four out of five chance that I will get a red ball out of bag 2. And there's a 50% probability that I would have gone to bag 2 as well. So if I multiply those two probabilities, I can get the total probability out of getting a red ball out of bag 2 and a red ball out of bag 1, and I add them together, that gives me the total probability of my system, doing so gives me this 3/5.

So, that's great. That gives us our total probability. Now, I know all of the terms in the Bayesian statistics problem that I showed up of here, right? And I know the probability of getting any red ball from either bag. So if I continue on and plug in all of my values, I can see that the probability of getting a red ball out of bag 1 is 2/5, just like I've shown, multiplied by the probability of going to bag 1, which is 1/2, and divide it by the probability of getting any red ball out of either bag 1 or bag 2. You can see how the math is showing out just a little bit of simple fractional math. I get 1/5 / 3/5 and this reduces down to 1/3. So, then I can say that the probability of getting a red ball out of bag 1 is going to be 1/3 or a 33% chance. That's a little bit on how we can use Bayesian statistics to calculate a simple conditional probability problem.

4. Video: Generative vs. Discriminative Models (it_gcdgaidj_02_enus_04)

Upon completion of this video, you will be able to recognize the differences between generative and discriminative modeling.
recognize the differences between generative and discriminative modeling
[Video description begins] Topic title: Generative vs. Discriminative Models. Presented by: Elias Zoghaib. [Video description ends]
In this video, I'm going to dive deep into the differences between generative and discriminative modeling. Deep learning is mostly being used for supervised machine learning algorithms, and this is really where we get into the discriminative approach. So, when we're talking about supervised machine learning algorithms like the artificial neural networks, or the convolutional neural networks, and even the recurrent neural networks, what we're referring to here is essentially a discriminator. And what this means is that it's going to try and classify things. Now the easiest one to understand first is the artificial neural network or the ANN. This is the earliest in the trio and it leverages things like the artificial neurons, backpropagation, weights, and biases to identify patterns based on inputs.

Now, the CNN or the convolutional neural network is mostly used for image recognition in computer vision tasks, but how it works is by pulling important features from an important image and then classifying the image based on those pooled features. Recurrent neural networks or the RNN, which is the latest of the three uses in advanced fields like natural language processing and handwriting recognition, even time series analysis. These are the fields that discriminative models here are the most effective and better used when we're working with deep learning, and they work well in supervised tasks. As in, I have every data point that I have in my training set has a label that it's trying to predict, OK, or a class that it's trying to classify each label as. So, with supervised learning, there's also unsupervised learning, as in my training dataset does not have a label attached to it.

And so, then we've got deep learning and neural nets that can be used to cluster images based on their similarities inside of them. Now the algorithms like the autoencoder or the Boltzmann machine, and even self-organizing maps are usually the most popular unsupervised deep learning algorithms today. They make use of what's known as generative models for tasks like exploratory data analysis of really high dimensional datasets. They can even do things like image compression or anomaly detection. And when working with a generator, which we'll speak about in just a moment, these discriminators, or these unsupervised learning models that we're working with, will help the generator generate new images. And this brings me to my next point. What's the difference between discriminative versus generative models?

Well, this difference is subtle. The discriminative models will model the decision between classes. Their job is to act as the discriminator. They take data points and they classify them with their labels. The generative models will model the actual data distribution of each class. Now what this means is that when we are trying to generate new data, we have to provide our training data for it to generate new data. Its job as a generative model is to learn the distribution within our training data so that it was able to generate new data points within that distribution that's determined from the training data. That's all there is to this. I know it sounds a little bit complicated at first, but once you start seeing some examples or some pictures like you all have here on this slide, it'll start to make a lot of sense.

Remember, the discriminative models are differentiate between classes and they're often used for direct classification tasks. They do this by using a decision boundary. This is the dotted black line that you see on the screen here. It puts all of its data points together, and the dark blue class and the light blue class of data points, and it tries to draw a line between them so that it's able to understand anything above this line is a dark blue and anything below this line is a light blue. So if I come in and I give it new data, a data point that we don't know what color it is, it's going to see where it lies on this graph. Is it above the line or below the line? And from there it's able to make a classification. Now that's all that's going on here with the discriminative approach, and it might seem a little strange.

But if you think about different types of discriminative approaches like logistic regression or support vector machines, and most deep neural networks are used for classification tasks and they're discriminative models. Now the generative models, their whole point is to learn how the data is generated. They aim to model the distribution that actually generated the data. So, what you're seeing here, rather than creating a dotted black line, that's the decision boundary, it's going to learn the distribution of the dark blue and the light blue data points and it's going to then take those distributions, mix them together, and add some random noise to generate a new data point. That's all that's happening here. So, what we're actually doing underneath the hood is we're modeling the probabilities and by using Bayes theorem they can then use these tasks for generating new data.

And some examples include things like the generative adversarial networks and the variational autoencoders deep neural networks. Now what the discriminative model is doing, it's learning the conditional probability distribution. Now, I can't go into too much detail here because it's kind of goes beyond the scope of the video, but when you see this, the terminology, or this notation anywhere in the literature, in a website, or a textbook, what this is a conditional probability distribution and it's read in English as the probability of y given x. Now both of these models are generally used in the supervised learning approach. So, let's try and think about some discriminative classifiers. Well, we have traditional neural networks from before. The ANNs, the CNNs, and the RNNs are all great tools for this. There's also logistic regression, a little different.

Whereas the traditional neural networks can differentiate between two or more classes, the logistic regression does the same thing without the use of a deep neural network. But it's now able to classify things on either two classes, 0 or 1, apple or orange. And the idea then can be expanded even simpler as the nearest neighbor approach. This makes a lot of sense. If I have a data distribution and I'm trying to figure out where my new data point that doesn't have a label lies, I just plot it on the graph and see what's its closest neighbor, what's its closest relation to it. If it's closer to the dark blue points, then it's a dark blue class, if it's closer to the light blue points, then it's a light blue class. And there's all sorts of ways that we can actually measure the distance, but the most common one is just to measure the Euclidean distance.

There's also the scalar vector machines. This is a really profound tool. Now the scalar vector machines, or what you'll sometimes see them as support vector machines, will identify between classes. And the Conditional Random Fields. This actually models a high dimensional probability, more than two variables or three variables. Think of it as a sheet of paper and what's happening is is some sheets are higher. There's mountainous areas and valleys and we're plotting our data point on this sheet of paper and seeing how high or how low is the probability compared to other things. This is a little complex to see right now. You would have to actually visualize this random field for us. So, the idea of the generative model is to learn the probability distribution p(x,y).

It predicts the conditional probability with the help of Bayes theorem. So, I give it two parameters x and y and it tells me what the probability is. In this case, x and y could be the position of a data point on a graph and we can identify what that probability would be for a particular class. Now, there are a few different types of generative classifiers and I can't go into all of them here, but I'll just try and touch on some of these. So, there's the Bayesian networks, and there are Naive Bayes, and Hidden Markov Model, and the Markov random fields, are all examples of generative classifiers. And like I said, have a look at these and see if you can expand your knowledge on some of these generative classifiers. These tools and models all work on the same idea that they're trying to classify using a generative data approach by learning the joint probability distribution. So, that's a little bit about the differences between discriminative and generative AI approaches.

5. Video: Framework of Generative Modeling (it_gcdgaidj_02_enus_05)

After completing this video, you will be able to outline the foundational framework of generative modeling.
outline the foundational framework of generative modeling
[Video description begins] Topic title: Framework of Generative Modeling. Presented by: Elias Zoghaib. [Video description ends]
In this video, I'm going to teach you about the foundational frameworks of generative models. Before we talk about generative models, I just want to try and give a brief overview of what's been happening in the AI field so far. A lot of the advances that we've seen in the machine learning world usually is coming from a discriminative model approach. It was the primary focus behind our machine learning ideas and a discriminating model has been the driving force behind most progress in machine learning methodology today. And in the last two decades, both in academia and industry, we've really been focused on these discriminative models that we were talking about. And so why is that? Well, because they show practical solutions to business problems.

Discriminative models have historically been more readily applicable to business problems more than generative models. Now, think about it like this. In a business settings, we don't care how the data was generated, but instead, we want to know how a new example should be categorized or valued. Let's say we have a satellite image and a government defense official is trying to look at this image. They only care about the probability that it contains enemy units, not the probability that the particular image should appear with. They're interested in what's in the image, and a customer relations manager would only be interested if the sentiment of an incoming email is positive or negative. Wouldn't find much use in a generative model that could output examples of customer emails.

Or a doctor wants to know the chance that you have a retinal image that has glaucoma rather than have access to a model that can generate novel pictures of eyes. So this is the whole idea of what's known as machine learning as a service. We're building some type of model, we're allowing people to access it to discriminate between their data points. That's the whole point of all this. Let's go ahead and take a look at specifically the generative models. So, generative models means that training data contains many representative examples. When we're first understanding idea of generative models, we need to start with the training data. We know that each training data has a bunch of different data points, and each data point is known as an observation. Now that data point can contain things like features, such as the particular feature of a data point.

So, if I had demographic data, it could be age, race, location, all sorts of things. And then the label could be something that I'm trying to predict, such as weight, for example, whatever it might be. Now that's the whole concept of an observation and a data point. So, just remember, training data has many different observation points. So, this is the idea of the generative models. You see our training data that contain our observation points. We're going to feed that training data directly into the generative model. Now what's happening here is that the generative model is going to take that information, add some random noise to the data that it's trying to generate, and then create a generated item. That's the whole flow of this.

All we're doing is we're taking how a dataset is generated and we're trying to figure out, in terms of a probabilistic model, what's the probability of generating a particular piece of data from the training data. Now by sampling from this model, we're able to generate this new data. Make me a picture of a dog, make me a picture of a cat, whatever might be the task. And so, the idea that I like to give, or the example I like to give is suppose we have a dataset containing images of horses. I want to build a model that can generate a new image of a horse, but that's never existed but still looks real because the model has learned the general rules that governs an appearance of a horse. That's the kind of problem that can be solved using these generative models. So, let's try and think about why generative model has gained popularity. The interesting thing is that we're interested in how things are created. That's the whole point of this.

We're from a theoretically point of view, we want to be able to not just categorize our data, but we should also seek a more complete understanding of how the data was generated in the first place. And this becomes the future of machine learning. So, what we're going to try to think of, it's highly likely. And again here we're not exactly sure that generative models will be central to driving future developments in other fields of machine learning such as reinforcement learning or understanding the concept of trial and error. That's the idea of what's happening. And finally, if we're truly to say that we built a machine that has some acquired form of intelligence, and I use that term very loosely because we don't really know exactly what that would mean, it's going to be difficult because at this point, our intelligence, if we look at these ChatGPT models, they're really just kind of like a mirror.

They take data and they learn the probability, but they're not generating any new insights. They're not doing things that are completely different from the dataset. They are just essentially acting as a really advanced parrot, in a way, this is something worth looking into. How is it possible that we can have a form of intelligence that are comparable to a human and maybe generative model might be part of that solution? So, it's something to continue our research on. That's the reasons why these generative models have gained popularity. Now, there are some key aspects to the generative models, right? So, I talk a little bit about the idea of what's a feature. So, each observation consists of many features.

Now, for an image generation problem, the features are usually the individual pixels, values, and their brightness with three colors if it's color or sometime if it's black and white, it's just zeros to ones. But it's our goal to build a model that can generate new sets of features that look as if they've been created using the same rules as the original data. And we do this by going to probabilistic approach rather than a deterministic approach. If our model is just a fixed calculation, such as taking the average value of each pixel in the dataset, it's not generative because the model produces the same output every time. Now the model must include what's known as a random or stochastic element that influences the individual examples generated by the model. So, if you can imagine that there's some unknown probabilistic distribution and we're trying to see why some images are likely to be found in the training dataset and other images are not that's the idea of doing this.

So, we're taking this randomness and it has to have that influence on the individual samples that are generated by the model. That's the whole purpose of these things. OK, take the features of our data point, identify the probability distribution, add some randomness when we generate new data based on that probability distribution, and see how likely it is to change. Now, there's a couple of other things here when it comes to labeling. So, generative models typically used unlabeled data. As in, we're dealing with an unsupervised data approach, and labeled data can be used. But what we're trying to do mainly is estimate the probability that our observation belongs to a particular category. Generative model doesn't care about labeling the observations, but it attempts to estimate the probability of seeing the observations at all.

So, the key point that I want to show here is that to build a perfect discriminative model to identify like a particular painting, it would still have no idea how to create the painting but it's able to classify them. And so, that's what we need to do here. But you can use labeled data with it, it does help it. But discriminative model is synonymous with a supervised learning approach. But a generative model is more of the unsupervised learning approaches, and the data does not have a label. So, let's talk about some of the generative model frameworks, right. So, we understand that a GAN structure has two main pieces. The first piece is the discriminator. What this does is it's going to discriminate or distinguish the generator's fake data from real data. The discriminator penalizes the generator for producing implausible results.

And this is where the interesting thing is. What we're trying to do here as the discriminator is tell the generator what it's doing wrong so that it can actually approach this a little carefully. And this is known as backpropagation. And I'll give a brief example of what that is in just a moment. But the idea is that the generator will learn to generate plausible data, and the generated data that it comes become negative training examples for the discriminator. So, what I'm trying to do here, get the generator to generate data, see if the discriminator can tell if it's real or fake, and then tell the generator what it's done right or wrong so that it can adjust accordingly. And this is what's known as backpropagation. And you'll see here in this example, we've got the real images and we've got the sample.

And what we're going to do is we're going to sample some of the real images, feed them into the discriminator, and take the random input, pass it to the generator, generate a new sample of data, and pass that into the discriminator. Now the discriminator has the real images, and it has the generated images, and it's going to try to tell them apart. And this little piece right now is it's going to calculate what's called the discriminator loss, as in how far off is this generated sample from the real thing? Can we get some type of behavior out of the generator? Can the discriminator tell something to the generator? We're using this discriminator loss, as in the error, what to do and how to fix it? And this is known as backpropagation.

So, backpropagation has a couple of things, like I want to give a brief example of what this does and what this means. Backpropagation is short for backward propagation of errors. Now what does this mean? Well, think about it like this. Imagine you're trying to teach a robot how to throw a ball into a hoop. You have the feed-forward pass. What this means is that the robot will throw the ball and then you compute the loss, in this case, would be the discriminator loss. And in this example, you'll see how far the ball is from the hoop.

If it's too far to the left, right, short, or far, you make note of that. Then you tell the robot exactly how it went wrong. Your arm was too high, you threw too hard. What the robot will do is it'll listen to that feedback and adjust its throw for the next try. And then we'll iterate, we'll do it again. The robot will keep trying and I'll keep giving feedback each and every single time. That's all that's happening here. Backpropagation, in this example, the robot would be the generator, and the discriminator would be the person watching what the generator is doing. And then it's going to give feedback to the generator so that it can adjust. And it does this over and over and over again. That's the concept of backpropagation. So, eventually, it's going to start generating data based on the discriminator's feedback, and it's going to start being very real.

So, here's another one that I want to give another same kind of deal. We're taking the generator and the sample and the discriminator, and we're calculating the generator loss and then feeding back what's called a backpropagation. The discriminator is going to calculate the loss, tell it what went wrong, and go back to the generator and make the decisions again. Same kind of architecture that's happening. Now, the generative adversarial network training. This is known as the discriminator, we'll train for one or more epochs. OK, epoch is essentially just a training time, so that's all we have to do. The generator and the discriminator have different training processes, but we train the GAN as a whole, and there's a few different periods in alternating periods. We have to understand that the discriminator needs to be smart enough to detect what's real or fake.

So, we'll train that first, and we'll train that for one or more epoch times. The generator will train for one or more epoch as well. But we train them separately. Now what we're going to do is we're going to repeat these first two steps to continue to train the generator and the discriminator networks together. Eventually, what we're going to reach is this thing known as convergence. And convergence just means that it's reached a stable state, as in it's not going to get any more accurate than it is. And then finally, I want to talk a bit about loss functions. Now, a loss function is just an error, as in generative adversarial network will attempt to replicate the probability distribution, that's the first step. And the loss functions will capture the difference between the two distributions, the real and the fake. And then it's going to adjust based on that loss function.

So, if that's just the error, the loss function is just the difference between the real and the fake so that we can better make the fake look more like the real. GANs will have two common loss functions, the minimax loss and the Wasserstein loss. And what these are is a function that takes two inputs, the real and the fake data point, and it does some operations with those two parameters and returns a value. And our job is to minimize that loss function as close as possible. Now, the minimax loss and the Wasserstein loss are the two most common functions, and they're just basically a mathematical equation that will take those two parameters. So, that's a little bit on how we can use discriminative models and generative models and what it means to train them.

6. Video: Generative Model Types (it_gcdgaidj_02_enus_06)

Upon completion of this video, you will be able to identify popular types of generative models.
identify popular types of generative models
[Video description begins] Topic title: Generative Model Types. Presented by: Elias Zoghaib. [Video description ends]
In this video, I will explore and explain some of the popular types of generative models. Let's first take a look at the Naive Bayes classifier. Now, this is just a classifier that's a supervised machine learning algorithm and it's used for classification tasks. It does things like text classifications very well. It's also part of what's known as the generative learning algorithms. So what this does, it can seek to model the distribution of the inputs of a given class or category. And we want to use these things rather than a discriminative classifier such as logistic regression, because in logistic regression or any other discriminative classifier, it learns what features are most important to differentiate between classes.

But in this case with a generative classifier, we're actually just modeling the inputs for a distribution. So it's a little different from your standard classifiers that you would have probably seen in a previous machine learning course. Now, the Bayesian networks is a probabilistic graphical model. You see these in the literature called Bayes network or Bayes net or a belief network or decision network. They're all referring to this, the Bayesian network. So what we're doing here is creating this probabilistic graphical model that just represents a set of variables, and this set of variables and conditional dependencies are usually represented via a directed acyclic graph called a DAG. It's one of these several forms of causal notation. And what I want to also bring up is that this is perfect for casual predictions based on actual events. So if you have an event that occurred and you want to predict the likelihood that one of several possible known causes was the contributing factor, you can use a Bayesian network to identify this. Now, Bayesian networks could represent the probabilistic relationships between diseases and systems, for example, but you can also have certain symptoms like the network can be used to compute probabilities.

So what we're actually doing here is with the example that I gave with diseases and symptoms is we're trying to understand the inference and learning part in the Bayesian network. And this is the part of the efficient algorithm that the Bayesian network is when it comes to these two things. And Bayesian networks that model sequences of variables such as speech signals or protein sequences or whatever it is, they're called dynamic Bayesian networks. And it's dynamic because as these things change with speech signals, with the function of time, you're going to have a dynamic view of the data. It's not static, things will change. So, the generalizations of Bayesian networks that can represent and solve decision problems are called influence diagrams. So, now we've understand like the different diagrams and networks within Bayesian networks. Let's move on to Markov fields.

So, Markov Random Fields or MRF. Now what this is typically not very powerful, lacks expressive power in terms of this, but it's simplistic factors to capture patterns, so it's not that difficult to understand in terms of the statistics used like with some of the other tools. But with Markov Random Fields, you have the simplistic factor to capture these patterns and what you'll see these things, they're used in generative image modeling. They have long been plagued by this lack of expressive power and when it comes to this, but they offer this very simplistic factor to capture local patterns within our data, and that's where they excel. And they have the cyclic dependency structure for their dataset that they're working on. Now, the Hidden Markov Field is a statistical Markov model in which the systems being modeled is assumed to be a Markov process.

So if we're trying to think about the observable process whose outcomes are affected in a known way, what we're talking about here is a non hidden state, but with something that's unobservable, we refer to them as hidden states, and when we're talking about an HMM, what we're really trying to figure out is we're trying to learn by observation. That's the whole point of this. And the idea is that with HMM, outcome must be affected exclusively by the outcome, and your outcomes must be conditionally independent, and that's something that's important. So, where Hidden Markov models are actually used a lot, they're actually known for their applications in physics. If you've studied things like thermodynamics or statistical mechanics, they're used a lot there. But also in economics and finance, anything with signal processing,

I've seen that used, but it's very very powerful when it comes to things like pattern recognition. So, there are a few different types of generative AI models that I just want to discuss here. So just to go over it again, we know that in the generative adversarial network, there's the generator and there's the discriminator. And this class of model is really important because if you think about a tool such as ChatGPT, well, that's actually a great multimodal generative adversarial network. And this is great for synthetic generation. That's what makes it generative, is they're very good at generation. So when we're talking about a generator and a discriminator, one part of the model generates new data, and the discriminator will try and detect if the data is fake, as in it's been generated by a machine. So the discriminator is just a really good classifier.

And what these two models are actually doing under the hood is they're working in an adversarial manner. So the generator is trying to generate data to trick this very advanced classifier to trick it into thinking that a human wrote this text or generated this image. And so, what you'll end up seeing is that the discriminator will then tell the generator what it got wrong if it's able to detect something is not real, as in it's generated by AI, and the generator will then take its input and reweight itself and then do the data again. So let's look at the variational autoencoder. The encoder-decoder infrastructure is how this works. Now, these are just like a generative model, but they utilize this encoder-decoder architecture and all they're doing is they're mapping input data into what's known as the latent space and then reconstructing it back to the original data domain. And so, what we're trying to do here is we're trying to just transform data using a mathematical system such as the variational autoencoder, but then try and put it back together in its original domain.

And what we want to do here is we want to be able to see how well are we able to reconstruct our data after it's passed through this latent space. And if the reconstruction and the input are very similar to each other, then we know that the variational autoencoder is doing well. It's got the right answer. Now, there are certain use cases for VAEs and it's very diverse areas. Obviously, image generation is a big one, but it's used a lot in anomaly detection and data compression. They enable the generation of things like realistic images or art synthesis and even interactive exploration of different statistical sets. Now if you've got image, audio, and video content creation and you need synthetic data to be photo-realistic then you want to have an encoder-decoder infrastructure with this variational autoencoder to help you out. Now, transformer-based models. This is best for generation and content code completion. This uses a deep learning architecture and if you're into the generative pre-trained transformer or GPT that is an example of what's going on here.

Now, what's going on with the transformer-based models, they're just a type of deep learning architecture that's gained significant popularity with things like natural language processing. The transformer-based models are a type of deep learning architecture and it's gained significant popularity and success in the specific NLP tasks. So, anything with translation. So, a notable application of this transformer, and I've mentioned it before, is using things like GPT-3 or GPT-4. They've demonstrated impressive capabilities in generating coherent and contextually relevant text given a prompt, and they do this using this transformer-based model. Autoregressive model. These generate new samples by modeling the conditional probability of each data point based on the perceived context. Their sequential data generation is what happens here. So, the autoregressive models are trained to predict the next data point, and that's where the sequential data generation is interesting and they really understand the previous context of what's given before the data point.

So if you're doing inference, they generate new samples by sampling from the learned conditional distributions. And that's the interesting part behind this. Certain use examples of autoregressive models include things like language modeling and music composition. And what they're doing is they're capturing dependencies in sequence and they produce coherent and contextually relevant outputs based on the previous data points that you fed it. Flow-based models directly model the data distribution by defining an inevitable transformation or an invertible transformation between the input and output spaces. Now what this means is we're going to take the input, use a transformation, and then be able to get the output and pass it through a reverse transformation, invert it basically, and get the input again. Now they allow for both data generation and efficient destiny estimation.

Now, destiny estimation and data generation have applications and things like image generation, density estimation, and anomaly detection as well. Now they offer advantages such as tractable likelihood evaluation. And there's all sorts of statistical tools that you can use like exact sampling and flexible latent space modeling that'll help you with these tools as well. Now that Latent Dirichlet Allocation. Now, this is used in natural language processing and latent LDA is a Bayesian network. Now what this does is it'll try and explain a set of observations through its unobserved groups in each group explains why some parts of the data are similar. Now this is just an example of a Bayesian network, but in this observation, let's say if we're dealing with words, what you're doing is you're kind of collecting the words into documents and each word's presence is attributable to one of the documents topics. And if you think about documents, they'll contain a small number of topics. So it's basically able to look at the words and associate them with the topic in such a way that essentially classifying with natural language processing. So that's a little bit about some of the popular models that are used in natural language processing and various other industries.

7. Video: Deep Dive into Variational Autoencoders (VAEs) (it_gcdgaidj_02_enus_07)

After completing this video, you will be able to outline the intricacies and applications of variational autoencoders (VAEs).
outline the intricacies and applications of variational autoencoders (VAEs)
[Video description begins] Topic title: Deep Dive into Variational Autoencoders (VAEs). Presented by: Elias Zoghaib. [Video description ends]
In this video, I'm going to explain the intricacies and applications of variational autoencoders. The concept of the variational autoencoder really just comes down to understanding what dimensionality reduction is. Before we can go into VAEs, we need to make sure that we understand what this means. Now what is dimensionality reduction? Now referring to machine learning and dimensionality reduction is just the process of reducing the number of features that describes some data point. Now this reduction is done either by selection as in you can extract it by eye, some existing features are conserved where you can have some type of extraction. A reduced number of new features are created based on the old features. And this is useful in many situations.

And what I want to first understand is how do we select those features. So, if we understand that there's going to be an encoder, which is the process that produces these new features and the representation from the old features. And what we're going to do is have this encoder do either a selection or extraction. And then the decoder is the reverse process, take this new features, and piece it back together, and get me the old features. Now, that's all that's really happening here. If you think about figuring out what columns or what dimensions you want to reduce, and then the encoder's job is to do that. Once you have your dimensions reduced and you've got just the features that you're interested in, maybe you want to go back so you have your job is just to find the best encoder/decoder pair for this.

That is the essence of what dimensionality reduction is. Now here I've got this mathematical formula and that explains it. Now the main purpose of a dimensionality reduction method is just to find the best encoder/decoder pair. And what really this means is that we're just looking to keep the maximum of information when encoded and so it has the minimum of reconstruction error when decoding. So remember, the idea of the encoder is to take a lot of dimensions and reduce it down to the most important bits. Our job for the encoder is to make sure that we're doing a very minimal removals of dimensions. So, we want to keep the maximum amount of information when encoding as much as possible and just remove the bits that aren't really important. And this is going to make sure our reconstruction error when we try and go back is minimized.

Now, this formula here defines the reconstruction error and measures between the input data, which is known as X, and the encoded-decoded data. So, once we look at the differences between those two, we understand that the minimum error, which is this argument that you're seeing is going to be the correct answer for this variational autoencoder. Now, I don't expect everyone to know what this mathematics is, but I just keep this stuff here in for completeness. OK, so now if we look at the principal component analysis, this is one tool that does dimensionality reduction really well. Now, one of the first methods that come in mind when speaking about this dimensionality reduction is this PCA. And so, what we're trying to do here is we're trying to build new independent features.

Now, in other words, to show what PCA does is we're going to build these features that are linear combinations of the old features. OK, so if you think about n_e as the new features and you can think about n_d as the old features, for example. And what we're trying to do here is take the old features, understand that there are going to be some new independent features that are linear combinations of that. And now what this means is that I'm going to take a little bit of some of this feature from the old feature and mix them together and I get some type of linear combination. Now these linear combinations of these old features are going to give us the best linear subspace, and this comes down to some type of orthogonal basis of new features such that the error of the approximating the data by the projections on the subspace is as small as possible.

So, here's this PCA that we're seeing. I'll give you an example. So, the blue dots represent the data that comes in, and I need to figure out what axes is this data on. So, I find the two components of the data and I see that, OK, well, along this axis PC1, I can see that this is going to be an axis that the data can be represented on. And so, this is the principal component analysis at work. The other component is the vertical component PC2. So, you can almost see on these little crosshairs for PC1 and PC2 that all the data can be represented on this new axis. And what we're trying to do here, this comes down to a little bit of linear algebra, and we just don't have time to go over this.

But basically, what we're trying to do is we're doing a transformation of data from axis 1 to axis 2 to PC1 to PC2 axis and represent our data that way. So, it's easier to understand. So, that's the idea of what principal component analysis is and that's kind of the tool without going too deep into the mathematics, that's what's causing it. Now the autoencoders are a little bit different, right? So, if we now first discuss autoencoders, so the encoder and decoder neural networks, and this is used for dimensionality reduction, right? Again, you can see that the encoder is just meant to do what we were talking about, which is reduce the dimensionality. Now the autoencoder comes into the idea is that it's pretty simple and it consists in setting an encoder and decoder as neural networks. And what we're trying to do is we're trying to learn the best encoding-decoding scheme that uses this iterative optimization process.

And all this means is that we're doing a very advanced trial and error. We're trying something else out and seeing how it affects the error. And once we find this iterative optimization technique, we can feed the autoencoder architecture with some new data and we compare the encoded-decoded output with the initial data and we can backpropagate this error so that we can get the architecture to update. Now that's a lot of words, so let me just draw it out for you here in a picture. So, if you look at the left-hand side of my diagram here, you'll see that that's input data. And the idea is that the input data immediately has, if you think about the number of blocks there, it's the number of dimensions. You can see that when we go from the first layer to the second layer, the number of blocks decreases. So, we've now reduced our dimensionality.

This is known as the encoding process. Now with this new encoded data, we pass it through this latent space where we do some operations on this encoded data. And what'll happen is, is that this is going to transform it, but we're still not going to have the original data dimensions that we had to begin with. So, that's what you see here on the darker blue on the right past the latent space. What we're going to do now at this point is we need to now decode it so that we get the original number of data points that we had. And this is known as the reconstructed data. Now, the difference between that reconstructed data and the input data is known as our reconstruction error, and that's the idea of what these autoencoders will do. Now, the variational autoencoder, right?

It comes down to the idea that we've just been discussing dimensionality reduction in terms, you know, in introducing things like the encoder/decoder architectures and how we can train them using these things known as gradient descent, which is a mathematical process that all machine learning algorithms are tied to. It's not the main one, but it's a way for it to measure and reduce the error. So what does this mean? Where does the variational autoencoder piece come? So, what we're trying to do is we're trying to training that's regularized to prevent overfitting. This is what the variational autoencoder really does in terms of the regular autoencoder. The autoencoder is just trained to encode and decode with as few as loss as possible, no matter how the latent space is organized, its job is to be able to use this latent space and enable this generative process.

Now if we think about these variational autoencoder, it can be defined as just being an autoencoder like we were just talking about before, but the training is just regularized to avoid overfitting and ensure that the latent space just make sure that it has a generative process that is enabled. That's all that's going on here. The only difference between variational autoencoder is the fact that one is regularized and one isn't. So, the training variational autoencoder comes down to just a few different steps. First, we have to decode the input as distribution over the latent space. Now instead of encoding it as an input as a single point, we encode it as a distribution over the latent space. And that's the first step.

So, we first decode this and then what we have to do is we make sure that there's a sample point from the latent space is from that distribution. So, we take a point from the latent space as sampled from this distribution that we've just decoded, and then this sample point is decoded and then reconstruction error can be computed. Now once we have this reconstruction error, we have to now backpropagate it through the network so that it can understand what it did wrong, so that it can update its weights inside of the neural network, and so that it can better associate the correct tool that we're going to be using the correct weights for the neural networks so that it's better able to classify and reduce dimensions. Now, this is really all that's going on here.

And what we're doing is we're taking something that was deterministic with the regular autoencoder and adding a variational autoencoder, which is probabilistic. And this is, we're careful to decode input as a distribution rather than just a single point of data and we sample from that distribution in our data. And then decode it into the latent space and then compute the reconstruction error after we've decoded that sample point. And then we do this over millions of data points. And eventually what will happen is that you'll have this variational autoencoder that is really well-trained and very good at doing some type of generation in dimensionality reduction. So, that's a little bit about how variational autoencoders work underneath the hood.

8. Video: Generative Adversarial Networks (GANs) Unveiled (it_gcdgaidj_02_enus_08)

Upon completion of this video, you will be able to provide an overview of generative adversarial networks (GANs) and their groundbreaking applications.
provide an overview of generative adversarial networks (GANs) and their groundbreaking applications
[Video description begins] Topic title: Generative Adversarial Networks (GANs) Unveiled. Presented by: Elias Zoghaib. [Video description ends]
In this video, I'm going to dive into generative adversarial networks and some other groundbreaking applications. Over the past decade, there's been a huge amount of available data, and this is where big data starts to come in. What we've noticed is that with this big data, one of the main things that we want to do is the optimization of algorithms and the constant evolution of computing power. It's just enabled us to come up with very advanced artificial intelligence to perform more and more human tasks. And this is where the generative adversarial networks come in. What they are is that they use this generative model approach with implicit estimation and this is part of the unsupervised learning. And there are two neural networks that are working against each other and they're adversarial in the sense because they are working against each other.

So, we understand the terms generative and networks. But together with generative adversarial networks comes the ideas that you've got a system that's going to be generating data and its adversary which is going to figure out if that data is human-generated or AI-generated. And what'll happen is that once these two are basically trying to trick each other, you're going to identify errors, and the discriminator or the classifier that's trying to identify if something's real or fake is going to tell the generator what it needs to do to be a little more real in the sense that it's not able to detect if something is human-generated or not. Now this is the idea of this generator versus discriminator. Think of it as a two-player game.

You've got a neural network called the generator and a neural network called the discriminator. And what's going on here is that the generator is just going to try to fool the discriminator by generating real-looking images, while the discriminator is going to try to distinguish between these real and fake images. Now I'm using images here as just an example, just for us to contextualize what's happening. Keep in mind that it can actually be for any piece of text or anything really. And so, that's what's happening here. We're basically feeding real images to the discriminator network and fake images from the generator network. And we're trying to see, OK, is it able to detect if something's real or fake? And what'll happen is that this adversarial and this generative adversarial network becomes really apparent to us. Now, I'm not going to go too deep into this.

I just want to put this out here to explain some things and the fact that what's happening here is basically a very advanced mathematical game between these two networks, and it's calculating things based on their weights, denoted by Theta. Now, the discriminator and the generator or D and G here each have their own set of weights, and they're constantly adjusting these sets of weights so that they can continue to minimize their error. And that's all that's happening here. I don't want to go too much depth into this mathematical system but keep in mind that if you're interested in math, this is definitely a way to apply it in AI. Now in GAN training, when we're dealing with generative adversarial network, we first have to figure out how to train these tools, and what we're going to do is we're going to eventually have the generator improve at image creation.

Now the discriminator is going to become better at telling these fake and real images apart. That's really what's going to happen during this training process. The process will reach what's known equilibrium. Now what I mean by that is when the discriminator can no longer distinguish between real from fake images, we've reached this equilibrium and if the discriminator is well-trained and the generator images to generate real-looking images that fool the discriminator, then we have a good generative model. The generative model did what we wanted it to do and that is beat the discriminator and win this game. And so, what are some of the generative adversarial network frameworks that are available? Primarily, a lot of these are going to be used with Python, in fact, I think most of them are.

And with the TensorFlow, you've got the open-source machine learning framework and this is developed by Google and it provides various tools and libraries for implementing and training these GANs. You can also have things like PyTorch, which is an open-source machine learning framework developed by Facebook and it provides tools and libraries for implementing and training these generative adversarial networks. And there's the GANLab as well. Now with the GANLab, this is a web-based tool that allows users to experiment with GANs in a visual interactive environment. And Chainer is an open-source deep-learning framework developed by Preferred Networks. And what this does is it provides tools and libraries for implementing and training these GANs.

And, of course, there's Keras. This is an open-source deep-learning framework developed by high-level APIs for building and training deep-learning models. It includes a GAN class that can quickly build and train these GANs as well. Now, here's some common generative adversarial network applications. Why are these things so interesting? Well, they're interesting because what we're seeing here are the realistic photos. When we want to generate examples for image datasets, we want to try and generate real images, right? But we can also do things with human faces as well, even with humans. So, if I take a whole bunch of pictures of people and I can feed them into a generative adversarial network, it'll know what a human being looks like across many different ethnicities depending on my training set, and it'll generate new data points and new photos of people that don't exist at all.

And what we can also do is we can do this translating image-to-image. So, there's this known as image-to-image translation, and this means that it can translate images from one domain to another. And you can convert things like a photograph of a real-world scene into a line drawing or a painting. You can create all sorts of new content from this in various ways. And we can also translate text-to-image, which are some popular tools. And these generative adversarial networks are the core of how they work. So, I can generate images based on a given text description, and I can use it to create visual representations of concepts or generate images from a machine learning task. There's also semantic translation of image-to-photo.

What this means is I can translate images from a semantic representation such as a label map or a segmentation map into a realistic photograph, and you can use it to generate synthetic data for training machine learning models or to visualize concepts more practically. Another thing that's kind of fun that they can do is they can generate cartoon characters that are similar to those found in popular movies or television shows. And these developed characters can create new content or customize existing characters and games and other applications. So, you can actually have generative movies, generative TV shows, video games, all sorts of pieces of entertainment coming from this. Some more common GAN applications is that they offer facial recognition. They're very, very good at doing this. Another thing that they're great at is human pose generation. They can generate images of people in new poses, such as difficult or impossible for humans to achieve, and it can be used to create new content or to augment existing image datasets.

And there's the photographic images editing. They can be used to edit photographs in various ways such as changing the background, adding or removing objects, all sorts of things that would be done in photographic editing. And they can take photos and migrate them into emojis, creating a more portion-wise and expressive form of communication. Now some more common GAN applications include things like facial aging. They can be used to generate images of people at different ages and it allows users to visualize how they might look in the future or to see what they might have looked like in the past. Super resolution, GANs can enhance images resolutions so it allows users to produce higher quality versions of low-resolution images that currently exist. And there's photographic blending.

This can blend two or more photographs, creating a new image that combines elements from the original images and photographic reconstruction. Exactly what you'd sound like. What this does is I'm going to fill in missing or damaged parts of photographs and create more complete and visually appealing images. Some more things that I want to discuss here is exactly what does this mean when we're trying to use these common GAN applications. If I want to generate 3D objects, I can also do this. So, 3D object generation creates 3D models of objects or scenes from 2D images of data. They can also do like video prediction, which is fascinating.

What this is doing is it's actually generating future frames of a video based on a given sequence of past frames. GANs have been used to develop systems that can generate realistic, high-quality video frames that predict the future evolution of the scene that they're in. And then there's things like clothing style generation, which is fascinating, and this is converting an image of clothing from one style or design to another. And GANs have been used to develop systems that can translate images of clothing from one type to another. And it can include things like changing the color or pattern of a shirt and a dress. So, this has been a basic introduction of common GAN frameworks and applications.

9. Video: Autoregressive Models Explored (it_gcdgaidj_02_enus_09)

After completing this video, you will be able to outline the use of long short-term memory (LSTM) and PixelCNN under the umbrella of autoregressive models.

outline the use of long short-term memory (LSTM) and PixelCNN under the umbrella of autoregressive models
[Video description begins] Topic title: Autoregressive Models Explored. Presented by: Elias Zoghaib. [Video description ends]
In this video, I'm going to talk to you a little bit about PixelCNNs and other autoregressive models. So, what are autoregressive models? Before we can do that, we first want to understand the concept of distribution estimation. Now, this is a problem that's not limited to deep learning, and it's a problem used in many applications actually. The goal here is to estimate the distribution of the probability underlying a particular dataset called D. Therefore, what's going to happen here is that our estimated distribution should be close to actual distribution and it should be tractable, which should be easy to compute and efficient to obtain. That's all what I mean by tractable distribution.

It has a closed form and is computationally efficient to obtain. So if you see that term in math terms, this is usually what they're trying to do is they're trying to explain it how simple it is for us to compute. And our whole concept with this distribution estimation is to estimate the distribution of our dataset's probability and something that's actually quick and simple. So, why do these other methods become intractable? Well, one of the most popular methods using these kind of Bayesian networks, and this is called directed graphical models or these Bayesian networks. We call this thing called the bag of words or the BOW method. And this is used by Bayesian networks and it tries to estimate the feature vector of a sentence with a vocabulary and then it gives you some type of probability. So, if we had you know we're trying to figure out these assumptions and constraints that need to be intractable.

We need to figure out some way to get those words in a vector and that vector could form a sentence and that could have a particular vocabulary. Now the feature vector X has this particular size of words inside of it, and that's kind of the size of that vocabulary in that vector. And so, what we're doing here with this bag of words is we're trying to say, OK, well, what happens when N increases? When we increase the number of words in the sentence, will it become too large? Will it require heavy computations? What exactly is going on? And so, this is the concept of the bag of words questions, right? Let's say if I give you an example of, you know, Bob likes Alice, and Alice likes Bob, they're entirely different sentence but they ended up with the same feature vector as you can see the similarities between Bob likes Alice and Alice likes Bob.

Well, these Bayesian network models will require many assumptions and constraints to make those kind of situations tractable. So, if we look at things like long-term or short-term memory networks, well, this is also known as LSTM in our models here. You first have to understand that with LSTM, what we're trying to do here is in deep learning, we're just trying to learn long-term dependencies. It is basically a variation of our recurrent neural networks that you may be familiar with and they're capable of learning long-term dependencies, sequence prediction, and feedback connections. And this long or short-term memory has feedback connections and it means that it's capable of processing the entire sequence of data apart from the single data points such as images. Now this is kind of interesting because it'll find applications and things like speech recognition or machine translation.

Remember what we're trying to do here is trying to show outstanding performance on a large variety of problems and this is something that long short-term memory networks are capable of doing. Now the pixel conditional neural networks or PixelCNNs as I was talking about earlier. This is like a generative model that uses these autoregressive connections that you would have seen with long short-term memory. And this will allow us to model images pixel by pixel, for example. And then we can decompose things like the joint distribution and/or is also known as the image distribution and this is a product of conditionals. So, they're much faster to train than the PixelRNNs because the convolutions are inherently easier to use parallel processing on our computers rather than serial processing.

And so, this gives the vast number of pixels present large in image datasets and it's an important advantage. So, remember, with PixelCNNs, they're just a generative model. They've been around since 2016. They're just designed to generate images or even other data types iteratively. And all you got to do is provide it some type of input vector where it's a probability distribution of the prior images that are in your dataset. That's all you got to do. And then these things will be able to generate your images clearly. Now, this also leads me to the concept of the autoregressive model. Now, autoregressive models are just a type of statistical model that are used for modeling time series data. And the main idea behind autoregression is that the current observation at time series can be expressed as what's known as a linear combination of its past observations.

Now that sounds like a lot, but really what I'm saying here is that we take the probability product rule of the current dataset that we're facing and we use this probability to guess what the next piece is going to be based on our previous data that we're collecting in that time series. Now this is pretty interesting because what ends up happening is that we can have weight sharing within the models. So, because this is a neural network architecture and it utilizes the probability product rule and a weight-sharing scheme to yield what's called a tractable estimator with good generalization performance. And this tractable estimator with generalization performance makes autoregressive models have good results in modeling with both binary and real-valued observations.

But remember, and they're great when it comes to dealing with things like the time series data that I was just talking about. Now remember what we said here is that we are talking about the autoregressive models as being an architecture that relies only on past values to predict current data values. So, what I've shown here is the math behind that sentence. It's a little advanced, you may already be familiar with it, in which case, you don't need me to hold your hand with it. I can't go into too much detail behind this in the format of these slides, but I just want to show this here for completeness. I just explained to you what's going to happen. So, in a simple example, if you think of autoregressive models use lagged values to indicate current values based on their past values and that's kind of what we're seeing here.

yt would be here the new value, and that's getting the new value from the probabilities of the weights that are attached to each of the previous values from before and the time t. So, here yt is the current value of the time series, yt-1, yt-2, they're what's called the lag values of the time series, and c in this equation would be a constant. a1, a2 are what's known as the coefficients of the model or the weight, and e sub t there is the error term. And so, this really comes down to we're going to use this information to then calculate their joint probability and then this is what's going to be used in the autoregressive models. And remember, like I said, they're used in time series applications, but they can also be used for a wide variety of things. We usually have to deal with data where we can embed them into vectors. This is a tool that allows us to do that. So, we can estimate the distribution of each dimension of the vector by the previous dimensions values.

So, the neural autoregressive model is here. This is a model that the joint distribution p of x if the input vector x of dimension d for any order o. And this is what we're trying to do. Now, this is the idea of this probability here. You know it's a mathematical term here, I can't go into too much detail about what this does. But what it is, is that it's we're using this to compute the probability via prediction layer in the neural network and the weight-sharing scheme that you're seeing here makes the parameter complexity of the model.

And so, this model is going to be trained under the following particular objectives. And this is what you're seeing here is known as the optimization step of this neural autoregressive model. And this is going to be the distribution probability of that input vector x, however, many dimensions that we're facing, basically, this is the core of what the neural autoregressive model is trying to do, and that is predict the next value based on the vector that we've provided it. So, that's a little bit about autoregressive models in PixelCNNs.

10. Video: Introduction to Normalizing Flow Models (it_gcdgaidj_02_enus_10)

Upon completion of this video, you will be able to outline the concept, applications, and benefits of normalizing flow models.
outline the concept, applications, and benefits of normalizing flow models
[Video description begins] Topic title: Introduction to Normalizing Flow Models. Presented by: Elias Zoghaib. [Video description ends]
In this video, I'm going to delve into the concept and applications as well as benefits of normalizing flow models. Now, normalizing flow models are just a class of deep generative models designed to enable flexible and invertible transformations. Now what this means is when we do an invertible transformation, we are talking about the invertibility of what's known as a flow. So, this refers to a sequence of these invertible transformations that are usually applied to a simple distribution like a Gaussian to transform it into a more complex distribution that resembles a target data distribution.

This invertibility ensures that we can go back and forth between the simple and complex distributions. That's all we're doing here is just really applying an analytical inverse of a mathematical function. Now this is just simple algebra, but let's go ahead and take a look at a couple examples of reversible mathematical functions. Now remember a function in this context refers to a number in and a number out mathematical formula. So if I give x to the first equation, you can see that this is a completely invertible system.

I can add to and reverse to. But a non-reversible function would be something like a power law. Now what this is a unique output exists and vice versa. And now with this f(x) x equals x square. It's not a reversible function because for each input, a unique output does exist. It doesn't happen backwards. For example, there are two numbers that can give me 4 in the non-reversible function +2 +2 squared is 4 and as well as -2 because -2 squared is also +4. And so this leads to this non-reversible idea. Now, the flow-based model is a little bit different. Now, these are trained using what's known as the negative log likelihood loss function which I've shown here on the graph.

Now, this is a lot of mathematics, I can't really go into too much detail here to explain, but just know that this basically is what our algorithm of flow-based models is trying to optimize. Now, this loss function where you see here of p Theta of Z is the probability function. Now, this loss function is obtained using what's known as the change of variables formula from basic statistics and so if you're not familiar with that, I encourage you to look at the mathematical insight behind these normalizing flows. Now, this is the change in variables formula, which again I can't get too much detailed into this. So, there are some advantages of the flow normalization. One thing is that normalizing flow offers various advantages over things like GANs and variational autoencoders. And some of them could include things like the training process of a flow-based model is really stable compared to the generative adversarial networks.

The training of GANs which requires careful training and hyperparameters of both generators and the discriminators. Now, there's also another thing is that there's very little noise, so no noise requirements. The normalizing flow models do not need to be put noise on the output and they can have much more powerful local variance models compared to the generative adversarial networks. Now, the simplified convergence of the system is another powerful reason. So, these are much easier to converge when compared to things like the generative adversarial networks and variational autoencoders. You can clearly tell when these systems are completely converged because their performance doesn't increase or decrease as you give it more data. Now, there are some disadvantages as well, and some of them include things like, well, they're less expressive.

Now what this means is that what we're talking about when we say less expressive is that they are poor performance of flow models on tasks such as destiny estimations. And now it is regarded that they are not expressive as some of the other approaches. The other thing that I want to bring up is that they're more difficult to interpret the one of the two things they require for flow models to be bijective. And what I mean by that is that the follow and preservation over the transformations are clear. This often leads to very high dimensional latent space and it's usually much harder to interpret to see what's actually going on in this latent space. Remember the latent space of these neural networks are very difficult for a human to understand especially as the network grows.

Now the samples that are generated through flow-based models are not as good when compared to things like the generative adversarial networks and the variational autoencoders. Couple of things that I want to bring up here and this is known as the Glow architecture. Now, we take the Glow architecture as just an example for better understanding of what flow-based model proposed by OpenAI is easier to understand. Now the Glow architecture is made from a combination of some superficial layers, and these are the repeating layers that scale. And so, the implementation of this comes down to the basics of normalizing flows in the Glow model. And they'll implement the model using things like PyTorch and train on something like the MNIST dataset.

Now what's happening here is we're using these repeated layers for various different models that we're seeing. It's for different applications, such as different dataset coming from MNIST. Now these layers will repeat and they'll scale depending on what you're trying to do. The next thing is that there's something known as the squeeze function. Now, the squeeze function itself is a great way to see if something is going to converge, as in are we going to start seeing an input tensor? And what I mean by that is basically a matrix of numbers into a particular size, and we want to reshape that tensor into what's known as a crosswise manner. And when we reshape the tensor, what happens to our error? Are we employing what's known as the squeeze function? During this testing phase, we want to see if that tensor can actually be squeezed into a convergent solution.

Now this gets really complex to sort of explain without solid examples, just know that what we're doing here is we're taking our input tensor, for example, and we're transforming it in a way that causes that error to sort of converge. Now there are some other layers that we want to also provide, such as the ACT norm or what's known as the 1 by 1 convolution. And these can be understood just by looking basically at a solid example of what 1 by 1 convolution is, which I won't be able to show. But they are a couple of things that you can look at to sort of understand how these neural networks work. And that's the ACT norm, 1 by 1 convolution and what's known as the affine coupling layer. Those 3 layers are the purpose of the Glow architecture. And these are what I'm referring to when we're trying to repeat these layers to try and scale our neural network depending on our data.

That's ideally of what the Glow architecture is for. Now we've talked a little bit about the variational autoencoders, the generative adversarial networks, and this is the main normalizing flow model that are used to sort of expand on the generative adversarial networks a little bit. They have some pros and cons, and this is what we've discussed in this video.

11. Video: Energy-based Models: A Primer (it_gcdgaidj_02_enus_11)

After completing this video, you will be able to recognize the principles, advantages, and uses of energy-based models in artificial intelligence (AI).
recognize the principles, advantages, and uses of energy-based models in artificial intelligence (AI)
[Video description begins] Topic title: Energy-based Models: A Primer. Presented by: Elias Zoghaib. [Video description ends]
In this video, I'm going to represent and explain what energy-based models are, exploring the principles, advantages, and usage of these energy-based models in AI specifically. Let's start with the concept of what it means by energy-based models. So think of the energy function as the core of all of our energy-based models, like a scorekeeper in a game. But let's say that in this game, each possible situation or state gets a score, and the lower the score, the better that move is considered. So in an energy-based model, the energy function just gives possible states a score-based on how good or bad it is, and we want lower energies.

So what I'm going to do here is talk about the probability of a state. Now what we're trying to do in these models, the probability of a state is referring to how likely that state is to happen is higher if the energy is lower. Think of it like rolling a dice where there are some numbers come up more often because they have lower energy. So state with lower energy is like a number on the dice that's more likely to come up. So the concept of the energy-based function, remember, is just trying to model nature in a way. So, these come from physical nature, and what you'll notice is that things in general want to be at their lower energy.

So if you think of pot of boiling water, for example, and you put it on your stove and it's still boiling and you turn off the stove, eventually over time you'll notice that the hot water will start to cool down on its own and it goes down to room temperature because that is the lowest possible energy for that system. It wants to go back to this situation. Now the concept then can be represented with what's known as a Boltzmann distribution. In a Boltzmann distribution, what we're really talking about is this distribution will allow us to describe the probability of a system like states in an energy-based model to be in a certain state as a function so that the state's energy and the temperature of the system are lower. Now the key idea is that the lower energy states are more probable than the higher energy states, but there's still a non-zero probability for the system to be in any state, and this is what the Boltzmann distribution will allow us to do.

Now if we think about no constraints in our system of a Boltzmann distribution, really what we're saying is that every state is theoretically possible. There are no constraint. Any values for our weights inside of our energy-based model is allowed, and that's OK. The Boltzmann distribution will apply to such systems, and it'll do this by assigning probabilities to those states based on their energy levels, their higher energies, they're going to have a smaller probability, so it doesn't restrict which states can exist. Really all we're trying to do is provide a way to quantify how likely each state is, and that's the concept of using the Boltzmann distribution. Now, in relation to distribution with largest entropies, what we're really referring to here is a measure of the uncertainty or randomness in our system.

The Boltzmann distribution is particularly significant because under these conditions with fixed energy, it represents the distribution with the largest entropy. This means if there's a unbiased or natural distribution of states, assuming only the knowledge of the average energy, this is what's going to be available for us. It doesn't oppose any additional structure or constraints beyond what is defined by the energy of the states and the temperature of our system. And that's the idea of favoring a system with the largest entropy. Now when we're talking about temperature, really what we're referring to is the probability of higher energy states. Now this is just a relative probability of observing these states, and they're not just the low energy ones.

So if we have zero temperature, only states corresponding to a global minima of our energy function can be observed. So it's essentially a way of measuring our randomness. We can quantify average energy of interactions between the system and environment. So the higher the temperature, the more common and widespread massive jumps towards the states of higher energy will be. This role of the temperature in a Boltzmann distribution is really just meant for us to measure the variance in our distribution. Now, there's an energy-based models, you'll see that there'sa few different types of architecture. In this architecture, you'll see that we need to choose a energy function so that we can match this Boltzmann distribution that we're talking about. You want to be able for the AI on its own to be able to learn what these rules are so that a less favorable state has a high temperature and a favorable state has a lower temperature. So, it needs to figure out what those rules are, and this comes with the selection of a proper energy function.

Now there's also things like sampling and specifically Gibbs sampling. So in sampling what we need to do is use these things as a way to sort of measure how this energy-based model is performing. And there's a type of sampling known as Gibbs sampling which allows us to measure things as a function of time. So, every few seconds I'm looking at the state of my system and seeing how it's evolved from the last step that it was on, and this is the idea of using Gibbs sampling to do this. So every time I measure a state or a state is updated, it'll spread to all other nodes inside of my machine. And this is going to allow me to quantify how my system is evolving as a function of time. Next thing we want to talk about is the concept of training. Now there are a few different types of energy-based models and the two main ones I want to talk about are the Hopfield networks and the Boltzmann's machines.

Now in Hopfield network, when we're talking about training something, you can think of a Hopfield network like a group of friends who all have opinions on a topic and this group of friends forms the network. So training this network is like teaching these friends to agree on certain topics. And remember think of them as memories or patterns the network should remember. So let's say each friend in this case would be a neuron in the network. That's considered the correct term for them. They can have one of two opinions. They can either agree or disagree. So, when you train the network, you're essentially telling these groups of friends or these group of neurons which opinions they should have for certain topics. In this case, think of a topic as a pattern in our data. Now the interesting thing is that each neuron, or each friend in this case, will influence each other's opinion in this Hopfield network, and you'll find that each neuron will affect others based on how they're connected. So training will involve settling these influences, so that when you mention a topic or a pattern in your data, the neurons end up agreeing on the opinions you've taught them.

Now, this gives us some idea of the Hopfield network. We want them to have a very good stability. We want to reach the point where their opinions don't change anymore. And this is like the network kind of remembering a pattern. So we model this from the mind because that's kind of how we learn from a biological perspective. Now, Boltzmann's machine, you can think of them as a group of friends or group of neurons deciding where to go for dinner, but they're a bit uncertain and they changed their minds based on different factors, like how much they like certain cuisines or what their other friends chose that can also influence their decision. So we want to be able to control their choices and influences. So each friend or neuron in my Boltzmann machine can choose yes or no or dislike a place for example. Basically some type of binary agree or disagree. They influence each other's choices and some are more easily swayed than others. Now we can train with something known as feedback in this with the Boltzmann machine.

Training a Boltzmann machine is essentially like giving feedback on their choices. You tell them how good their decisions were and based on the patterns you want the network to learn and adjust how they influence each other and how decisive they are. Now this is where we get the concept of probability and energy with the energy-based model. Unlike the Hopfield network where friends are quite sure about their opinions, here there's a sense of probability. There's a game of chance in their decision. The lower energy, meaning that they're more agreeable with each other, shows that decisions are more likely. So training involves adjusting these probabilities to match the desired patterns, and this is where the data engineer or the data scientist would come in.

But in both cases what we're really doing is we're training involves adjusting how the neurons influence each other and respond to the inputs. In this case, it could be topics or dinner choices so that the network can reliably produce or remember specific patterns or outcomes. This is the core of how energy-based models behave. So that's a little bit on how the inner workings of energy-based models, specifically the Boltzmann's machines and the Hopfield networks.

12. Video: Diffusion Models (it_gcdgaidj_02_enus_12)

Upon completion of this video, you will be able to outline the concept and significance of diffusion models in generative AI.
outline the concept and significance of diffusion models in generative AI
[Video description begins] Topic title: Diffusion Models. Presented by: Elias Zoghaib. [Video description ends]
In this video, we're going to be discussing the significance of diffusion models in generative AI. A diffusion models are just generative model approaches that they've been seeing growing in popularity in the past several years. And you'll see here that the whole concept of the diffusion models, they're just generative models, that they are used to generate data similar to the data on which they're trained, just like any other generative model. But fundamentally they work in a little bit different.

The diffusion models work by destroying the training data through what's known as successive addition of Gaussian noise. Basically, they're adding and perturbing some randomness to the training data, thereby destroying it. And the whole concept of our diffusion model is to learn to recover the data by reversing this noise process. So if you're adding Gaussian noise, the idea then should be to recover the data point without any addition of Gaussian noise. So that's what we mean by recover our data. And so the diffusion model after its training, we can use the diffusion model to generate data by just passing some randomly sample noise through the learned denoising process.

And through this learned denoising process, we're going to take this sampled noise un noise, this data point, if you will denoise it so that we end up with the original data point and see how much does this denoised data point and the actual data point differ from each other. And then that's what we known as the loss of our diffusion model. Now specifically, a diffusion model is just a latent variable model. This means is that we're going to map it to the latent space using what's known as a Markov chain or actually a fixed Markov chain. And what this means is that this chain is going to gradually add noise to the data in order to obtain the approximate posterior point of our data point. And what's going to happen is there are going to be some latent variables with the same dimensionality of the Markov chain that we have.

Now, ultimately, the whole concept of this diffusion model is to asymptotically transform pure Gaussian noise. The goal of a training of a diffusion model is to learn the reverse process, the undoing of this gradual addition of noise. And what we want to do is traverse backwards along this hidden fixed Markov chain and we can generate new data points this way. There are some advantages to the diffusion model that I just want to go over. One of them is that, well, they are great for things like image quality. Beyond cutting edge image quality, diffusion models will have all sorts of state-of-the-art image quality that you're going to see. And there are things like adversarial training is not needed. So with a generative adversarial network, we know that there has to be a discriminator that's being trained with the diffusion model that does not have to exist at all.

There's also the added benefits of things like scalability and parallelizability in our training performance, and this parallelization is really useful. Diffusion models seem to be producing results almost out of thin air. And what we're trying to see here is that there's a lot of careful, interesting mathematical choices that provide the foundations for the results that you see out of diffusion models and kind of goes beyond the scope to discuss them here. But the best practices are usually just still evolving. Probably in a few months we're going to see more breakthroughs in these diffusion models. Now there's some understanding of diffusion models that come into it here. One of them is known as the forward or the diffusion process. Its data or a single data point is usually an image is progressively noised and the whole concept then is to use this reverse diffusion process or this reverse process to undo the noise.

The noise is then transformed back into a sample from the target distribution. Now the sampling of this chain transitions in the forward process are usually set to things like conditional Gaussians when the noise level is sufficiently low. Now again, this gets a little bit advanced with what noise distribution you're going to be using to actually do this forward process and this reverse process. But the idea is that I add noise to the sample, I send it through the network. I try and reverse that process as much as I can. Now the understanding of this difference here and our diffusion model is trained by finding the reverse Markov transitions that maximizes the likelihood of the training data. And this comes into the idea of this KL convergence. Now what is KL convergence? Now, this is more of a math term than it is a AI term, but the mathematical form of what's known as the KL divergence for continuous distributions really just indicates the function is not symmetric with respect to its arguments.

It's kind of a little difficult to sort of show you kind of need to see a few different mathematical systems. Basically what we're looking for is non-symmetric mathematical functions based on the arguments that we're passing. And then there's things like casting. So what we're trying to do here is we're trying to condition the four posterior of there are results in a tractable form that leads to KL divergence and we do this through casting between Gaussians. This means that the divergence can be exactly calculated with close form expressions rather than things like Monte Carlo estimations. Again, it's a little bit complex to sort of go over these casting and KL divergence terms here, but just understand that they exist. And if you're interested in and learning more about them, I implore you to look at some mathematical text, just sort of understand what these in stats.

Now remember what we're doing here is we're following a forward process. Now this forward process is just as I was saying before, it needs to be defined and in particular, we have time dependent constants that adds this noise to our data points. Now the reverse process is going to undo this. Now, this is where we try and define the functional forms of the noise that we're trying to add. And there are more complicated ways to sort of do that. But this is what we try to assume is what's known as a multivariate Gaussian, and we set these parameters in this multivariate Gaussian to be equivalent to our forward process and see if we can undo this noise that's additioned to this diffusion model. And then there's things like our network architecture. Now the network architecture, really is just we have a very simplified loss function to train a model and we have not to find the architecture of this model.

And so that the only requirement for the model is that its input and output are dimensionally identical. So if you have this restriction, it's really what you'll see with diffusion models that commonly implemented with unit like architectures. So if you are familiar with unit architectures again goes a little bit beyond the scope of this video. That's what we're trying to look for. That's the common architecture used in diffusion models. And then there's the reverse process decoder. Now the path along this reverse process consists of many transformations. Remember, the whole concept for especially if we're dealing with Gaussian distributions, is we want to be able to produce an image which is composed of, let's say, if we're dealing with images, we want to compose it of integer pixel values, and so we want to devise a way to obtain, for example, a discrete likelihood, or what's known as a log likelihood for each possible pixel values across all the pixels.

And what we're trying to do here is we're trying to decode our final answer from this reverse process and get the original data point back. And again, this gets very mathematical very quickly, and it's hard to sort of explain this through a PowerPoint, but just understand that the whole process of decoding it comes from the ideas that we're using a conditional Gaussian distribution to do this process. OK, so that's a little bit about diffusion models and how they work and some of the advantages and disadvantages of using them.

13. Video: Generative Model Evaluation (it_gcdgaidj_02_enus_13)

After completing this video, you will be able to recognize the metrics and methods used to evaluate the performance of generative models.
recognize the metrics and methods used to evaluate the performance of generative models
[Video description begins] Topic title: Generative Model Evaluation. Presented by: Elias Zoghaib. [Video description ends]
In this video, I'd like to evaluate the generative models. So we want to learn about the various metrics and methods used to evaluate the performance of these generative models. In this example, I'm going to be talking about GANs, generative adversarial networks. Now a generative adversarial networks and models themselves is that there's no objective loss function.

So remember, a loss function is just the error between our expected output and what the network is outputting, and we try and minimize between the two, right? Basically, what is my answer supposed to be? And what is my network spitting out? And take the difference between those and that's called the loss function. That doesn't really exist to train a generator such as a GAN, and so there's no objective way to assess the progress of the training and the relative or absolute quality of the model just from the loss alone. Instead, there are a few things like qualitative and quantitative techniques that I'm going to be talking about. And those performance will usually involve things like manual inspection, which is the qualitative approach.

Well, there's no objective loss function, then we need to be able to evaluate the quality of generated synthetic images, for example. And one of the most basic and useful ways to evaluate this is just by inspecting it and judging the generated examples from different iteration steps and see does this make sense. Now that's more of a qualitative approach and there are different disadvantages with using this. One of the ones that I'd like to talk about is known as bias. It's all subjective and includes the biases of the reviewer and it also involves expertise. It requires domain knowledge to tell what is realistic and what is not. For example, if I'm trying to count on the aid of dermatologists who can assess fake examples of, let's say, if a tumors on somebody's skin, you would need a bunch of dermatologists to know what's a fake and what's a real tumor on someone's skin or mole.

Now, that makes it a little bit impractical and it's limited in terms of the number of images that can be reviewed because, well, you're not going to just sit in front of the machine and just start looking at the millions of images 1 by 1. It would take a very long time to do, so there's no clear best practices on how to precisely qualitatively review the generated images, and so it's likely to vary a lot from case to case. And so this is kind of the main issues with using qualitative approaches. Now, there are quantitative approaches to doing this, right? So, one of the interesting things that I'd like to talk about is, well, what are some of these quantitative things? Well, there's something known as fidelity, right? When we're measuring how well our GAN performs, we need to evaluate 2 main properties.

One of them is this fidelity. And what this means, it's the quality of the generated samples. We measure how realistic the images are in some way, and you can think of it as how different each fake sample is from its nearest real sample, for example, and coming up with some type of fidelity metric based on your training data. And then there's things like diversity, and this is the variety of the generated samples. So we'd like to measure how well the generated images cover the whole diversity or variety of the real distribution in the training data. And so those are all calculatable approaches and metrics that we can follow. And this involves with some good advantages with using quantitative approach one of the things is, well, if we capture fidelity and diversity, you can get a pretty good notion of how well the generator is generating these fake images.

And so we can look at things like authenticity or generalizability. And this is where we can measure the rate at which the model invents new samples and tries to spot overfitting to the real data. And so this is usually defined as the fraction of generated samples that are closer to the training dataset than the other training data points. And then there's the synthetic data should also have a predictive performances where we can use prediction. It should be just as useful as real data for the subsequent task when used for some predictive process. That's really important. Predictive performance we want to train our synthetic data and test on real data. And what we're trying to evaluate is how well the synthetic data covers the distribution of the real data. And we can train things like the test models with synthetic data. There are some key metrics that we want to also follow.

Most common metrics to evaluate GAN performance are things known as the Kernel Inception Distance. And what this does is it's KID or Kernel Inception Distance has been proposed as a replacement for another metric that we'll talk about here. And this other metric that we're talking about is known as Frechet Inception Distance. Now what this means is with the FID is we want to compare real and fake images. Classifiers can be used as feature extractors and we use things like feature extractor could be a few of them. One of them is known as the Inception V3 classifier. That's feature extractor and this is pre-trained on things like ImageNet. Now we've got the output layer that can get the embeddings of real and fake images to extract the feature distance.

And these embeddings are two usually normal distributions and we do some mathematical operations on these normal distributions and we get what's known as the FID score. And there are some shortcomings with doing this. It uses what's known as a pre-trained inception model and that may not be able to capture all the features. And the second is that it needs a very large amount of data for this to work. It's slow to run and uses limited statistics and it's basically working on mean and covariance statistics. Now here comes the Kernel Inception Distance. This is proposed as a replacement for the FID and this has no unbiased estimator which leads to higher expected value on smaller datasets. So immediately the cost of training goes down, cost of evaluation also goes down, and so the KID is suitable for smaller datasets.

Its expected value does not depend on the number of samples at all and it's computationally lighter, more numerically stable and it's simpler to implement than the Frechet Inception Distance. So you'll find that the Kernel Inception Distance might be a better approach to get this key metric for our GANs. There are a few more key metrics that we want to follow. One of them is known as precision and this a little bit related to fidelity. It looks to overlap between real and fake data and we want to over how many non-realistic images the generator produces, and that's usually known as the non-overlap. And then the recall relates to diversity. It looks at overlapping between reals and fakes. And what we're trying to do here is over all of the reals that the generator cannot model and that's called the non-overlap.

So a little bit different here between the precision and the recall. Another thing is known as the inception score. Now this is similar to our FID and KID that we saw on the previous slide. This score measures both the diversity and the fidelity of the images. The higher it is, the more real and diverse the generated images are. And so what we're trying to see here is that the inception score is calculated by first using a pre-trained inception model that we're talking about and we're going to try and predict the class probabilities for each generated images. So let's give you like a little bit of an example of what's going on here. So if you look at the precision of the data, if you look at the red dots here, those would be considered the real data. They overlap with that blue space that is the training data that we have. And so then you can honestly see that this is a nice representation of the blue data.

It will have a good fidelity score. And right away the dots that are outside of this blue distribution are what's noted as fake distributions. So that right there tells me that OK, that my generative adversarial network is able to generate proper data. If we look at the opposite side here with the recall, we can start to see OK, how much of this is considered junk data compared to the real data that we're generating. And we see right here that this table seems to be doing much better than the other ones. So a little bit difference between the precision and the recall. Remember the blue dots on the red surface just represent realness compared to the red dots on the blue. We're basically trying to identify any erroneous non-real like images if we're dealing with images or text.

And if you look at the one on the right, you'll see that there are more dots that are considered real versus unreal and vice versa on the left. So this tells me that we are doing pretty well with this generative adversarial network. That's a little bit about what those slides mean. And so let's go ahead and look at some more key metrics that I'd like to discuss and one of them is known as the perceptual path length. Now this is our PPL. You'll see it referred to in the literature. Now this is a measure of feature, what's known as feature disentanglement. And what we do is this is just a type of regularization that encourages good conditioning and prevents overfitting in the mapping from latent codes to images.

And you'll find that we should also discuss what's known as the StyleGAN2 latent spaces. Now we find 2 latent spaces in our GANs and that is the Z space and what's known as the W space. And really what you can think of these as the Z space is what's known as vectors which come from a Gaussian distribution reside. And we have a standard GAN which is a Z factor is directly fed into a generator to generate this fake image, but in a style again, what's happening here is that vector, that Z vector that we're talking about which comes from a Gaussian distribution, is passed through a mapping network to produce a transformation of that Z vector which is known as the W vector.

And this is referred to in the literature as the W space. And it doesn't follow any specific distribution like the Z vector did, but it's learned during training so that it can better model the underlying distribution of the real data. So it's a little bit of hand-waving there underneath the technology in statistics but the idea is that these are 512 dimensional latent spaces that help to understand the underlying distribution of our training data in our latent space. So, that's a little bit of the perpetual length and the StyleGAN2 latent spaces, and the differences between precision and recall and generative adversarial network.

14. Video: Course Summary (it_gcdgaidj_02_enus_14)

In this video, we will summarize the key concepts covered in this course.
summarize the key concepts covered in this course
[Video description begins] Topic title: Course Summary. Presented by: Elias Zoghaib. [Video description ends]
So in this course, we've examined generative models and types. We did this by exploring the training generative models, generative vs. discriminative models, and the framework of generative modeling. Generative model types, variational autoencoders, generative adversarial networks, and autoregressive models, normalizing flow, energy-based and unpacking diffusion models, and how to evaluate the performance of generative models. In our next course, we'll move on to explore large language models and their use.

© 2023 Skillsoft Ireland Limited - All rights reserved.