{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8246b47c",
   "metadata": {},
   "source": [
    "<details><summary style=\"display:list-item; font-size:16px; color:blue;\">Jupyter Help</summary>\n",
    "    \n",
    "Having trouble testing your work? Double-check that you have followed the steps below to write, run, save, and test your code!\n",
    "    \n",
    "[Click here for a walkthrough GIF of the steps below](https://static-assets.codecademy.com/Courses/ds-python/jupyter-help.gif)\n",
    "\n",
    "Run all initial cells to import libraries and datasets. Then follow these steps for each question:\n",
    "    \n",
    "1. Add your solution to the cell with `## YOUR SOLUTION HERE ## `.\n",
    "2. Run the cell by selecting the `Run` button or the `Shift`+`Enter` keys.\n",
    "3. Save your work by selecting the `Save` button, the `command`+`s` keys (Mac), or `control`+`s` keys (Windows).\n",
    "4. Select the `Test Work` button at the bottom left to test your work.\n",
    "\n",
    "![Screenshot of the buttons at the top of a Jupyter Notebook. The Run and Save buttons are highlighted](https://static-assets.codecademy.com/Paths/ds-python/jupyter-buttons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae023367",
   "metadata": {},
   "source": [
    "**Setup**\n",
    "\n",
    "Run the following cell to import the model and tokenizer for DistilGPT-2 as well as PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9378e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import random\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c74d0",
   "metadata": {},
   "source": [
    "#### Checkpoint 1/2\n",
    "\n",
    "\n",
    "We'll begin our exploration of perplexity by first seeing how to obtain the cross-entropy of a given sequence. Thankfully, Hugging Face does all the hard work of implementing these equations for you! In fact, the `loss` attribute of the Hugging Face `model` object is the cross-entropy of the likelihood of the model generating the sequence in \"inputs\".\n",
    "\n",
    "Below, we calculate the cross-entropy (the `loss`) of the likelihood of GPT-2 generating the sequence in \"inputs\".\n",
    "\n",
    "To calculate `loss`, simply pass the tokenized inputs (`inputs[\"input_ids\"]`) to the model as both its `input_ids` and its `labels`. Then access the loss value by appending `.loss` to the line in which you call the model with these arguments.\n",
    "\n",
    "Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318baec",
   "metadata": {
    "deletable": false,
    "tags": [
     "cp1"
    ]
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"I'm feeling quite perplexed by all these math equations.\", return_tensors=\"pt\")\n",
    "\n",
    "## YOUR SOLUTION HERE ##\n",
    "loss = model(input_ids=inputs[\"input_ids\"], labels=inputs[\"input_ids\"]).loss\n",
    "\n",
    "print(loss.item()) # this returns the cross-entropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a172e5e",
   "metadata": {},
   "source": [
    "#### Checkpoint 2/2\n",
    "\n",
    "Now we simply exponentiate the cross-entropy above to obtain the perplexity.\n",
    "\n",
    "'Exponentiating' the cross-entropy simply means raising $e$ to the power of the cross-entropy, where $e$ is Euler's number, an irrational constant used to model growth that shows up several other places in ML (in the sigmoid equation, for instance.)\n",
    "\n",
    "We obtain $PPL$ (the abbreviation for perplexity) from our cross-entropy `loss` value in the previous cell by raising $e$ to the power of that `loss`.\n",
    "\n",
    "Thankfully, PyTorch has a method for such an operation (called 'exponentiating'), called `torch.exp()`. Pass `torch.exp()` our `loss` and execute the cell to see DistilGPT-2's perplexity at the sequence \"I'm feeling quite perplexed by all these math equations.\n",
    "\n",
    "Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ed471",
   "metadata": {
    "deletable": false,
    "tags": [
     "cp2"
    ]
   },
   "outputs": [],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "ppl = torch.exp(loss)\n",
    "\n",
    "print(f\"Perplexity: {ppl.item():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
