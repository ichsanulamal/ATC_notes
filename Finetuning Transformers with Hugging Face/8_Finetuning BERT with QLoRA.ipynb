{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8246b47c",
   "metadata": {},
   "source": [
    "<details><summary style=\"display:list-item; font-size:16px; color:blue;\">Jupyter Help</summary>\n",
    "    \n",
    "Having trouble testing your work? Double-check that you have followed the steps below to write, run, save, and test your code!\n",
    "    \n",
    "[Click here for a walkthrough GIF of the steps below](https://static-assets.codecademy.com/Courses/ds-python/jupyter-help.gif)\n",
    "\n",
    "Run all initial cells to import libraries and datasets. Then follow these steps for each question:\n",
    "    \n",
    "1. Add your solution to the cell with `## YOUR SOLUTION HERE ## `.\n",
    "2. Run the cell by selecting the `Run` button or the `Shift`+`Enter` keys.\n",
    "3. Save your work by selecting the `Save` button, the `command`+`s` keys (Mac), or `control`+`s` keys (Windows).\n",
    "4. Select the `Test Work` button at the bottom left to test your work.\n",
    "\n",
    "![Screenshot of the buttons at the top of a Jupyter Notebook. The Run and Save buttons are highlighted](https://static-assets.codecademy.com/Paths/ds-python/jupyter-buttons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae023367",
   "metadata": {},
   "source": [
    "**Setup**\n",
    "\n",
    "Run this cell to install and set up our finetuning run using code we've already used for two previous finetunes.\n",
    "\n",
    "We're making sure the training arguments are the same as those for LoRA and the full finetune for the sake of comparing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9378e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training, LoftQConfig\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv('imdb_data.csv')\n",
    "\n",
    "training_set = Dataset.from_pandas(data[data['dataset'] == 'train'])\n",
    "test_set = Dataset.from_pandas(data[data['dataset'] == 'test'])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") # this tokenizer will work for our smaller model\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_training_set = training_set.map(tokenize_function, batched=True)\n",
    "tokenized_test_set = test_set.map(tokenize_function, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./temp_results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=12,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c74d0",
   "metadata": {},
   "source": [
    "#### Checkpoint 1/4\n",
    "Now, on to the unique components of quantization.\n",
    "\n",
    "First, we'll define the config of our quantization library BitsAndBytes.\n",
    "\n",
    "There are several options for how many bits you'd like to quantize the model to, with 8 a common configuration. However to keep things simple, we'll quantize our BERT model to 4 bits.\n",
    "\n",
    "Now, let's fill in the configuration for our quantization library, `BitsAndBytes`.\n",
    "We will be using the following parameters:\n",
    " \n",
    "`load_in_4bit`: This parameter, when set to `True`, will load the model in 4-bit precision.\n",
    " \n",
    "`bnb_4bit_quant_type`: This parameter specifies the type of quantization. We will use `\"nf4\"` which stands for normalized float 4-bit.\n",
    "  \n",
    "`bnb_4bit_use_double_quant`: This parameter, when set to `True`, will use double quantization. Double quantization can help to reduce the quantization error.\n",
    "  \n",
    "`bnb_4bit_compute_dtype`: This parameter specifies the data type for computation. We will use `torch.bfloat16` which is a 16-bit floating point format. This is the precision of the weights when they're being used for computation and are temporarily not quantized. \n",
    " \n",
    "Fill in these parameters in the `BitsAndBytesConfig` in the cell below.\n",
    "\n",
    "Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318baec",
   "metadata": {
    "deletable": false,
    "tags": [
     "cp1"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "config = BitsAndBytesConfig(\n",
    "## YOUR SOLUTION HERE ##\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a172e5e",
   "metadata": {},
   "source": [
    "#### Checkpoint 2/4\n",
    "\n",
    "Now that we have our quantization configuration set up with BitsAndBytes, it's time to prepare our model for quantization-aware training using QLoRA.\n",
    "\n",
    "This time, when we instantiate our BERT model, we'll pass in a `quantization_config` argument that specifies the quantization configuration we defined earlier (the `config` in the previous cell).\n",
    "\n",
    "Hugging Face has another awesome, batteries-included library for QLoRA called LoftQ. It quantizes the model with LoRA finetuning in mind.\n",
    "\n",
    "We'll define a LoftQConfig that specifies that we want to quantize our model to 4 bits and assign it to `loftq_config`. We'll do so by passing the `LoftQConfig()` function the `loftq_bits` parameter set to 4.\n",
    "\n",
    "The `prepare_model_for_kbit_training` we import below prepares our model for quantization. `kbit` in this case just refers to quantizing the model to a certain number of bits (k). To prepare the model, simply pass `model` to the `prepare_model_for_kbit_training()` function.\n",
    "\n",
    "Pass the quantization config argument to the model instantiation, initialize the LoftQConfig, and prepare the model for kbit training in the lines below.\n",
    "\n",
    "Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ed471",
   "metadata": {
    "deletable": false,
    "tags": [
     "cp2"
    ]
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\", num_labels=2, quantization_config=config) \n",
    "## YOUR SOLUTION HERE ##\n",
    "loftq_config = LoftQConfig(loftq_bits=4)\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d481799",
   "metadata": {},
   "source": [
    "#### Checkpoint 3/4\n",
    "\n",
    "We've mostly finished adapting our code to run QLoRA. The final remaining difference is that we pass our `loftq_config` as a named argument to the `LoraConfig`.\n",
    "\n",
    "Let's also take the opportunity to practice setting up our LoRA hyperparameters once again.\n",
    "\n",
    "In the `LoraConfig()` call below, pass `loftq_config` to the named argument `loftqconfig`, then set up our LoRA with a rank of 16, an alpha of 32, and a dropout of 0.1.\n",
    "\n",
    "Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddf801b",
   "metadata": {
    "deletable": false,
    "tags": [
     "cp3"
    ]
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "## YOUR SOLUTION HERE ##\n",
    "    loftq_config=loftq_config,\n",
    "    r=16,  # Rank of low-rank matrices\n",
    "    lora_alpha=32,  # analogous to the learning rate in normal GD\n",
    "    lora_dropout=0.1  # Dropout rate, helps prevent overfitting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d041b18a",
   "metadata": {},
   "source": [
    "Execute the cell below to instantiate the QLoRA model and print the trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37e1b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "qlora_model = get_peft_model(model, peft_config)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "print_trainable_parameters(qlora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316233d2",
   "metadata": {},
   "source": [
    "The quantized model is much smaller than the original, which means we're training an incredibly small number of parameters in QLoRA. While this may result in disappointing performance on our IMDB BERT experiment, this exact technique can be used on the latest LLMs with great results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e141e6bd",
   "metadata": {},
   "source": [
    "### Checkpoint 4/4\n",
    "Alright, let's get our QLoRA model trained!\n",
    "\n",
    "To start, set up our trainer by passing it the `qlora_model` as `model`, with the `training_args` as `args`, the `tokenized_training_set` as `train_dataset`, and the `tokenized_test_set` as `eval_dataset`.\n",
    "\n",
    "Then, under the second comment of \"YOUR SOLUTION HERE\", instruct the model to `train()` and then `evaluate()` using the syntax we've already covered. If you've forgotten what that looks like, check the hint.\n",
    "\n",
    "Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b9a00",
   "metadata": {
    "tags": [
     "cp4"
    ]
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "## YOUR SOLUTION HERE ##\n",
    "    model=qlora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_training_set,\n",
    "    eval_dataset=tokenized_test_set\n",
    ")\n",
    "\n",
    "## YOUR SOLUTION HERE ##\n",
    "trainer.train()\n",
    "trainer.evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
