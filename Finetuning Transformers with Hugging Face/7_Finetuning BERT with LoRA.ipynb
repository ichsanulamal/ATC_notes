{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8246b47c",
   "metadata": {},
   "source": [
    "<details><summary style=\"display:list-item; font-size:16px; color:blue;\">Jupyter Help</summary>\n",
    "    \n",
    "Having trouble testing your work? Double-check that you have followed the steps below to write, run, save, and test your code!\n",
    "    \n",
    "[Click here for a walkthrough GIF of the steps below](https://static-assets.codecademy.com/Courses/ds-python/jupyter-help.gif)\n",
    "\n",
    "Run all initial cells to import libraries and datasets. Then follow these steps for each question:\n",
    "    \n",
    "1. Add your solution to the cell with `## YOUR SOLUTION HERE ## `.\n",
    "2. Run the cell by selecting the `Run` button or the `Shift`+`Enter` keys.\n",
    "3. Save your work by selecting the `Save` button, the `command`+`s` keys (Mac), or `control`+`s` keys (Windows).\n",
    "4. Select the `Test Work` button at the bottom left to test your work.\n",
    "\n",
    "![Screenshot of the buttons at the top of a Jupyter Notebook. The Run and Save buttons are highlighted](https://static-assets.codecademy.com/Paths/ds-python/jupyter-buttons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae023367",
   "metadata": {},
   "source": [
    "**Setup**\n",
    "\n",
    "Run the cell below to import the required libraries and initialize our random seed.\n",
    "\n",
    "Most of this code should be familiar, but observe the last import: `peft` is the Hugging Face library for Parameter Efficient Finetuning. It's got everything we need to conduct LoRA in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9378e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, TrainingArguments, AutoModelForSequenceClassification, Trainer\n",
    "from datasets import Dataset\n",
    "import random\n",
    "import pandas as pd\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "set_seed() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e09956",
   "metadata": {},
   "source": [
    "In the cell below, we run all the same setup code we did in the previous finetune. Review it and make sure you remember what's going on, then execute it and proceed to the next checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5210f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = pd.read_csv('imdb_data.csv')\n",
    "training_set = Dataset.from_pandas(data[data['dataset'] == 'train'])\n",
    "test_set = Dataset.from_pandas(data[data['dataset'] == 'test'])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") # this tokenizer will work for our smaller model\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"longest\", truncation=True)\n",
    "\n",
    "tokenized_training_set = training_set.map(tokenize_function, batched=True)\n",
    "tokenized_test_set = test_set.map(tokenize_function, batched=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./temp_results\", # do not change this\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=12,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=1e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\", # do not change this\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079e0e0c",
   "metadata": {},
   "source": [
    "#### Checkpoint 1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c74d0",
   "metadata": {},
   "source": [
    "Now for the unique portion of this training.\n",
    "\n",
    "We'll first important `LoraConfig`, the `TaskType` class, and the `get_peft_model` function from Hugging Face's remarkable `peft` library.\n",
    "\n",
    "Then we'll assign the variable `peft_config` below the `LoraConfig` object that we'll use to configure our LoRA model. Pass `LoraConfig` the following arguments:\n",
    " - `task_type`: The type of task to configure the model for (e.g., sequence classification, token classification, etc.)\n",
    "    - for this task, which is sequence classification, use `TaskType.SEQ_CLS` \n",
    " - `inference_mode`: Whether the model is being used for training or inference\n",
    "    - for this task, which is training, use `False`\n",
    " - `r`: The rank of the low-rank matrices that parameterize the LoRA layers\n",
    "    - Remember that the rank should be a smaller number than any dimension of the weight matrix. A decent start is 16, so enter that.\n",
    " - `lora_alpha`: The alpha hyperparameter is a way of indicating how much importance the model should place on the LoRA weights vs. the original weights of the model. Alpha should typically be about double the size of the the rank. So our value should be 32.\n",
    " - `lora_dropout`: The dropout rate is used to prevent overfitting. Set it to `0.1`\n",
    "\n",
    "\n",
    "Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318baec",
   "metadata": {
    "deletable": false,
    "tags": [
     "cp1"
    ]
   },
   "outputs": [],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,  # false in training, true when inferring\n",
    "    r=16,  # Rank of low-rank matrices\n",
    "    lora_alpha=32,  # analogous to the learning rate in normal GD\n",
    "    lora_dropout=0.1  # Dropout rate, helps prevent overfitting\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a172e5e",
   "metadata": {},
   "source": [
    "#### Checkpoint 2/3\n",
    "\n",
    "True to form, Hugging Face takes care of the rest of the grunt work in setting up our LoRA model. After instantiating the base model, there's only one line of code we need to write.\n",
    "\n",
    "Under our instantiation of `model`, assign to `lora_model` the result of calling `get_peft_model()`, to which we'll pass our `model` and the `peft_config` we defined above, then execute the cell.\n",
    "\n",
    "Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ed471",
   "metadata": {
    "deletable": false,
    "tags": [
     "cp2"
    ]
   },
   "outputs": [],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\", num_labels=2) \n",
    "lora_model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fade793",
   "metadata": {},
   "source": [
    "Excellent. In the following cell, we'll define a function that will show us the total number of parameters in the base model, the number of parameters we'll train using LoRA, and the percentage of the total weight count the LoRA weights are. Pass our `lora_model`, execute the cell, and check out how much we've lowered the compute required for this finetune!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00868180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "    \n",
    "print_trainable_parameters(lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d481799",
   "metadata": {},
   "source": [
    "#### Checkpoint 3/3\n",
    "\n",
    "OK, pop quiz: Remember how to move a model to the GPU?\n",
    "\n",
    "In the cell below, move `lora_model` `to` the `device` we specified in the first cell.\n",
    "\n",
    "Now for another pop quiz: Remember how to train a model with the `Trainer` class?\n",
    "\n",
    "`Trainer` will take the following named arguments:\n",
    "- `model`\n",
    "- `args`\n",
    "- `train_dataset`\n",
    "- `eval_dataset`\n",
    "\n",
    "You should be able to fill these in yourself now. Give it a try!\n",
    "\n",
    "In the last code block in the cell below, train `lora_model` with `trainer.train()` and evaluate it with `trainer.evaluate()`.\n",
    "\n",
    "Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddf801b",
   "metadata": {
    "deletable": false,
    "scrolled": true,
    "tags": [
     "cp3"
    ]
   },
   "outputs": [],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "# put the model on the GPU\n",
    "lora_model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "## YOUR SOLUTION HERE ##\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_training_set,\n",
    "    eval_dataset=tokenized_test_set\n",
    ")\n",
    "\n",
    "## YOUR SOLUTION HERE ##\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea765e2",
   "metadata": {},
   "source": [
    "Finally, same as the full finetune, we could save this model for future use using the `model.save_pretrained()` call below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e4ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./lora_finetunedmodel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
