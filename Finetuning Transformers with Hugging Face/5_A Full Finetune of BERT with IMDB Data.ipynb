{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8246b47c",
   "metadata": {},
   "source": [
    "<details><summary style=\"display:list-item; font-size:16px; color:blue;\">Jupyter Help</summary>\n",
    "    \n",
    "Having trouble testing your work? Double-check that you have followed the steps below to write, run, save, and test your code!\n",
    "    \n",
    "[Click here for a walkthrough GIF of the steps below](https://static-assets.codecademy.com/Courses/ds-python/jupyter-help.gif)\n",
    "\n",
    "Run all initial cells to import libraries and datasets. Then follow these steps for each question:\n",
    "    \n",
    "1. Add your solution to the cell with `## YOUR SOLUTION HERE ## `.\n",
    "2. Run the cell by selecting the `Run` button or the `Shift`+`Enter` keys.\n",
    "3. Save your work by selecting the `Save` button, the `command`+`s` keys (Mac), or `control`+`s` keys (Windows).\n",
    "4. Select the `Test Work` button at the bottom left to test your work.\n",
    "\n",
    "![Screenshot of the buttons at the top of a Jupyter Notebook. The Run and Save buttons are highlighted](https://static-assets.codecademy.com/Paths/ds-python/jupyter-buttons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae023367",
   "metadata": {},
   "source": [
    "**Setup**\n",
    "\n",
    "Below we import the libraries we'll use for our finetune. Take a look at the specific modules we're importing from `transformers`--they'll do the heavy lifting for us on our finetuning run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9378e93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TrainingArguments, AutoModelForSequenceClassification, Trainer\n",
    "from datasets import  Dataset\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079e0e0c",
   "metadata": {},
   "source": [
    "#### Checkpoint 1/4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8c74d0",
   "metadata": {},
   "source": [
    "In the cell below, write a line that assigns \"cuda\" to `torch.device` if \"cuda\" is available, else assigns device \"cpu\".\n",
    "\n",
    "Next, convert the imported DataFrame into a Hugging Face Dataset with the `Dataset.from_pandas()` method we covered earlier.\n",
    "\n",
    "After that conversion, we split it into training and test sets using the 'dataset' column in the original data. \n",
    "\n",
    "Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318baec",
   "metadata": {
    "deletable": false,
    "scrolled": true,
    "tags": [
     "cp1"
    ]
   },
   "outputs": [],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data = pd.read_csv('imdb_data.csv')\n",
    "## YOUR SOLUTION HERE ##\n",
    "full_dataset = Dataset.from_pandas(data)\n",
    "\n",
    "training_set = full_dataset.filter(lambda example: example['dataset'] == 'train')\n",
    "test_set = full_dataset.filter(lambda example: example['dataset'] == 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a172e5e",
   "metadata": {},
   "source": [
    "#### Checkpoint 2/4\n",
    "\n",
    "Now, in the `tokenize_function`, tokenize the examples' `text` column, set the padding to the longest sequence in the batch and enable truncation to ensure all sequences are of the same length.\n",
    " \n",
    "Then, map the `training_set` and `test_set` to the `tokenize_function` with `batched` set to True. This will apply the tokenization in batches which is more efficient.\n",
    "\n",
    "Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ed471",
   "metadata": {
    "deletable": false,
    "scrolled": true,
    "tags": [
     "cp2"
    ]
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") # this tokenizer will work for our smaller model\n",
    "## YOUR SOLUTION HERE ##\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"longest\", truncation=True)\n",
    "\n",
    "tokenized_training_set = training_set.map(tokenize_function, batched=True)\n",
    "tokenized_test_set = test_set.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d481799",
   "metadata": {},
   "source": [
    "#### Checkpoint 3/4\n",
    "\n",
    "Remember the TrainingArguments we defined when we evaluated the base model? Well, now we'll dive a little deeper.\n",
    "\n",
    "We've filled out many of these Training Arguments to sensible settings. \n",
    "\n",
    "- `output_dir` is where the model's output is saved. Don't change this, as we need to save results in `temp_results`. As the name suggests, these are wiped frequently, so don't expect to save anything there.\n",
    "- `warmup_steps` specifies the length of the warm up phase at the start of training. Gradually increasing the learning rate at the start of training can help the model avoid bad outcomes early in the training process.\n",
    "- `weight_decay` helps prevent overfitting by reducing the magnitude of the model's weights.\n",
    "- `logging_dir` specifies where to save the training logs\n",
    "- `learning_rate`, as you should know already, refers to the size of the steps the optimizer takes for each iteration of gradient descent\n",
    "-  `save_strategy` specifies how we wish to save checkpoints of the model across different epochs. Don't change this value.\n",
    "\n",
    "Now, you need to specify the number of training epochs and the batch size for both training and evaluation.\n",
    "Finetuning requires fewer epochs than pretraining.\n",
    "\n",
    "Set `num_train_epochs` to 3, `per_device_train_batch_size` to 12, and `per_device_eval_batch_size` also to 12.\n",
    "\n",
    "\n",
    "Don't forget to run the cell and save the notebook before selecting `Test Work`! Open the `Jupyter Help` toggle at the top of the notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddf801b",
   "metadata": {
    "deletable": false,
    "scrolled": true,
    "tags": [
     "cp3"
    ]
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./temp_results\", # do not change this\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=1e-4,\n",
    "    save_strategy=\"no\", # do not change this\n",
    "    ## YOUR SOLUTION HERE ##\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=12,\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cc4b1d",
   "metadata": {},
   "source": [
    "#### Checkpoint 4/4\n",
    "We've defined our training arguments and instantiated our model. Now we need to move the model to the GPU.\n",
    "\n",
    "Use the variable we defined in the first cell to move the model to the GPU if it's available.\n",
    "\n",
    "In the code below that line, we put everything together in Hugging Face's `Trainer` class. Now that the trainer has the model, the arguments, and both datasets, we simply need to call `trainer.train()` to perform our finetuning run.\n",
    "\n",
    "When you're done, evaluate the model's results with `trainer.evaluate()` on the final line of the cell.\n",
    "\n",
    "*Note: You may wonder, 'Why do we not need to move anything other than the model to the GPU in this code? The answer is that the always-ergonomic Hugging Face moves our data over with the `Trainer` class for us. Gotta love that comfortable developer experience!* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3628c83",
   "metadata": {
    "scrolled": true,
    "tags": [
     "cp4"
    ]
   },
   "outputs": [],
   "source": [
    "## YOUR SOLUTION HERE ##\n",
    "model.to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_training_set,\n",
    "    eval_dataset=tokenized_test_set\n",
    ")\n",
    "\n",
    "## YOUR SOLUTION HERE ##\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa3b5e0",
   "metadata": {},
   "source": [
    "Remember our evaluation loss from the base BERT model? So long as you ran the random seed code in that exercise, your loss value in that initial evaluation should've been 0.7226571440696716. The `eval_loss` above is quite the improvement, right?\n",
    "\n",
    "Finally, we'll run the line `model.save_pretrained(\"./our_finetuned_model\")`. This will save the model so we can use it on our own, which we'll go over in greater depth in the next exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3672d0f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./our_finetuned_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
